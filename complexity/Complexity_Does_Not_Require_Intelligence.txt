Welcome back to the deep dive. I want to start today with a mental image. It might sound a little
bit cliche at first, but just just stay with me on this. Okay, I'm with you. I want you to picture
the ultimate machine, the perfect clockwork universe. Right. Brass gears, intricate springs,
the whole thing. Endless brass gears. You can see them turning, springs coiling up, pistons firing
with, you know, perfect precision. It's engineered. It is completely deterministic. So if you know
where one gear is, you know where everything else is going to be. Exactly. And that image,
that clock has really been our dominant metaphor for, for how things work for what, 300 years? At
least since Newton, really. But now I want you to take that machine in your mind and I want you to
break it. Break it? How? Just melt it down. Watch the brass soften. The gears start to lose their
teeth. Watch them dissolve into something organic. Okay. And suddenly you're not looking at a machine
anymore. You're looking at a coral reef. Ah, okay. That's a massive shift in texture from cold metal
to, to life. Right. It's messy. It feels almost chaotic. You've got thousands of different species
interacting. They're eating. They're dying. They're growing on top of each other. It's a
teeming biological ecosystem. I'm with you. The clock and the reef. So here's the question that
really launches our whole investigation today. Which one of those two images, the perfect clock
or the messy reef, is actually more complex? Well, that's a trap of a question, isn't it? It is a bit
of a trap. Because historically, science and, you know, especially the field of artificial intelligence
would have pointed straight at the clock without hesitation. Why? Because we built it. Because we
can explain it. We love the clock because it's rational. It's reducible. We look at the reef and
we just see, well, chaos. We wave our hands and call it nature or instinct. Exactly. And it's that
bias, that deep-seated preference for the engineered over the organic that we are going to try and
dismantle today. It's a big project. It is. We're opening up a file that I genuinely think is going
to fundamentally change how you, the listener, look at intelligence and complexity. The paper is titled
Complexity Without Intelligence. It's by a researcher named Flixian, and it was just published January 24,
2026. And that title alone is, I mean, it's deliberately provocative. Complexity Without Intelligence.
It feels like an oxymoron, right? A contradiction in terms. We are just so trained to think that
smart equals complex. It's our default setting. If I show you a supercomputer that's, you know,
beating a grandmaster at chess, you're going to say, wow, that is a complex system. Of course.
But if I show you a common garden spider weaving this incredibly intricate, beautiful web,
you're probably going to say, well, that's just instinct. That's just biology doing its thing.
We dismiss it. We dismiss the spider. We assume that because the spider isn't consciously thinking
about, I don't know, the tensile strength of its own silk or calculating the geometric angles in its
head, that the activity itself must be simple. But Flixian is here to argue that we have this
completely fundamentally backward. The mission of this paper, and really our mission for this deep dive,
is to take a scalpel and surgically separate two ideas that have been glued together for decades.
And those two ideas are? What an agent knows so, its intelligence, its ability to learn, and what an
agent does, its complexity. And to really appreciate why that's such a heavy lift, you have to understand
the sheer amount of historical baggage we're all carrying. What do you mean?
Well, for the last 70 years or so, cognitive science, AI, philosophy of mind, they've all been
completely obsessed with the internal.
The brain.
The brain, the CPU, the little ghost in the machine.
We're always trying to peek inside the black box.
Precisely. We measure an agent's complexity by looking for evidence internal deliberation.
We look for planning. We look for reasoning. And most of all, we look for the ability to learn
new tricks.
And if a system can't learn, we basically write it off. We call it dumb or brittle.
We do. But this paper argues that this view is just incredibly narrow. It's like looking at the
world through a keyhole. It misses the staggering, mind-boggling complexity of systems that just work.
Systems that don't have to think about it.
Systems that function perfectly in the present moment, whether or not they have an inner life
or are consciously deliberating about their actions.
You mentioned before we started that we have this habit of ranking things, like some cosmic
leaderboard of existence.
Oh, we're obsessed with it. It's the great chain of being all over again. You've got humans at the
very top naturally.
Of course.
Then maybe chimps, then dogs, maybe dolphins if we're feeling generous. And then way,
way down to the bottom, you've got the insects and-
And the poor Roombas.
And the poor Roombas.
But the point is, we rank them based on one single criterion. How close are they to us?
How close do they come to mimicking human-style cognition?
That's what Fliction calls the deliberation bias, right?
We confuse the complexity of an action with the perceived effort of thinking really hard
about that action.
Okay. So the paper identifies this huge problem, this bias. What's the solution it offers? It
proposes a new framework.
It does. They call it an affordance-based account.
Right. Now, affordance is a word that gets thrown around a lot, you know, in user experience,
design, in psychology. But how is it being used here? It feels very specific.
It's very specific, and it signals a fundamental pivot. We are pivoting away from trying to count
how much RAM an agent has or how many neurons are in its frontal cortex.
But we're not looking inside the box anymore.
We're looking at the relationship between the agent and its environment.
The relationship. Okay. That feels like the key.
It is. The central claim here is that complexity isn't something you carry around inside your head
like a wallet. Complexity is something that happens between you and the world. It emerges
in the interaction.
Okay. I want to dig into that relationship a lot more. But first, let's establish the first
big pillar of the paper's argument, this concept of synchronic complexity.
Yes. This is the absolute foundation for everything else.
Synchronic? It sounds, I don't know, like a heavy metal band or maybe a fiercely technical
term from linguistics.
It actually does come from linguistics. From Saussure. He made this distinction. You have
diachronic linguistics, which is the study of how language changes over time. You know,
its history, its evolution.
Right. Like how Latin became French and Spanish.
Exactly. And then you have synchronic linguistics, which studies the language system as it exists
at one specific point in time. A snapshot. Frozen.
So when Fliction talks about synchronic complexity, they're asking us to take a snapshot and completely
ignore the history.
Completely and utterly.
Yeah.
Forget how the agent got to be the way it is. Forget if it evolved over millions of years
of natural selection. Forget if it was programmed by a genius five minutes ago. Forget if it learned
a skill through 10,000 hours of practice.
You're saying none of that matters.
For measuring synchronic complexity, no. We don't care. We are asking one simple question.
What can this agent do with rich, structured competence right now?
Okay. But that seems a little unfair to the learners, doesn't it? I mean, if I spent 10 years
of my life learning to play the piano and you build a robot that can just play it instantly,
don't I get some credit for all that effort?
Oh, absolutely. You get enormous credit for intelligence. You get credit for having a powerful
learning capacity. But the paper makes a very sharp distinction here. It argues you do not get
extra credit for the complexity of the performance itself.
So if the robot plays the concerto perfectly and I play it perfectly...
The complexity of the activity, the organized, goal-directed behavior, is identical in that
moment.
Ah, okay. I see the line they're drawing. It's really about separating the journey from the
destination.
Precisely. And they don't just state this, they formalize it. And I think it's worth just touching
on the logic, even if we skip the deep math.
Go for it.
They create two distinct variables. There's TCA, TT, T-day, which is the complexity of agent
A at a specific time T.
Right. C of A at time T. That T is doing a lot of work there, emphasizing the now.
It is. And then there's a completely separate variable, the learners, which stands for the
learning operator. That's the agent's ability to change its complexity over time.
So it's ability to get better or worse.
Exactly. And here's the paper's most radical claim. TAA, AT, and LRLers are independent variables.
They're not necessarily correlated.
Meaning, you can have an agent with incredibly high complexity, but basically zero ability
to learn.
Exactly. And conversely, you can have an agent with a massive capacity for learning, but very
low complexity in the present moment.
Give me an example of that second one. High learning, low complexity.
A human baby. An infant is arguably the most powerful learning machine on the planet. Its LRL value
is off the charts. It's just this incredible sponge for information and new skills.
But it's TAA. TAA. Its ability to actually do stuff right here, right now. It's pretty low. I mean,
it can barely hold its own head up. It can't navigate. It can't feed itself. Its present moment
complexity is minimal.
Okay. Versus the spider.
Versus the spider. The spider might have a learning operator that's close to zero. It's not going to
learn calculus. It probably won't even learn how to weave a fundamentally new style of web if you move
it to a different continent. It's fixed. It's largely fixed. But it's CA. The geometric precision
of the web. The material science of handling the silk. The vibration sensing it's performing right
this second to monitor for prey. That is astronomically high. So for decades, we've been judging the spider
for being stupid because its cellios is low, while completely ignoring that its city A-T-Feo is performing
a feat of engineering that our most advanced robots still struggle to replicate.
That's the internal bias again. We've been looking for the brain that learns and we've totally missed
the body that performs so beautifully.
This leads directly to the idea of fluency. And I have to admit, this is the part of the paper that
really flicked my entire perspective upside down.
I had the same reaction.
Because usually if I do something without thinking, I consider that, well, mindless. Like,
I'm on autopilot. And we usually use autopilot as a criticism.
You're just phoning it in. You're not really present.
Right. But Felines comes along and says that autopilot, or what they call fluency,
is actually a sign of higher complexity, not lower.
It's what I call the paradox of proficiency. The better you get at something, the less you think
about it.
Walk me through an example.
Okay. Think about a student who's learning a second language for the first time. Let's say
it's an American student in Paris trying to order a cup of coffee.
A universally high-stress situation.
Incredibly high-stress. Their brain is firing on all cylinders. They are consciously accessing
vocabulary, wrestling with grammar. They're translating every single word in their head.
Je voudrais un caf√©. They are deliberating. They're using massive amounts of intelligence
and working memory.
They are thinking very, very hard.
Now, look at the Parisian locals standing in line next to them. They lean over and order
their coffee while simultaneously scrolling on their phone, maybe lighting a cigarette,
and dodging a person walking by.
They don't think about the French at all.
Not for a second. They are fluent. Now, who is exhibiting more complexity in that moment?
Well, my old intuition would say the student because their brain is working harder. But based
on this paper, it's the local.
It's the local by a landslide because the local can sustain multiple modes of activity at the same
time. They can engage in a social transaction, process information from their phone, perform a
motor task like lighting a cigarette, and navigate physical space all at once.
Whereas the student is completely consumed by the single task, they're paralyzed by it.
The student is brittle. The local is robust.
Brittle versus robust. That feels like a really key distinction. The student is brittle because
if the barista asks an unexpected follow-up question like, do you want sugar with that?
Their whole system crashes.
Exactly. They have to reboot the whole deliberation process. The local adapts instantly without even
breaking their stride. The paper's point is that deliberation, that process of stopping and thinking
is actually a cognitive bottleneck. It consumes precious resources.
So fluency frees up those resources.
It frees them up, which allows you to layer more activities on top. So in this framework,
not thinking, is a superpower.
It's the ultimate role of complexity, to push all the difficult stuff into the background so
you can interact with the world on a higher, more integrated level.
You've got it.
I want to pause on that term modes of activity for a second. The paper defines them as stable,
goal-explainable patterns of engagement. But isn't there a danger in defining complexity by
stability? I mean, a rock is very stable.
Ah, that's a fair challenge.
A rock has a very stable pattern of engagement with the ground. It just sits there. It's a rock
complex.
It's a valid pushback. But the key is in the second part of the definition. Goal-explainable
patterns of engagement. A rock isn't really engaging with the environment to solve a problem
related to its own viability. It's just being subjected to the laws of physics, like gravity.
It's passive. It's passive. The person ordering coffee is actively solving multiple viability
problems simultaneously. Social interaction, caffeine acquisition, information gathering,
nictine addiction, navigation. They are juggling multiple balls to maintain their state.
The rock isn't juggling anything.
Okay. So complexity is the number of balls you can keep in the air at once.
Without having to consciously look at any of them.
That's a fantastic way to put it. The more you can do without deliberation, the higher
your synchronic complexity.
Exactly.
Which brings us right back to that crucial relationship with the world. We've established
that we're looking at the now, and we're looking for fluent, layered activity. But where does
this flow actually happen? It happens in what the paper calls the affordance landscape.
The landscape of possibilities. It's the space you move through.
Let's switch the visual in our heads again. The clock is gone. The reef is gone. Now I want you
to picture a map. A massive, stylized subway map, like the London Underground map, with thousands
of lines diverging and converging.
That's a great image for it. That's the activity space. And each station, each potential stop
is an affordance.
Okay, so let's define affordance again, but really strictly in this relational sense, because
I think people, myself included, can get tripped up thinking an affordance is just a feature.
You know, my phone has the camera affordance.
Right, and that's not quite it. It's not a feature of the object, and it's not a skill
of the user. It is the handshake between the two. The potential for a meaningful interaction.
The classic example is a chair.
Right. The classic example is a chair. Does a simple wooden chair have the affordance
of sitability?
Well, yes. Of course. That's what it's for.
For you, a human, yes. Your body has the right size, the right joint structure. The chair has
the right height, the right flat surface. The relationship works. The affordance of sitting
exists in that coupling.
But what if you're an elephant?
Then the chair is absolutely not for sitting. For an elephant, it might afford being smashed
or being a minor obstacle. But the sitting affordance does not exist for the elephant
chair coupling. It's gone.
The paper uses a slightly more terrifying example. The affordance of breathing.
Right. Let's look at the formula they use, just conceptually.
Four netters is the set of all affordances, all activities, available to you at time, $10.
My menu of options.
Your menu of options. Right now, sitting in this studio, the activity breathing is on
your menu. It's in your set four netters. You have lungs. That's the agent's contribution.
And the room has air. That's the environment's contribution. The relation holds.
But if you take me and drop me in the middle of the Pacific Ocean...
Your internal machinery hasn't changed one bit. You still have the same lungs. You're just
as smart. You have the same memories, the same capabilities you had a moment before.
But the environment has changed.
The environment has changed dramatically. The air is gone. The relation is broken. And
so the breathing affordance instantly vanishes from your set affordance.
And my overall complexity drops to zero pretty quickly after that.
Tragically so. But it proves the point with brutal clarity. Complexity isn't something you
just carry around inside your skin. It's relational.
So a supercomputer. A machine with more processing power than all of humanity combined. If you just
float it in the vacuum of space.
It has almost zero complexity. It has no inputs to process. No outputs to affect the world.
It can't do anything. It's just a warm, inert box.
This really dismantles those old brain-in-a-vat thought experiments, doesn't it? We used to think
if we could just perfectly simulate a brain, it would be conscious and complex all by itself.
And this framework, which is part of a broader movement called inactivism, says that's nonsense.
You are not a brain riding a body like a driver in a car. You are the entire coupled system of brain,
body, and world all acting together.
So if you take a simple organism, let's go back to our spider, and you put it in a really rich,
complex environment with lots of anchor points, wind currents, different kinds of prey.
Its activity space, its set of affordances just explodes. The spider in the rich environment
system becomes incredibly complex.
So this is the relational view. Complexity is what you can do with what you have, where you are.
It's not additive. You can't just keep adding more parts to the robot to make it more complex.
Yeah. You have to design the robot to better couple with its world.
Okay. And that coupling, that idea of the affordance landscape,
leads us to a really counterintuitive point. I think this is in section three.
Yes. Constraints.
Right. If complexity is about having options, about having a big, diverse menu of affordances,
then surely the goal is to have all the options. Infinite affordances.
I want to be able to fly and breathe underwater and see through walls.
I'd definitely take the flight option.
Me too. But the paper argues that infinite options wouldn't just be unhelpful,
they would actually destroy you. This is a section on constraints and the power of no.
It's one of my favorite parts of the paper.
So explain this to me. Why would no be powerful? Why is can't sometimes better than can?
It all comes down to the problem of information processing. Think about the concept of absence,
as the paper puts it. Some affordances are absent simply because they are physically impossible for
you. Like me trying to fly by flapping my arms. Exactly. That is a hard constraint imposed on you
by gravity and your own biology. Now you might see that as a limitation. A bummer.
It is a bummer. But the paper argues it's an incredible gift because constraint is prior to
choice. What does that mean prior to choice? It means that because that option is physically
impossible, your brain and your body do not have to waste a single calorie, a single millisecond,
even considering it. Yeah. The decision has been made for you by the universe. I see. So when I'm
standing at the edge of a cliff, I don't have this complex internal debate. Hmm. Should I flap my wings?
Today? Or should I take the stairs? The universe has already removed flap my wings from my menu.
It's already crossed it off for you. Now imagine if you could do anything. Imagine if every single
atom in this room was a valid handle for you to grab. Imagine if every direction was a valid path
for locomotion, including straight up. I'd be paralyzed. I wouldn't even know where to start.
It's overwhelming. We call that combinatorial explosion in computer science. If the number of
possibilities is nearly infinite, the time it would take to calculate the best one also becomes
infinite. You would just freeze, trapped by choice. So constraints act like, what, blinders on a horse?
That's a perfect analogy. They cut off all the peripheral noise and distraction,
so you can just move forward on the path that's actually available to you.
The paper talks about a relevance function here. It uses the Greek letter row.
Right. The relevance function one rowway is the great filter. In this mathematical model,
all the irrelevant or impossible activities, like you trying to fly or trying to walk through that
solid wall, they get a relevance score of zero. So they're filtered out before they even reach my
decision-making process. They never even enter the dynamics. This is what the paper calls the
economization of activity. By making 99.9% of all theoretical actions impossible, nature allows us
to become really, really good at the tiny fraction that is possible. I love the analogy of the musician
for this. I think it makes it really clear. Well, if you sit a musician down on a piano and say,
play any note that exists in the universe, they might just produce noise. It's too open. But if
you give them a key signature, say B-flat minor, you're imposing a constraint. You're limiting their
options. You're saying, don't play these six notes, only play these other ones. Yes. But that very
limitation is what creates the structure. Because they don't have to waste brainpower thinking about
all the wrong notes, they can now creatively and fluently explore the rich relationships between
all the right notes. The constraint enables the complexity. That is a perfect encapsulation of
the argument. Structure is just a positive-sounding word for constraint. You can't build a complex
skyscraper without rigid steel beams that limit where the floors and walls can go. Without those limits,
you don't have a building, you just have a pile of bricks. So, okay, let's recap where we are.
We have the now, which is synchronic. We have the relation, which is the affordance.
And we have the limit, which is the constraint. The three pillars so far.
But let's look at the structure of these activities themselves.
Section four of the paper is called structure, composition, and automation.
Right. Because your affordance landscape, your menu of options, isn't just a flat,
unorganized list. It's not a grocery list. Eat, sleep, walk, talk.
It has a hierarchy. Exactly. It has a hierarchy. The paper makes a distinction between basic
affordances and composite affordances. Okay. What's the difference? Basic affordances are the
sort of atomic units of action. They're the Lego bricks. Things like grasping an object with your
hand, focusing your eyes on a point, lifting a single foot. And composite affordances are the
complex structures you build out of those Lego bricks, like making a sandwich.
Making a sandwich is a perfect example. Or driving a car. You can't just drive a car
as a single action. You have to perform a whole sequence of basic affordances.
Turn the wheel, press the pedal, look in the mirror, move the shifter. You chain them together
in a stable pattern. But here's the crucial part, and it gets back to fluency. If I had to consciously
think about every single one of those basic affordances while making a sandwich, I'd starve to
death before I ever got to eat. Okay. Now I must grasp the knife. Now apply downward pressure.
Now slice the bread. Now stop slicing. It would take forever.
And this brings us to what might be the most important mechanism for complexity growth in
the entire paper. Relegation of control.
Relegation. It sounds like you're being demoted or sent away.
In this context, it's a huge promotion for the system's overall efficiency.
When a composite activity like driving or making a sandwich becomes stable and reliable through
practice, the agent stops consciously managing the details.
You stop thinking about turning the wheel and you start thinking about the goal, like going to the
store. Precisely. The low-level details get compressed and automated. They get chunked together.
The paper has a term for this, right? The projection operator.
Yes. The projection operator, which they denote with a dollar.
The math is a bit dense, but the concept is beautiful. Imagine taking a thousand tiny distinct
gods that represent your basic actions and then drawing a big circle around them and giving that
whole collection a single simple label. One thing.
So turn wheel, press gas, check mirror becomes drive.
It gets projected onto a simpler higher level description.
You now treat that complex chain of actions as if it were a single basic unit. This is the
automation we were talking about earlier.
And this solves the manager problem, the homunculus fallacy. Tell me more about that.
Well, there's this old sort of fallacy in AI and cognitive science that for a system to be
coordinated, you need a central executive or a manager in the brain that's supervising everything,
a little man in the head pulling all the levers. And this model says you don't need that little
man. Right. If the system can automate its own subroutines, if the legs can just handle the walking
so the brain doesn't have to, you don't need a manager for the legs. You relegate control.
You relegate the control down to the limbs themselves or to the spinal cord or to the
cerebellum, to these subconscious embodied habits. And this allows the agent to become incredibly
mind bogglingly complex without needing a massive energy guzzling brain to micromanage every single
twitch. So complexity isn't just about how many things you can do. It's about how many things
you can do without thinking about them. It's about how many layers of trusted automation you were
standing on top of at any given moment. You're standing on a pyramid of your own automated habits.
But pyramids can crumble. And this is where the paper takes a darker, but I think even more fascinating
turn. This is section five. Robustness, breakdown and flexibility. Right. This is the,
so what happens when things go wrong section. And it starts by redefining robustness. We usually
think of a robust system as being like a tank. It's hard, it's armored, it's unbreakable. If a tank
hits a small wall, the wall breaks, the tank keeps going. But that's a very rigid, brittle definition of
robustness. The paper argues for a different view, which they call viability. So what's a viability?
Viability, or true robustness, isn't about never failing. It's not about executing action A perfectly
every time. It's about having action B, C, and D available in your back pocket for the moment when
action A inevitably fails. So it's the difference between a tank and maybe a tracer, a parkour athlete.
That's a great comparison. The tank has one primary mode of engagement. Drive forward, crush obstacle.
If the ground suddenly falls away into a deep chasm, the tank is finished. It has zero other options.
It's stuck. It's stuck. The parkour athlete is all about flexibility. If the wall is too high to climb,
they find a way to jump off another surface. If a ledge is slippery, they slide along it. If they fall,
they know how to roll to dissipate the impact. They have a rich and diverse space of affordances.
They have options. Exactly. The paper defines robustness as the ability to maintain a navigable
space of activity. As long as the set of possible actions, which they call TAT, is greater than zero,
as long as you have at least one move you can make, you are still in the game. You're viable.
And this leads to the most interesting part, the idea of breakdown. Usually we think of a breakdown
as the ultimate failure state. You know, the system crashed. But Fliction says a breakdown event is
actually informative. It's a source of information. Think back to your driving analogy. You're fluently
driving to work. You're thinking about your day, listening to the radio. You are on autopilot.
The drive to work composite affordance is active. It is. Then suddenly your car hits a patch of black
ice and begins to skid. Okay. The autopilot disengages instantly. All alarms go off. What
happens to your awareness? Where does your attention go? It zooms right in. Suddenly I am intensely aware
of the steering wheel in my hands. I can feel the vibration from the tires through the chassis. I'm
consciously pumping the brakes. I'm no longer driving to work. I am correcting a dangerous skid. You've put it
perfectly. The high level composite affordance of drive to work has collapsed. It's failed. And the
basic affordances, steer, brake, look, have rushed back to the surface of my consciousness. The
relegation of control has been undone. It's been undone. And the peeper's brilliant insight is that this
moment of breakdown proves the underlying complexity of the system. How so? Because if you were a truly
simple machine, like a toy car on a track, you would just crash. You wouldn't have any lower layers of
control to fall back on. The very fact that you can decompose the fluent action, you can fluidly shift
from driving to steering and braking, shows that all that complexity was there all along. It was just hidden
by your own fluency. So a system that breaks down gracefully and then reorganizes itself to cope with the
new situation is actually the height of complexity. It's the pinnacle. And this reorganization doesn't
have to be some high level intelligent plan. It can be what the paper calls morphological interaction.
You don't need a master plan to correct a skid. You just use your body. You turn into the skid.
You feel the physics of the situation. You sort of bump into the world in a different way until
stability returns. This feels like it connects directly to the philosophical part of the paper,
the intentional stance discussed in section six. It does. Because when my car is skidding on ice,
I'm not really planning in a cool, detached way. I'm just reacting. But an outside observer would
absolutely say he is trying to regain control. They would assign a goal to my actions.
And that brings us to the big philosophical pivot. Can we legitimately say a system has a goal
or an intention, if it doesn't have a human-like brain, to consciously represent that goal?
This is the ghost in the machine question all over again, isn't it?
It is. And the paper handles it in a very pragmatic way by borrowing from the philosopher Daniel Dennett.
He proposed something called the intentional stance.
What's the stance?
It's a practical tool, not a metaphysical claim. It simply asks the question, is it explanatorily fruitful
to describe the system's behavior as if it has a goal?
So does it help us understand and predict what's going to happen next?
Exactly. Take a heat-seeking missile. Does it love heat? Does it desire to be near the jet engine?
Does it hate the cold emptiness of the sky?
No, of course not. It's just a bunch of sensors and circuitry.
Right. But if your job is to predict where that missile is going to go, it is incredibly useful
and efficient to say it wants to hit the hot target.
Because if I tried to predict its path by calculating the voltage across every wire inside it,
I'd be there for 100 years and the plane would already be gone. The goal is a predictive shortcut.
It's an explanatory filter. It filters out all the messy, low-level details of the mechanism,
allows us to focus on the coherent, organized structure of the behavior.
And the paper argues that for the purpose of measuring complexity,
we don't care if the agent really has a goal deep in its soul.
We just care if its behavior is structured as if it has a goal.
You got it.
And this is the move that allows us to finally compare a biological cell
and a sophisticated robot on the same terms.
Yes. A white blood cell that is chasing a bacterium through your bloodstream,
and a Roomba that is seeking its charging dock when its battery is low.
Neither of them has a brain in the human sense. Neither is thinking,
oh, I'm hungry for electricity.
But both of them exhibit what the paper calls behavioral complexity, or CBATT.
They behave in a stable, goal-directed way. They act as if they have intent.
And by adopting this stance, we can use the same language, and even the same math,
to describe the complexity of a living organism and an artificial one. We don't have to get bogged
down asking, but does it have a soul? We just ask, does its behavior have a direction?
It cleanly removes the metaphysics from the equation, which is absolutely essential if you
want to build a rigorous objective science of complexity.
This leads us to the final problem. And it's a very human problem. The problem of comparison.
We love a winner.
We love a winner. We need a leaderboard. We want to be able to rank things.
We want to know who is number one.
Exactly. I want to be able to say, humans are a hundred out of a hundred on the complexity scale.
Chimps are an 80. Spiders are a 10. My Roomba is a 2.
And the paper looks that desire and says, quite firmly, stop it. You can't do that. It's a
meaningless exercise. Why not? Why can't we just measure everyone's affordance landscape and see
who has the most? Because of a concept from mathematics called incomparability.
The paper uses the symbol of two parallel lines to represent this.
$8 key AT. Parallel A to an AT. That means the two agents are running on parallel tracks that never
intersect in a way that allows for a simple greater than or less than judgment.
It's the classic apples and oranges problem.
It's the submarine versus the skyscraper problem. Which one is better or more complex?
Well, it's a nonsensical question. The submarine is better at moving underwater,
and the skyscraper is better at staying still and being tall.
Exactly. Their activity spaces, their sets of affordances are almost completely disjoint.
One has affordances related to buoyancy and pressure. The other has affordances related
to structural load and wind shear. There's no universal neutral metric of complexity that you
can apply fairly to both. So you can only compare them locally?
You can only compare them locally. You can ask which is better at withstanding water pressure,
and the submarine wins. You can ask which can house more people, and the skyscraper wins.
But you can't ask which is globally more complex.
The question is mathematically meaningless within this framework,
and this leads to what the paper calls plurality and indeterminacy.
We just have to accept that there is no single peak on the mountain of evolution or design.
There are many, many different peaks.
That is surprisingly humble. It's a framework that intentionally takes humans off the pedestal.
We aren't the ultimate goal of evolution. We're just one possible type of complex system among many.
We're just one specific and in many ways very weird configuration of affordances.
A spider is another. A coral reef is another. A sophisticated AI running a global supply chain is
another. They are all valid. They are all real forms of complexity.
Okay, so let's try to bring this all home. Let's synthesize this. We started this journey with the
image of the clock and the reef. We did. We moved from the clean, deterministic,
mechanical view to the messy, relational, ecological view. We learned about synchronic complexity,
the radical idea that what matters is what you can do now, not how you learn to do it.
Then we learned about the affordance landscape, that complexity doesn't live inside your head.
It lives in the relationship between you and your world.
We saw how constraints, the power of no, are not limitations but are actually the secret ingredient
that structures behavior and makes complexity possible in the first place.
And we saw how relegation and automation are the engines that allow that complexity to scale up,
to build pyramids of habit so you don't get overwhelmed by the details.
And finally, we have to let go of our desire to rank everything on a single human-centric line.
We have to accept that complexity is plural.
Rich forms of agency need not be intelligent in order to be real.
I think that's the final powerful argument of the paper.
It really does change how I look at the world. I find myself looking for the brain
behind things less and less. Instead, I'm looking for the flow. I'm looking for the fit.
The coupling.
The coupling. When I see a bird effortlessly navigating a dense forest at high speed,
I'm no longer impressed by its abstract IQ. I'm just in awe of its perfect,
fluent coupling with the air, the branches, its own momentum.
It democratizes agency. It allows us to finally appreciate the genius of the mindless.
So I want to leave you, our listener, with a thought to chew on today. A bit of homework,
if you will.
Always good to send them off with a puzzle.
Think about your own day. We pride ourselves on our intelligence,
our conscious planning, our deliberate learning, our inner managerial brain.
But I want you to conduct an audit of your own complexity. How much of your day is actually spent
in that mode of thinking? And how much is spent in a state of fluency?
Exactly. Driving your car, typing on a keyboard, walking down a crowded street,
cooking a familiar meal, navigating the subtle cues of a conversation with a friend.
All the things you do without really doing them. Right. So here's the question. Are you actually
at your most complex, your most capable, when you are thinking the least? Is your autopilot
actually the most sophisticated, most impressive thing about you?
That is a very humbling thought to end on. Thanks for taking the deep dive. See you next time.
