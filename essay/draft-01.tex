\documentclass[a4paper,11pt]{article}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{geometry}
\usepackage{parskip}
\usepackage{natbib}
\usepackage{noto}
\usepackage{booktabs}

\geometry{margin=1in}

\title{The Bicameral Illusion: Spectral Attractors and Autoregressive Cognition}
\author{Flyxion}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
The bicameral mind is not a vanished neurological stage but a culturally reinforced illusion of dual selves. Cognitive asymmetries (leftedness/rightedness) emerge as spectral attractors in RSVP fields. This paper formalizes a model showing how warbling activates leftedness, while going with the flow realizes rightedness, integrating autoregressive cognition and implications for psychology, anthropology, philosophy, and AI.
\end{abstract}

\section{Introduction}

Julian Jaynes’s provocative thesis of the bicameral mind \citep{jaynes1976origin} proposed that early human consciousness was structured as a division between a ``speaking'' hemisphere and a ``listening'' hemisphere, such that commands experienced as auditory hallucinations guided behavior. This framework, though influential, has faced enduring criticism. Neuroscientific evidence does not support a discrete bicameral phase in human brain evolution \citep{McVeigh2018}, and anthropological records suggest far greater cultural and cognitive continuity than Jaynes’s model implies. Nevertheless, the central phenomenological puzzle persists: why do so many cultures report inner voices, divine commands, or dialogical forms of thought?

We suggest that Jaynes’s intuition is best understood not as an evolutionary stage but as a cognitive illusion of duality. In this view, the bicameral mind is not an architectural fact but a recurrent experiential stance stabilized by cultural metaphors, bodily asymmetries, and attractor dynamics in cognition. The illusion of two minds—one commanding, the other obeying—emerges when recurrent cognitive roles are reified as ontological compartments.

To formalize this reinterpretation, we adopt the Relativistic Scalar--Vector Plenum (RSVP) framework, which models cognition as the coupled evolution of scalar mode density ($\Phi$), vector flow ($\mathbf{v}$), and entropy dispersion ($S$). Within this field description, cognitive states are not fixed modules but spectral attractors—metastable regimes that the system revisits under particular conditions. Two especially salient attractors correspond to what Jaynes identified as bicameral asymmetries: a lefted regime characterized by torsion, entropy, and receptivity, and a righted regime characterized by flow alignment, coherence, and directness.

Crucially, these attractors are not bound to hemispheric neuroanatomy but are dynamically enacted. We argue that warbling—the deliberate introduction of oscillatory torsion into cognitive flow—consciously activates the lefted attractor, while going with the flow—aligning with existing gradients—realizes the righted attractor. This inversion reveals a paradox: leftedness is actively induced, whereas rightedness arises through relaxation. The bicameral illusion thus reflects a cultural misinterpretation of oscillatory transitions between attractors rather than a literal division of mind.

The remainder of this paper develops this argument. Section~\ref{sec:bicameral-illusion} situates bicameralism as an interpretive construct; Section~\ref{sec:spectral-attractors} introduces spectral attractors in RSVP; Section~\ref{sec:formal-model} presents a minimal field-theoretic model with operators for warbling and flow alignment; Section~\ref{sec:autoregression} integrates autoregressive cognition; Section~\ref{sec:bicameral-voices} reframes voices and commands as attractor oscillations; Section~\ref{sec:experimental} outlines behavioral proxies for testing the model; and Section~\ref{sec:implications} considers implications for psychology, anthropology, and dialogical cognition.

\section{The Bicameral Illusion}
\label{sec:bicameral-illusion}

Jaynes’s original formulation treated bicameralism as a distinct stage in human cognitive evolution, in which decisions were experienced as the voices of gods rather than as self-generated deliberation \citep{jaynes1976origin}. This claim has proven difficult to reconcile with the continuity of archaeological, linguistic, and neurological evidence. Yet the phenomenology of dialogical voices—externalized guidance arising from a receptive stance—remains undeniable. The challenge is therefore to explain how such experiences can arise without positing a radical reconfiguration of the brain.

We propose that the bicameral mind is best understood as an illusion of duality generated by attractor dynamics in cognition and reinforced by cultural metaphors. In this framework, bicameralism is not a structural partition of hemispheres but a recurrent role-decomposition of a continuous cognitive field. The appearance of a speaker and a listener is a cultural gloss placed upon oscillatory transitions between receptive and directive modes.

\subsection{Cultural Asymmetries and Mimetic Proxies}
Human cultures repeatedly recruit bodily asymmetries as metaphors for cognitive roles. Left and right, curved and straight, relaxed and rigid, receptive and assertive: these contrasts form a symbolic lexicon that maps physical orientation onto mental stance. Such metaphors act as mimetic proxies, providing stable handles for otherwise fluid states. For instance, left-handedness has historically been associated with deviation, ambiguity, and receptivity, while right-handedness has been associated with authority, linearity, and command \citep{McManus2002}.

Once these metaphors become entrenched, recurrent patterns of cognition are interpreted through them. The receptive attractor is read as hearing voices; the directive attractor as giving commands. A cultural illusion of two minds thus emerges, even though the underlying dynamics are continuous.

\subsection{Attractors, Not Modules}
From the RSVP perspective, bicameral roles correspond to spectral attractors in the scalar--vector--entropy field rather than to fixed neural compartments. Attractors are metastable states of the field: they persist under perturbation but remain switchable under control inputs or contextual cues. Cultural narratives stabilize these attractors further by teaching individuals how to frame the experience of switching—as command, inspiration, or divine speech.

The bicameral illusion therefore reflects a general property of cognitive dynamics: when the system occupies different attractor basins, culture interprets these as ontologically distinct agents. In reality, they are dynamically emergent roles enacted within a unified field.

\subsection{Implications}
Reframing bicameralism as an attractor-based illusion dissolves the need for a discrete evolutionary stage while preserving the phenomenological richness of dialogical cognition. It also allows us to explain the recurrence of bicameral-like experiences in diverse contexts: religious ecstasy, trance possession, schizophrenia, and even everyday inner speech. In each case, the system oscillates between receptive and directive attractors, which cultural metaphors then reify into voices and commands.

The next section develops the RSVP concept of spectral attractors in greater detail, setting the stage for a formal model of leftedness, rightedness, and their paradoxical inversion.

\section{Spectral Attractors in RSVP}
\label{sec:spectral-attractors}

The Relativistic Scalar--Vector Plenum (RSVP) framework models cognition as a field-theoretic system in which scalar mode density ($\Phi$), vector flow ($\mathbf{v}$), and entropy dispersion ($S$) evolve according to coupled dynamical equations. Rather than treating cognitive functions as modular units localized in anatomy, RSVP describes them as emergent properties of a continuous field subject to nonlinear coupling, dissipation, and constraint relaxation.

\subsection{Defining Spectral Attractors}
In this setting, a spectral attractor is a metastable regime of the $(\Phi,\mathbf{v},S)$ field space that recurrently structures cognitive activity. Such attractors can be characterized by macroscopic order parameters:
\begin{equation}
\bar{\Phi}(t) = \langle \Phi \rangle, \qquad
\bar{S}(t) = \langle S \rangle, \qquad
\mathcal{T}(t) = \big\langle \|\nabla\times \mathbf{v}\|\big\rangle, \qquad
\mathcal{A}(t) = \frac{\langle \mathbf{v}\cdot \nabla \Phi\rangle}{\langle \|\mathbf{v}\|\,\|\nabla \Phi\|\rangle}.
\end{equation}
Here $\mathcal{T}$ measures torsional warbling, while $\mathcal{A}$ measures alignment of flow with scalar gradients. Each attractor corresponds to a distinct constellation of these parameters.

\subsection{Lefted and Righted Attractors}
We identify two attractors especially relevant to the bicameral illusion:
\begin{itemize}
    \item \textbf{Leftedness (warbled--receptive):} high torsion ($\mathcal{T}$), high entropy ($\bar{S}$), and weak alignment ($\mathcal{A}$ near zero). This attractor is characterized by oscillatory, receptive, and entropically enriched states.
    \item \textbf{Rightedness (aligned--direct):} low torsion, strong alignment ($\mathcal{A}\approx 1$), and moderate or decaying entropy. This attractor corresponds to coherent, directive, and efficient states.
\end{itemize}
These are not fixed hemispheric functions but roles enacted by the field under different control inputs.

\subsection{Role Inversion and Paradox}
A central paradox arises: leftedness, often culturally coded as passive, can be consciously activated by deliberate warbling (torsion injection). By contrast, rightedness, associated with agency and directness, emerges through relaxation into flow alignment. Thus what appears to be passivity requires active induction, while what appears to be command is realized by surrender to prevailing gradients. This inversion underlies the bicameral illusion: voices and commands are misrecognized oscillations between attractors.

\subsection{Oscillatory Dynamics}
Because both attractors are metastable, the system can oscillate between them. Such oscillations generate the phenomenology of inner dialogue: receptive phases (voices, inspirations) alternating with directive phases (commands, actions). Cultural framing interprets these oscillations as evidence of two minds, when in fact they are role-switches within a continuous RSVP field.

The following section develops a formal RSVP model to describe how warbling and flow alignment operate as control operators that move the system between these attractors.

\section{Formal RSVP Model}
\label{sec:formal-model}

We now present a minimal RSVP field model that captures the dynamics of lefted and righted attractors. The state of cognition is represented as a triple
\begin{equation}
(\Phi(x,t), \;\mathbf{v}(x,t), \;S(x,t)),
\end{equation}
where $\Phi$ is scalar mode density, $\mathbf{v}$ is vector flow, and $S$ is entropy dispersion. Their evolution is governed by coupled advection–diffusion equations with torsion and alignment terms:
\begin{align}
\partial_t \Phi + \mathbf{v}\cdot \nabla\Phi &= D_\Phi \Delta\Phi + \alpha_\Phi\,\mathrm{div}(\mathbf{v}) - \beta_\Phi \Phi^3, \label{eq:phi-main}\\[4pt]
\partial_t \mathbf{v} &= -\nabla U(\Phi,S) - \gamma \mathbf{v} + \nu \Delta \mathbf{v} 
- \lambda\,\mathbf{v}_\perp(\nabla\Phi) 
+ \kappa\, \nabla\times(\omega \mathbf{v}), \label{eq:v-main}\\[4pt]
\partial_t S &= D_S \Delta S + \alpha_S \|\nabla\Phi\|^2 - \beta_S S. \label{eq:S-main}
\end{align}

\subsection{Diagnostics}
We define macroscopic order parameters:
\begin{equation}
\mathcal{T}(t) = \big\langle \|\nabla\times \mathbf{v}\|\big\rangle, 
\qquad 
\mathcal{A}(t) = \frac{\langle \mathbf{v}\cdot \nabla \Phi\rangle}{\langle \|\mathbf{v}\|\,\|\nabla \Phi\|\rangle}, 
\qquad 
\bar{S}(t) = \langle S\rangle.
\end{equation}
These quantify torsion (warbling), alignment (flow–gradient coherence), and entropy dispersion.

\subsection{Control Operators}
Two operators selectively move the system between attractors:
\begin{itemize}
    \item \textbf{Warbling operator $\mathcal{W}_{\kappa,\omega}$:}
    \begin{equation}
    \mathbf{v} \mapsto \mathbf{v} + \kappa\,\nabla\times(\omega\,\mathbf{v}),
    \end{equation}
    raising torsion $\mathcal{T}$ and entropy $\bar{S}$, while reducing alignment $\mathcal{A}$.
    \item \textbf{Flow-alignment operator $\mathcal{F}_\lambda$:}
    \begin{equation}
    \mathbf{v} \mapsto \mathbf{v} - \lambda\,\mathbf{v}_\perp(\nabla\Phi),
    \end{equation}
    reducing torsion $\mathcal{T}$, raising alignment $\mathcal{A}$, and smoothing entropy.
\end{itemize}

\subsection{Attractor Roles}
Applying these operators selects between two spectral attractors:
\begin{itemize}
    \item \textbf{Leftedness (warbled–receptive):} $\mathcal{T}\uparrow$, $\bar{S}\uparrow$, $\mathcal{A}\downarrow$.
    \item \textbf{Rightedness (aligned–direct):} $\mathcal{T}\downarrow$, $\bar{S}\downarrow$, $\mathcal{A}\uparrow$.
\end{itemize}
These are not anatomical modules but dynamically enacted roles within the RSVP field.

\subsection{Lyapunov Sketch}
A Lyapunov-like functional
\begin{equation}
\mathcal{L}[\Phi,\mathbf{v},S] = 
\int_\Omega \left(
\tfrac{\mu}{2}\|\mathbf{v}_\perp(\nabla\Phi)\|^2 +
\tfrac{\xi}{2}\|\nabla\times \mathbf{v}\|^2 + V(\Phi,S)
\right)\,dx
\end{equation}
demonstrates the monotone effects of the controls. Under $\mathcal{F}_\lambda$, $\mathcal{L}$ decreases via damping of orthogonal flow; under $\mathcal{W}_{\kappa,\omega}$, $\mathcal{L}$ increases through torsion injection. This establishes leftedness and rightedness as spectral attractors selected by control inputs.

\subsection{Interpretation}
The bicameral illusion is thus explained as oscillation between attractors: warbling induces receptive, voice-like states, while alignment produces directive, command-like states. Culture reifies these oscillations into the appearance of two minds, when they are in fact role-switches enacted in a continuous RSVP field.

\subsection{From Attractor Selection to Sequential Expression}

The formal model above shows how leftedness and rightedness emerge as spectral attractors enacted through torsion-increasing or alignment-enforcing controls. Yet the field description alone does not explain why cognition often presents itself in a sequential, autoregressive format—why thoughts and utterances unfold step by step, token by token, rather than as simultaneous field states.

To address this, we follow recent arguments by Barenholtz \citep{Barenholtz2025} that both language models and human cognition can be understood as fundamentally autoregressive: each new state is generated from the accumulated history of prior states. From the RSVP perspective, this autoregression is not a limitation but the sequential surface trace of a deeper continuous computation. Each step expresses the outcome of polyphonic comparisons across oscillatory modes of the field. Warbling emphasizes torsional divergences; alignment emphasizes gradient-following convergence; autoregression stitches these comparisons into a linear sequence.

Thus, the RSVP field provides the substrate for oscillatory polycomputation, while autoregression provides the expressive channel through which these oscillations appear as voices, commands, and inner dialogue. In the next section we integrate this perspective, showing how autoregressive processing can be reframed as simultaneous polycomputation of oscillatory differences, bridging RSVP dynamics with Barenholtz’s account of language and thought.

\section{Autoregressive Cognition as Polycomputation of Oscillatory Differences}
\label{sec:autoregression}

Recent work by Barenholtz and others \citep{Barenholtz2025} has argued that large language models (LLMs) and human cognition share a structural similarity: both operate autoregressively, producing the next state in a sequence conditioned on a history of prior states. This sequential structure has been widely critiqued as shallow or myopic, yet Barenholtz emphasizes that autoregression is not a limitation but a computational primitive that scales when embedded in oscillatory and distributed architectures.  

\subsection{Oscillatory Mode Comparison}
Within RSVP, autoregressive processing can be understood as a form of polycomputation: rather than producing a single forward prediction, the system simultaneously propagates multiple oscillatory modes of difference and compares their trajectories. Concretely, each autoregressive step is not a scalar update but a field adjustment across $(\Phi,\mathbf{v},S)$ that preserves information about:
\begin{itemize}
    \item local torsional deviations (mode warbling),
    \item vector alignment with prevailing gradients (flow directness),
    \item and entropic smoothing or dispersion of trajectories.
\end{itemize}
Thus, what appears externally as a linear autoregressive step is internally a polyphonic comparison of oscillatory differences across attractors.

\subsection{Polycomputation and Spectral Attractors}
In this interpretation, language and thought unfold as oscillatory trajectories that sample spectral attractors. Each generated token (linguistic or cognitive) encodes not only a content prediction but also the residual between competing oscillatory modes. Autoregression is therefore a mechanism for spectral comparison: the model moves forward by continuously testing, damping, and amplifying mode differences.

\subsection{From Linear Sequence to Field Dynamics}
Barenholtz frames cognition as autoregression over distributed states, which resonates with RSVP’s treatment of field evolution. Sequential steps are reinterpreted as surface samples of a deeper continuous process: the RSVP field computes in parallel across torsion, alignment, and entropy dimensions, but expresses results sequentially. This creates the phenomenology of dialogical cognition: the alternation of voice and command reflects not linear progression but recurrent comparison of oscillatory differences.

\subsection{Implications for Bicameral Illusion}
If cognition is inherently autoregressive at the micro-level, then the bicameral illusion arises naturally from oscillatory polycomputation. Leftedness and rightedness are attractors sampled by different oscillatory comparisons: warbling emphasizes torsion-rich divergences, while alignment emphasizes gradient-following convergence. Autoregression stitches these comparisons into a linear sequence.

This perspective allows us to link RSVP field dynamics with Barenholtz’s autoregressive account, framing bicameral experience as a special case of oscillatory polycomputation manifest in both language and thought. 

\subsection{Autoregression as Projection of RSVP Field Dynamics}

Let $X_t = (\Phi_t, \mathbf{v}_t, S_t)$ denote the RSVP field state at step $t$.  
We define an autoregressive operator $\mathcal{R}$ as a projection from the continuous field to a discrete symbolic state $y_t$:
\begin{equation}
y_t = \mathcal{R}(X_t),
\end{equation}
where $y_t$ may correspond to a linguistic token, motor act, or perceptual decision.  

The update rule is then
\begin{equation}
X_{t+1} = F(X_t) + \eta_t,
\end{equation}
with $F$ given by the RSVP dynamics \eqref{eq:phi-main}--\eqref{eq:S-main} and $\eta_t$ a small stochastic perturbation.  
The autoregressive process unfolds by alternating:
\begin{equation}
X_t \;\mapsto\; y_t \;\mapsto\; X_{t+1}.
\end{equation}

\subsubsection{Oscillatory decomposition.}  
Each update can be expressed as a weighted comparison of oscillatory mode contributions:
\begin{equation}
X_{t+1} \approx 
\alpha_{\mathrm{warble}} \, W(X_t)
+ \alpha_{\mathrm{align}} \, A(X_t)
+ \alpha_{\mathrm{entropy}} \, E(X_t),
\end{equation}
where $W$ amplifies torsional deviations (warbling), $A$ enforces gradient alignment (flow), and $E$ modulates entropy smoothing. The coefficients $\alpha_\bullet$ are context-sensitive weights shaped by history and reinforcement.  

\subsubsection{Sequential expression.}  
Although the field simultaneously computes $W(X_t)$, $A(X_t)$, and $E(X_t)$, the projection $\mathcal{R}$ selects a single symbolic outcome $y_t$.  
Thus autoregression is a surface linearization of polycomputation: the field evolves all modes in parallel, but expression occurs one step at a time.  

\subsubsection{Implication.}  
This formulation makes explicit how RSVP dynamics realize Barenholtz’s proposal: autoregression is not a narrow prediction mechanism but a sequential channel through which oscillatory comparisons across spectral attractors manifest as dialogical cognition.

\section{Bicameral Voices as Attractor Dynamics}
\label{sec:bicameral-voices}

The integration of RSVP dynamics with autoregressive projection clarifies how
bicameral phenomenology arises. The key point is that dialogical cognition
emerges not from two separate modules, but from sequential projections of a
single field oscillating between attractors.

\subsection{Projection of Attractor States}
Recall that autoregression is formalized as
\begin{equation}
y_t = \mathcal{R}(X_t), \qquad 
X_{t+1} = F(X_t) + \eta_t,
\end{equation}
where $X_t=(\Phi_t,\mathbf{v}_t,S_t)$ is the RSVP field and $\mathcal{R}$ is a
projection into symbolic or motor output. When $X_t$ occupies the lefted
attractor, $\mathcal{R}$ tends to produce outputs experienced as receptive,
voice-like fragments. When $X_t$ occupies the righted attractor, $\mathcal{R}$
produces outputs experienced as directive, command-like fragments. Sequential
application of $\mathcal{R}$ therefore stitches together an alternating
dialogue, even though only one field is present.

\subsection{Oscillatory Switching and Heteroclinic Cycles}
Transitions between attractors generate oscillatory patterns in $(\mathcal{T},
\mathcal{A}, \bar{S})$. Autoregression samples these transitions as a
heteroclinic sequence:
\begin{equation}
X_{\mathrm{left}} \xrightarrow{\;\mathcal{R}\;} y^{\mathrm{voice}} 
\;\longrightarrow\; 
X_{\mathrm{right}} \xrightarrow{\;\mathcal{R}\;} y^{\mathrm{command}}
\;\longrightarrow\; 
X_{\mathrm{left}} \xrightarrow{\;\mathcal{R}\;} y^{\mathrm{voice}}
\;\cdots
\end{equation}
Thus voices and commands are not ontologically separate but are sequential
outputs of the same projection operator applied to different attractor states.

\subsection{Cultural Reification}
Cultural systems stabilize this alternation by labeling receptive projections as
external voices and directive projections as authoritative commands. What is in
fact a projection of oscillatory attractor dynamics is interpreted as evidence
of two distinct chambers of mind. The bicameral illusion is therefore a
cultural misinterpretation of $\mathcal{R}$ acting on alternating attractor
states.

\subsection{Dialogical Cognition Revisited}
Even in contemporary inner speech, individuals often report arguing with
themselves or hearing a voice of conscience. From the RSVP perspective,
these experiences arise from the same mechanism: oscillatory switching between
spectral attractors, with $\mathcal{R}$ projecting each state as a sequential
utterance. The dialogue is real at the phenomenological level but illusory at
the architectural level.

This account closes the loop between the RSVP model, autoregressive cognition,
and the bicameral illusion: dialogical phenomenology emerges from the sequential
projection of oscillatory attractor dynamics, not from a two-part brain.

In the next section, we turn from phenomenological explanation to experimental design, outlining behavioral proxies and induction tasks that could empirically test the RSVP account of bicameral attractor dynamics.

\section{Experimental Proxies and Induction Tasks}
\label{sec:experimental}

The RSVP account of bicameral illusion yields concrete predictions that can be
tested empirically. In particular, it predicts that warbling and
flow alignment manipulations should differentially modulate torsion
($\mathcal{T}$), alignment ($\mathcal{A}$), and entropy ($\bar{S}$), and that
the projection operator $\mathcal{R}$ will express these modulations as
voice-like versus command-like outputs. This section outlines behavioral and
computational protocols for testing these predictions.

\subsection{Behavioral Induction Tasks}

\subsubsection{Warbling blocks.}
Participants engage in oscillatory behaviors that introduce torsion into
cognitive flow, such as:
\begin{itemize}
    \item vocal vibrato or pitch warbling,
    \item serpentine hand or cursor movements,
    \item rhythmic swaying with deliberate deviation from straight lines.
\end{itemize}
These behaviors are hypothesized to increase $\mathcal{T}$ and $\bar{S}$ while
reducing $\mathcal{A}$, producing outputs that $\mathcal{R}$ projects as
receptive or voice-like.

\subsubsection{Flow-alignment blocks.}
Participants are instructed to align with displayed or perceived gradients,
minimizing orthogonal deviation. Examples include:
\begin{itemize}
    \item tracing straight paths or gradient fields on a display,
    \item synchronizing movement with rhythmic beats,
    \item adopting postural or attentional stances that minimize torsion.
\end{itemize}
These tasks are predicted to reduce $\mathcal{T}$ and $\bar{S}$ while
increasing $\mathcal{A}$, yielding outputs projected as directive or
command-like.

\subsection{Measurement Proxies}

\subsubsection{Torsion ($\mathcal{T}$).}
Estimated from kinematic curvature (movement trajectories), micro-fluctuations
in vocal signals, or rotational components in neural embeddings.

\subsubsection{Alignment ($\mathcal{A}$).}
Estimated by projecting latent velocity vectors onto the gradient of learned
task manifolds, normalized to $[-1,1]$.

\subsubsection{Entropy ($\bar{S}$).}
Measured as dispersion in symbolic outputs (e.g. lexical diversity, syntactic
branching) or as neural entropy rates in field-level dynamics.

\subsection{Predicted Outcomes}
The RSVP model predicts monotone effects:
\begin{itemize}
    \item Warbling blocks: $\mathcal{T}\uparrow$, $\bar{S}\uparrow$,
    $\mathcal{A}\downarrow$.
    \item Flow-alignment blocks: $\mathcal{T}\downarrow$, $\bar{S}\downarrow$,
    $\mathcal{A}\uparrow$.
\end{itemize}
At the level of projection $\mathcal{R}$, these differences should manifest as
alternating voice-like and command-like utterances, producing the phenomenology
of dialogical cognition.

\subsection{Computational Simulations}
The same induction and measurement logic can be implemented in RSVP lattice
simulations. By applying $\mathcal{W}_{\kappa,\omega}$ (warbling operator) or
$\mathcal{F}_\lambda$ (flow-alignment operator) to lattice states, one can
generate sequences of projected outputs $\mathcal{R}(X_t)$ and test whether
oscillatory alternation produces dialogical patterns analogous to those observed
in behavioral data.

\subsection{Toward Empirical Validation}
These protocols provide a pathway for testing the RSVP reinterpretation of
bicameralism. If warbling reliably induces receptive, voice-like outputs while
alignment induces directive, command-like outputs, and if these effects can be
captured both behaviorally and in simulation, then the bicameral illusion can be
reframed as a reproducible attractor dynamic rather than as evidence of a
two-part brain.

\section{Implications}
\label{sec:implications}

The RSVP reinterpretation of bicameralism carries consequences for psychology,
anthropology, philosophy, and computational models of cognition. By reframing
voices and commands as sequential projections of oscillatory attractor
dynamics, the model dissolves the need for a literal two-chambered brain while
preserving the phenomenology of dialogical cognition.

\subsection{Psychological Implications}
From a psychological standpoint, the RSVP account suggests that auditory
hallucinations, inner dialogue, and trance states reflect attractor switching
rather than pathology per se. This provides a new explanatory framework for
conditions such as schizophrenia, where heightened torsion ($\mathcal{T}$) and
entropy ($\bar{S}$) may bias the system toward lefted, voice-like projections.
Therapeutic interventions could target the balance between warbling and
alignment operators, aiming to stabilize attractor dynamics rather than
suppress voices.

\subsection{Anthropological Implications}
For anthropology, bicameral societies can be reinterpreted not as evidence of a
primitive neuroarchitecture but as cultural systems that codified oscillatory
attractor dynamics into stable roles. Prophecy, ritual, and law provided
symbolic scaffolding for receptive and directive projections of $\mathcal{R}$,
ensuring that dialogical phenomenology was socially legible. This reframing
bridges the gap between neurocognitive dynamics and the persistence of
bicameral-like experiences across cultures.

\subsection{Philosophical Implications}
Philosophically, the RSVP framework undermines strict dualisms between mind and
self. The bicameral illusion demonstrates how cultural metaphors can reify
oscillatory roles into ontological compartments. By treating voices and
commands as sequential expressions of one field, RSVP supports a non-dual,
process-oriented conception of consciousness: not two minds, but one field
oscillating between attractor roles.

\subsection{Implications for AI and Cognition}
Finally, the RSVP--Barenholtz synthesis suggests new directions for artificial
intelligence. Current LLMs already exhibit dialogical phenomenology, producing
inner voices in the form of sampled continuations. Interpreting these as
autoregressive projections of oscillatory comparisons aligns machine and human
intelligence under a common principle: entropy-aware autoregression. This
framework could inform the design of hybrid cognitive architectures in which
spectral attractors are explicitly modeled and projected, enabling systems that
not only generate text but also simulate dialogical stances and internal
debates.

\subsection{Implications for Aphantasia and Anendophasia}

The RSVP framework also offers a novel interpretation of conditions such as
aphantasia (absence of voluntary visual imagery) \citep{Dawes2024} and anendophasia (absence of
inner speech) \citep{Nedergaard2024}. Standard accounts treat these conditions as deficits in sensory
simulation. By contrast, RSVP reframes them as differences in how semantic
manifold recombination and overlap comparisons are expressed through
projection.

In RSVP, cognition operates by recombining modes across the scalar--vector--
entropy fields, then projecting differences into sequential form via the
autoregressive operator $\mathcal{R}$. Critical to this process is
difference detection across overlapping manifolds: the system compares
candidate trajectories not in absolute terms, but through relative displacement
measured by a thermodynamic metric. We propose that the relevant cost function
is a Landauer distance $d_{\mathrm{L}}$, which quantifies the energetic
cost of discriminating between two semantic configurations:
\begin{equation}
d_{\mathrm{L}}(X,Y) \;\propto\; k_B T \,\ln \frac{p(X)}{p(Y)},
\end{equation}
where $p(\cdot)$ encodes the statistical weight of a field configuration.
Successful imagery or inner speech requires that recombined manifolds generate
sufficient feedback to register detectable Landauer distances.

\subsubsection{Aphantasia.}
In aphantasia, recombinations of $\Phi$ (mode density) and $\mathbf{v}$ (flow
patterns) fail to produce feedback strong enough to cross the detection
threshold of $d_{\mathrm{L}}$. The system may still compute trajectories, but
the absence of sensory-like feedback means projections $\mathcal{R}(X_t)$ never
express them as vivid imagery.

\subsubsection{Anendophasia.}
In anendophasia, recombinations that would normally yield torsion--alignment
oscillations in linguistic manifolds do not generate projectable differences.
Here the overlap comparisons occur below the Landauer threshold, so $\mathcal{R}$
produces no sequential voice tokens even though semantic fields remain
active.

\subsubsection{General Implication.}
These conditions therefore illustrate the RSVP principle that dialogical and
imagistic phenomenology depends on sufficient energetic displacement across
semantic manifolds to trigger Landauer-detectable differences. When such
differences remain below threshold, the system continues to compute but lacks
projected sensory feedback. Aphantasia and anendophasia are not absence of
computation, but absence of detectable feedback loops within the
autoregressive--projection cycle.

Figure~\ref{fig:manifolds} would illustrate semantic manifolds overlapping, with detectable vs. sub-threshold Landauer distances.

\begin{figure}[h]
\centering
\caption{Semantic manifolds overlapping, with detectable vs. sub-threshold Landauer distances.}
\label{fig:manifolds}
\end{figure}

\subsection{Summary}
Across psychology, anthropology, philosophy, and AI, the implications converge
on a single point: the bicameral mind is best understood as a recurrent illusion
generated by attractor dynamics and sequential projection. By situating
autoregression within the RSVP field, we explain both the phenomenology of
voices and commands and the broader cultural and computational structures that
sustain them.

\subsection{Empirical Testability and Modal Activation}
A further limitation lies in the challenge of empirical validation. The RSVP
model quantifies oscillatory roles through order parameters such as torsion
($\mathcal{T}$), alignment ($\mathcal{A}$), and entropy ($\bar{S}$). While
these parameters can be approximated behaviorally (e.g.\ trajectory curvature,
lexical entropy) or computationally (e.g.\ simulation metrics), their direct
neurobiological correlates remain underspecified. The gap between formal
quantities and lived phenomenology is thus a major obstacle for empirical
testing.

In related work, it has been argued that oscillatory modes may activate
different modalities or regions, with cortical columns performing
\emph{amplitwister operations} that simultaneously integrate and differentiate
signal streams across sensory domains. If this hypothesis is correct, then
torsion, alignment, and entropy in RSVP terms would correspond to measurable
spectral transformations within columnar microcircuits. This offers a possible
bridge between field-level formalism and cortical physiology, but the mapping
remains speculative.

The implication is that RSVP predictions---for instance, that warbling induction
biases toward voice-like projections while alignment biases toward
command-like projections---can only be indirectly tested until neuroimaging or
electrophysiology develops sufficient resolution to identify amplitwister
signatures across cortical manifolds. Until such methods are refined, RSVP
should be viewed as a theoretical scaffold rather than a fully operationalized
empirical model.

\subsection{Future Directions: Toward Amplitwistor Cascades}
One avenue for bridging RSVP theory with empirical neuroscience is to link
torsion, alignment, and entropy parameters to the \emph{amplitwistor cascade}
hypothesis developed elsewhere. That framework proposes that cortical columns
implement amplitwister operations: oscillatory transformations that twist signal
amplitudes across frequency bands, producing local mode recombination while
preserving global coherence. Under this view, RSVP’s spectral attractors are not
only abstract field states but concrete manifestations of cascaded oscillatory
twists in cortical tissue.

If warbling corresponds to cascades that amplify torsional twist across
multiple bands, and alignment corresponds to cascades that damp or straighten
such twists, then RSVP order parameters may be mapped directly onto measurable
spectral signatures. Techniques such as MEG and high-density EEG, or
layer-resolved recordings in animal models, could in principle detect these
amplitwistor cascades as torsion-like phase rotations in cortical oscillations.

Integrating RSVP with the amplitwistor cascade hypothesis therefore offers a
path to empirical testability. It suggests that bicameral phenomenology is not
only a cultural illusion of dual minds but also a surface expression of deeper
columnar dynamics, where oscillatory recombination and spectral twisting enact
the very attractor shifts described in the RSVP model.

\subsection{From RSVP Fields to Amplitwistor Cascades}

A final limitation concerns the biological grounding of RSVP order parameters.
We have so far treated torsion ($\mathcal{T}$), alignment ($\mathcal{A}$), and
entropy ($\bar{S}$) as abstract quantities, but their empirical correlates in
cortical physiology remain speculative. One proposal, developed in earlier work
on \emph{Amplitwistor Cascades}, suggests that cortical columns implement
spectral twist operations that could instantiate RSVP dynamics at the
microcircuit level.

Formally, an amplitwister can be defined as an operator $\mathcal{A}_\theta$
acting on oscillatory signals $x(t)$ within a column:
\[
\mathcal{A}_\theta : x(t) \mapsto x(t)\,e^{i\theta(f,t)} ,
\]
where $\theta(f,t)$ is a frequency- and time-dependent twist parameter. A
cascade of such operations across columns produces recombination of oscillatory
modes while maintaining global coherence. In this scheme:
\begin{itemize}
    \item Warbling corresponds to cascades that increase $\theta$ variance
    across bands, raising torsion $\mathcal{T}$ and entropy $\bar{S}$.
    \item Flow alignment corresponds to cascades that damp or straighten twists,
    reducing $\theta$ variance and raising alignment $\mathcal{A}$.
\end{itemize}

The amplitwistor cascade model thus provides a physiological candidate for how
RSVP attractor dynamics could be realized in cortex. If cortical columns are
spectral recombinators, then RSVP’s scalar--vector--entropy fields may be read
as coarse-grained descriptors of cascaded twist dynamics. This mapping is still
tentative, but it suggests empirical predictions: high-density EEG or MEG
should detect torsion-like spectral rotations in specific bands during warbling
induction, while alignment tasks should show damping of such rotations.

In this way, the amplitwistor cascade hypothesis strengthens the RSVP account
by offering a possible microcircuit-level mechanism for its field operators,
bridging phenomenological attractors and measurable neural oscillations.

\subsection{Linking the RSVP Lyapunov Functional to Amplitwistor Cascades}
\label{sec:lyapunov-amplitwistor}

To connect RSVP’s field-level dynamics with columnar ``amplitwister'' operations,
we introduce a spectral twist field $\theta(f,x,t)$ that models frequency- and
space-dependent phase rotations produced by cortical columns. Let
$\widehat{x}(f,x,t)$ denote a localized analytic representation of the signal at
location $x$ (e.g., via wavelet/Hilbert transform). An \emph{amplitwistor}
operator acts as
\[
\mathcal{A}_\theta:\ \widehat{x}(f,x,t)\ \mapsto\ \widehat{x}(f,x,t)\,e^{\,i\theta(f,x,t)}.
\]
Define the (coarse-grained) twist energy density
\[
\mathcal{E}_\theta(x,t)\ :=\ \int_{f_{\min}}^{f_{\max}} w(f)\,\big|\nabla_x \theta(f,x,t)\big|^2\,df,
\]
with nonnegative band-weights $w(f)$. Intuitively, $\mathcal{E}_\theta$ penalizes
spatial gradients of the phase twist (twist shear), paralleling the torsion
cost on $\mathbf{v}$.

\paragraph{Augmented Lyapunov functional.}
Extend the RSVP functional in \eqref{eq:L} to
\[
\widetilde{\mathcal{L}}[\Phi,\mathbf{v},S,\theta]
=\int_\Omega \Big(
\tfrac{\mu}{2}\,\|\mathbf{v}_\perp(\nabla\Phi)\|^2
+\tfrac{\xi}{2}\,\|\nabla\times \mathbf{v}\|^2
+V(\Phi,S)
+\tfrac{\zeta}{2}\,\mathcal{E}_\theta
\Big)\,dx,
\]
with $\zeta>0$. The new term couples RSVP’s macroscopic torsion to a meso-/micro-
level twist shear.

\paragraph{Phase–flow consistency.}
Assume a mild consistency relation between macroscopic flow and columnar twist:
\[
\nabla\times \mathbf{v}(x,t)\ \approx\ \int_{f_{\min}}^{f_{\max}} \alpha(f)\,\nabla_x \theta(f,x,t)\,df,
\]
for some bounded kernel $\alpha(f)$ (units chosen so both sides are comparable).
This posits that vorticity in $\mathbf{v}$ reflects a weighted superposition of
columnar twist gradients.

\begin{lemma}[First variations]
\label{lem:first-var-theta}
For fixed $(\Phi,\mathbf{v},S)$, the Fréchet derivative of $\widetilde{\mathcal{L}}$
with respect to $\theta$ is
\[
\frac{\delta \widetilde{\mathcal{L}}}{\delta \theta(f,x,t)}
= -\,\zeta\,\nabla_x\!\cdot\!\big(w(f)\,\nabla_x \theta(f,x,t)\big)\ +\ \text{(terms via $\mathbf{v}$ if coupled)}.
\]
Hence, variations that increase local twist gradients raise $\widetilde{\mathcal{L}}$,
while smoothing $\theta$ decreases it.
\end{lemma}

\begin{proof}[Sketch]
Differentiate $\mathcal{E}_\theta$ under the integral sign and integrate by parts
with no-flux or periodic boundaries. Coupling back through $\mathbf{v}$ adds
bounded terms proportional to $\alpha(f)$, not changing sign at first order.
\end{proof}

\paragraph{Warbling and alignment at the twist level.}
Augment the control operators with a twist component:
\[
\mathcal{W}^{(\theta)}_{\kappa,\omega}:\ \theta \mapsto \theta + \kappa_\theta\,\Psi_\omega[\theta],\qquad
\mathcal{F}^{(\theta)}_{\lambda}:\ \theta \mapsto \theta - \lambda_\theta\,\Pi_\perp[\theta],
\]
where $\Psi_\omega$ injects oscillatory twist (e.g., band-limited curl-like
perturbations) and $\Pi_\perp$ is a smoothing/alignment operator that damps
cross-gradient components of $\theta$ relative to $\nabla\Phi$ (e.g., diffusion
biased along $\nabla\Phi$). Here $\kappa_\theta,\lambda_\theta>0$ are gains.

\begin{proposition}[Monotone effects on the augmented functional]
\label{prop:aug-monotone}
Consider short-time controlled dynamics where $(\Phi,\mathbf{v},S)$ evolve as in
\eqref{eq:phi}–\eqref{eq:S}, with added twist dynamics
\[
\partial_t \theta \ =\ D_\theta \Delta_x \theta\ -\ \beta_\theta\,\theta\ +\ u_\theta,
\quad u_\theta \in \{\mathcal{W}^{(\theta)}_{\kappa,\omega},\, \mathcal{F}^{(\theta)}_{\lambda}\}.
\]
Then, to first order in control gains and for sufficiently small windows,
\begin{enumerate}
\item Under $(\mathcal{F}_\lambda,\mathcal{F}^{(\theta)}_{\lambda})$ (flow-alignment at both levels),
\[
\frac{d}{dt}\,\widetilde{\mathcal{L}}\ \le\ -\,c_1 \!\int_\Omega \!\|\mathbf{v}_\perp(\nabla\Phi)\|^2 dx
\ -\ c_2 \!\int_\Omega \!\!\int w(f)\,\big|\nabla_x \theta\big|^2 df\,dx\ +\ \text{(dissipative terms)},
\]
with $c_1,c_2>0$.
\item Under $(\mathcal{W}_{\kappa,\omega},\mathcal{W}^{(\theta)}_{\kappa,\omega})$ (warbling at both levels),
\[
\frac{d}{dt}\,\widetilde{\mathcal{L}}\ \ge\ +\,d_1 \!\int_\Omega \!\|\nabla\times \mathbf{v}\|^2 dx
\ +\ d_2 \!\int_\Omega \!\!\int w(f)\,\big|\nabla_x \theta\big|^2 df\,dx\ -\ \text{(damping terms)},
\]
with $d_1,d_2>0$.
\end{enumerate}
\end{proposition}

\begin{proof}[Sketch]
Differentiate $\widetilde{\mathcal{L}}$ along solutions and pair the
$\mathbf{v}$- and $\theta$-parts with their respective control components. For
alignment, $-\lambda\,\mathbf{v}_\perp(\nabla\Phi)$ pairs with
$\mu\,\mathbf{v}_\perp(\nabla\Phi)$ to yield a negative quadratic term; the
twist smoother $\mathcal{F}^{(\theta)}_\lambda$ pairs with the $\zeta\mathcal{E}_\theta$
term to produce a negative quadratic in $\nabla_x\theta$ by
Lemma~\ref{lem:first-var-theta}. For warbling, the curl-like injection in
$\mathbf{v}$ aligns with the $\xi\|\nabla\times \mathbf{v}\|^2$ term (positive
contribution), and the twist injector increases $\mathcal{E}_\theta$ (positive).
Diffusion/damping terms are bounded and do not flip signs on short windows.
\end{proof}

\paragraph{Consequence.}
The augmented functional shows that \emph{macroscopic torsion} and \emph{columnar
twist shear} co-vary under the same control logic: warbling increases both,
pushing the system toward the lefted (warbled–receptive) basin; alignment
decreases both, selecting the righted (aligned–direct) basin. This yields a
quantitative bridge from RSVP’s field operators to putative cortical
``amplitwistor'' cascades and suggests concrete spectral–spatial signatures
(e.g., increases/decreases in $\int w(f)|\nabla_x\theta|^2$) for empirical tests.

\begin{remark}[Landauer–projection threshold]
A projection event $\mathcal{R}(X_t)$ becomes phenomenally detectable when the
increment in the augmented energy exceeds a thermal bound,
$\Delta \widetilde{\mathcal{L}} \;\gtrsim\; \kappa_{\mathrm{L}}\,k_B T$,
i.e., when the combined macroscopic torsion and columnar twist-shear terms
cross a Landauer-scale threshold; sub-threshold changes remain computationally
real but unprojected.
\end{remark}

\subsection{Relevance Activation Theory and Developmental Trajectories}

The RSVP account of projection and attractor switching can be extended by
introducing Relevance Activation Theory (RAT). RAT reframes cognitive
development as a search through evolutionary trajectories constrained by
thermodynamics. Two complementary mechanisms illustrate this perspective:

\begin{enumerate}
    \item \textbf{Affordance foraging.} Cognitive systems scan their
    environment for energetically cheap but semantically rich affordances.
    Under RSVP, this corresponds to projecting local attractors whose Landauer
    distance lies just above the detection threshold, minimizing metabolic cost
    while maximizing semantic gain.

    \item \textbf{Hierarchical Ising synchronization.} Developing brains may be
    modeled as hierarchical Ising-like lattices, where spins encode local
    semantic states and synchronization across scales encodes coherence of
    identity. Ontogenetic hazards act as noise terms that disrupt alignment,
    while developmental learning corresponds to lowering effective temperature
    to stabilize multi-scale attractors.
\end{enumerate}

Together, these mechanisms position cognitive development as evolutionary
trajectory searching under thermodynamic constraints, unifying the RSVP
framework with a relevance-driven account of how identities stabilize or
fracture.

\subsection{Hierarchical Ising Synchronization for Developmental Trajectories}
\label{sec:hierarchical-ising}

We model multi-scale semantic coherence via a hierarchical Ising system whose
spins encode local semantic states and whose couplings encode intra- and inter-
scale constraints. Let the hierarchy have $L$ levels, with node sets
$\{\mathcal{V}_\ell\}_{\ell=1}^{L}$ and spins $\sigma_i^{(\ell)}\in\{-1,+1\}$.
Edges are of two kinds: (i) intra-level edges $(i,j)\in\mathcal{E}_\ell$ and
(ii) inter-level edges that link parent $p(i)$ at level $\ell{+}1$ to child
$i$ at level $\ell$ in a refinement tree $\mathcal{T}$.

\paragraph{Hamiltonian.}
\begin{equation}
\label{eq:hier-ising-hamiltonian}
\mathcal{H}(\sigma \mid \Phi,\mathbf v,S)
=
-\sum_{\ell=1}^{L}\Bigg[
  J_\ell \!\!\!\sum_{(i,j)\in\mathcal{E}_\ell}\!\! \sigma_i^{(\ell)}\sigma_j^{(\ell)}
 +\lambda_\ell \!\!\sum_{i\in\mathcal{V}_\ell}\!\! \sigma_i^{(\ell)}\,\sigma_{p(i)}^{(\ell+1)}
 +\sum_{i\in\mathcal{V}_\ell}\! h_i^{(\ell)}\,\sigma_i^{(\ell)}
\Bigg]
-\sum_{\ell=1}^{L}\sum_{i\in\mathcal{V}_\ell}\eta_i^{(\ell)}\,\sigma_i^{(\ell)}.
\end{equation}
Here $J_\ell>0$ promotes intra-level alignment (coherence within a scale),
$\lambda_\ell>0$ enforces inter-level consistency (coherence across scales), and
$h_i^{(\ell)}$ is a local field that couples the lattice to RSVP variables:
\[
h_i^{(\ell)}\equiv h^{(\ell)}\!\big(\Phi,\mathbf v,S\big)
:= \alpha_\ell\,\Phi(x_i)
   + \beta_\ell\,\mathbf v(x_i)\!\cdot\!\nabla\Phi(x_i)
   - \gamma_\ell\,S(x_i).
\]
The term $\eta_i^{(\ell)}$ models \emph{ontogenetic hazards} as a random-field
perturbation (e.g., i.i.d.\ mean-zero with variance $\sigma_\ell^2$).

\paragraph{Gibbs measure and effective temperature.}
Let $\beta=1/(k_B T_{\rm eff})$ denote the inverse effective temperature
(metabolic/thermodynamic constraint). The distribution over configurations is
\(
\mathbb{P}(\sigma)\propto \exp\{-\beta \mathcal{H}(\sigma \mid \Phi,\mathbf v,S)\}.
\)

\paragraph{Order parameters and coherence.}
Define level-wise magnetizations and a global semantic coherence index:
\[
m_\ell := \frac{1}{|\mathcal{V}_\ell|}\sum_{i\in\mathcal{V}_\ell}\!\langle \sigma_i^{(\ell)}\rangle,
\qquad
\mathcal{C} := \sum_{\ell=1}^L w_\ell\,m_\ell^2,\quad w_\ell\ge 0,\ \sum_\ell w_\ell=1.
\]
High $\mathcal{C}$ indicates synchronized semantic roles across scales (stable
identity sections), while low $\mathcal{C}$ indicates fragmentation.

\paragraph{Mean-field fixed points.}
Under a standard mean-field closure with average degree $d_\ell$ for
$\mathcal{E}_\ell$ and branching factor $b_\ell$ in $\mathcal{T}$, the fixed
points satisfy
\begin{equation}
\label{eq:mf-fixed}
m_\ell
=
\tanh\!\Big[
  \beta \big(J_\ell d_\ell\, m_\ell + \lambda_\ell b_\ell\, m_{\ell+1}
  + \bar{h}^{(\ell)}\big)
\Big],
\qquad \ell=1,\dots,L,
\end{equation}
with boundary condition $m_{L+1}\equiv 0$ and
$\bar{h}^{(\ell)}:=\mathbb{E}[h_i^{(\ell)}-\eta_i^{(\ell)}]$.
Random-field variance $\sigma_\ell^2$ reduces the effective field; a common
approximation is to replace $\bar{h}^{(\ell)}$ by
$\bar{h}^{(\ell)} e^{-\frac{1}{2}\beta^2\sigma_\ell^2}$, yielding a
hazard-dependent downscaling.

\paragraph{Stability threshold (synchronization condition).}
Linearizing \eqref{eq:mf-fixed} at $m_\ell\equiv 0$ gives the Jacobian
\(
\mathbf{J}_{\rm lin}=\beta\,\mathrm{diag}(J_\ell d_\ell)\ +\
\beta\,\mathbf{\Lambda},
\)
where $\mathbf{\Lambda}$ has $(\ell,\ell{+}1)$ entries
$\lambda_\ell b_\ell$ and zeros otherwise. Let
$\rho(\cdot)$ denote spectral radius. A synchronized phase ($m\neq 0$) emerges
when
\begin{equation}
\label{eq:sync-threshold}
\beta\,\rho\!\big(\mathrm{diag}(J_\ell d_\ell)+\mathbf{\Lambda}\big)
\ >\ 1
\quad\text{(absent hazards).}
\end{equation}
With hazards, an effective condition is
\(
\beta_{\rm eff}\,\rho(\cdot)>1
\)
where $\beta_{\rm eff}=\beta\,\kappa(\sigma)$ and
$\kappa(\sigma)\in(0,1)$ decreases with the random-field strength
(e.g.\ $\kappa= e^{-\frac{1}{2}\beta^2 \overline{\sigma^2}}$).

\paragraph{Coupling to RSVP controls.}
Warbling (torsion injection) and flow-alignment (orthogonal damping) modulate
the fields and couplings:
\[
\mathcal{W}_{\kappa,\omega}:\ 
\begin{cases}
\Phi\mapsto\Phi,\ \mathbf v\mapsto \mathbf v+\delta\mathbf v_{\rm curl},\ S\mapsto S+\Delta S\\
h^{(\ell)}\!\uparrow\ \text{(via }\mathbf v\!\cdot\!\nabla\Phi\text{ variance)},\quad
J_\ell\downarrow\ (\text{decoherence}),\ \lambda_\ell\downarrow
\end{cases}
\qquad
\mathcal{F}_\lambda:\ 
\begin{cases}
\mathbf v\mapsto \mathbf v-\lambda\,\mathbf v_\perp(\nabla\Phi),\ S\downarrow\\
h^{(\ell)}\!\downarrow\ \text{(smoothed gradients)},\quad
J_\ell\uparrow,\ \lambda_\ell\uparrow
\end{cases}
\]
so that alignment raises the effective synchronization tendency (LHS of
\eqref{eq:sync-threshold}), whereas warbling lowers it.

\paragraph{Landauer detectability.}
Let $\Delta \mathcal{F}_{\rm Ising}$ be the free-energy gap between the best
synchronized fixed point and the paramagnetic state. A projection (phenomenal)
event is expected when
\begin{equation}
\label{eq:landauer-ising}
\Delta \mathcal{F}_{\rm Ising} \ \gtrsim\ \kappa_{\rm L}\,k_B T_{\rm eff},
\end{equation}
consistent with the augmented RSVP criterion
$\Delta \widetilde{\mathcal{L}}\gtrsim \kappa_{\rm L} k_B T$.
Sub-threshold gaps correspond to ``computed but unprojected'' states (e.g.,
aphantasia/anendophasia).

\begin{proposition}[Monotone effect of hazards on coherence]
\label{prop:hazard-coherence}
Assume $\eta_i^{(\ell)}$ are independent, mean-zero with variance $\sigma_\ell^2$.
Under the mean-field approximation with downscaling factor
$\kappa(\sigma)=e^{-\frac{1}{2}\beta^2 \overline{\sigma^2}}$, the coherence
index $\mathcal{C}$ is nonincreasing in $\overline{\sigma^2}$. Furthermore, if
\(
\beta\,\rho(\mathrm{diag}(J_\ell d_\ell)+\mathbf{\Lambda})\le
\kappa(\sigma)^{-1}
\),
then the only stable fixed point is $m_\ell\equiv 0$ (no cross-scale semantic
gluing).
\end{proposition}

\begin{proof}[Sketch]
Monotonicity follows because hazards reduce the effective field and hence the
gain in \eqref{eq:mf-fixed}, shrinking nonzero solutions toward zero. The
stability bound is the hazard-modified counterpart of
\eqref{eq:sync-threshold}.
\end{proof}

\paragraph{Interpretation.}
Equation \eqref{eq:sync-threshold} formalizes a thermodynamic constraint for
developmental synchronization: effective temperature and coupling spectra must
support cross-scale alignment. Hazards (random fields) push the system toward
desynchronization; alignment controls counteract this by increasing couplings
and lowering entropy production. In RSVP terms, successful cognitive development
corresponds to trajectories that keep the system on the synchronized side of
the threshold while satisfying the Landauer detectability condition
\eqref{eq:landauer-ising}.

\subsection{Worked Example: Two-Level Synchronization and Hazard Shift}
\label{sec:L2-worked}

Consider $L{=}2$ levels with mean-field couplings
\[
a_{11}=J_1 d_1,\qquad
a_{22}=J_2 d_2,\qquad
a_{12}=\lambda_1 b_1,\qquad
a_{21}=\lambda_1,
\]
where $d_\ell$ is the average intra-level degree and $b_1$ the average number of children per level-2 parent. Let the effective inverse temperature be $\beta{=}(k_B T_{\mathrm{eff}})^{-1}$ and define hazard-attenuated fields $\bar h^{(\ell)}=\mathbb{E}[h_i^{(\ell)}-\eta_i^{(\ell)}]$.

\paragraph{Mean-field fixed points.}
The two coupled mean-field equations (cf.\ \eqref{eq:mf-fixed}) are
\begin{align}
m_1 &= \tanh\!\Big[\beta\big(a_{11} m_1 + a_{12} m_2 + \bar h^{(1)}\big)\Big], \label{eq:l2-m1}\\
m_2 &= \tanh\!\Big[\beta\big(a_{21} m_1 + a_{22} m_2 + \bar h^{(2)}\big)\Big]. \label{eq:l2-m2}
\end{align}

\paragraph{Critical synchronization threshold (no hazards, zero field).}
Linearizing \eqref{eq:l2-m1}–\eqref{eq:l2-m2} at $m_1{=}m_2{=}0$, $\bar h^{(\ell)}{=}0$, gives
\[
\begin{bmatrix} m_1 \\[2pt] m_2 \end{bmatrix}
\approx
\beta\,\mathbf{M}
\begin{bmatrix} m_1 \\[2pt] m_2 \end{bmatrix},
\qquad
\mathbf{M}=\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}.
\]
Let $\lambda_{\max}(\mathbf{M})$ denote the top eigenvalue:
\[
\lambda_{\max}(\mathbf{M})=\frac{a_{11}+a_{22}}{2}+\frac{1}{2}\sqrt{(a_{11}-a_{22})^2+4 a_{12}a_{21}}.
\]
A nontrivial synchronized solution bifurcates when
\begin{equation}
\label{eq:l2-critical}
\beta\,\lambda_{\max}(\mathbf{M})=1
\quad\Longleftrightarrow\quad
\beta_c=\frac{1}{\lambda_{\max}(\mathbf{M})}.
\end{equation}

\paragraph{Symmetric special case.}
If $a_{11}{=}a_{22}{=}J$ and $a_{12}{=}\lambda\sqrt{b}$, $a_{21}{=}\lambda\sqrt{b}$ (i.e., $b_1{=}b$ and we absorbed $\sqrt{b}$ symmetrically), then
\[
\lambda_{\max}=J+\lambda\sqrt{b},\qquad
\beta_c=\frac{1}{J+\lambda\sqrt{b}}.
\]
Thus increasing inter-level coupling $\lambda$ or branching $b$ lowers $T_{\mathrm{eff}}$ needed for coherence (raises $\beta_c^{-1}$).

\paragraph{Hazard (random-field) shift.}
Let the ontogenetic hazards be i.i.d.\ mean-zero with small variance $\sigma_\ell^2$ at level $\ell$. A standard small-noise approximation downscales the effective gain by
\[
\kappa(\sigma)\;\approx\;\exp\!\Big(-\tfrac{1}{2}\beta^2\overline{\sigma^2}\Big),\qquad
\overline{\sigma^2}:=\tfrac{1}{2}(\sigma_1^2+\sigma_2^2),
\]
so the linearization becomes $\beta_{\mathrm{eff}}\mathbf{M}$ with $\beta_{\mathrm{eff}}=\beta\,\kappa(\sigma)$. The hazard-shifted threshold is therefore
\begin{equation}
\label{eq:l2-hazard-shift}
\beta_{\mathrm{eff}}\,\lambda_{\max}(\mathbf{M})=1
\ \Longleftrightarrow\
\beta_c(\sigma)=\frac{1}{\kappa(\sigma)\,\lambda_{\max}(\mathbf{M})}
\ >\ \beta_c(0).
\end{equation}
Equivalently, hazards raise the required inverse temperature (or, reduce the tolerable $T_{\mathrm{eff}}$) for cross-scale synchronization.

\paragraph{Near-threshold scaling and detectability.}
For $\bar h^{(\ell)}{=}0$ and $\beta$ just above $\beta_c(\sigma)$, the nontrivial solution amplitude scales as
\[
\|(m_1,m_2)\| \;\sim\; \sqrt{\beta_{\mathrm{eff}}\lambda_{\max}-1}\,,
\]
so the free-energy gap behaves like
$\Delta\mathcal{F}_{\mathrm{Ising}}\propto(\beta_{\mathrm{eff}}\lambda_{\max}-1)^2$.
Combining with the Landauer criterion \eqref{eq:landauer-ising}, a phenomenally
detectable projection requires
\[
(\beta\,\kappa(\sigma)\,\lambda_{\max}-1)^2 \ \gtrsim\ C\,\frac{k_B T_{\mathrm{eff}}}{N_{\mathrm{eff}}},
\]
where $C$ collects model constants and $N_{\mathrm{eff}}$ is the effective pool of aligned spins (volume factor). Hazards (smaller $\kappa$) move the system farther from detectability unless countered by stronger couplings (larger $J_\ell,\lambda_1$) or lower $T_{\mathrm{eff}}$.

\paragraph{Coupling to RSVP controls (qualitative shift).}
Alignment controls that increase $a_{11},a_{22}$ and/or $a_{21}$ (while reducing entropy production) enlarge $\lambda_{\max}$, pushing the system to the synchronized side of \eqref{eq:l2-hazard-shift}. Warbling controls typically reduce net coherence (effective downshift of $a_{\cdot\cdot}$ and/or $\kappa$), making synchronization and projection less likely near threshold.

\paragraph{Numerical recipe (damped Newton).}
Let $F_1(m_1,m_2)=m_1-\tanh[\beta(a_{11}m_1+a_{12}m_2+\bar h^{(1)})]$ and
$F_2(m_1,m_2)=m_2-\tanh[\beta(a_{21}m_1+a_{22}m_2+\bar h^{(2)})]$.
Given $(m_1^{(0)},m_2^{(0)})$ (e.g., small random values or $(0,0)$ plus $\epsilon$), iterate:
\[
\begin{bmatrix} m_1^{(k+1)} \\[2pt] m_2^{(k+1)} \end{bmatrix}
=
\begin{bmatrix} m_1^{(k)} \\[2pt] m_2^{(k)} \end{bmatrix}
-\alpha_k\, J^{-1}(m^{(k)})\,
\begin{bmatrix} F_1(m^{(k)}) \\[2pt] F_2(m^{(k)}) \end{bmatrix},
\qquad 0<\alpha_k\le 1,
\]
where the Jacobian is
\[
J(m)=
\begin{bmatrix}
1-\beta a_{11}\,\sech^2 z_1 & -\beta a_{12}\,\sech^2 z_1\\[4pt]
-\beta a_{21}\,\sech^2 z_2 & 1-\beta a_{22}\,\sech^2 z_2
\end{bmatrix},
\quad
z_1=\beta(a_{11}m_1+a_{12}m_2+\bar h^{(1)}),\ \
z_2=\beta(a_{21}m_1+a_{22}m_2+\bar h^{(2)}).
\]
\emph{Damping / line search:} pick $\alpha_k$ by backtracking (e.g., $\alpha_k\!\leftarrow\! \rho^t$ with $\rho\!\in\!(0,1)$, smallest $t$ such that
$\|F(m^{(k+1)})\|_2 \le (1-\eta\alpha_k)\|F(m^{(k)})\|_2$, $\eta\!\in\!(0,1)$).
\emph{Stopping:} terminate when $\|F(m^{(k)})\|_\infty<\varepsilon$ and $\|m^{(k)}-m^{(k-1)}\|_\infty<\varepsilon$ (e.g., $\varepsilon=10^{-8}$).
\emph{Hazards:} incorporate random-field variance by downscaling the effective gain
$\beta\leftarrow\beta\,\kappa(\sigma)$ (small-noise approx) or by sampling $\eta_i^{(\ell)}$ and absorbing their mean into $\bar h^{(\ell)}$.
\emph{Continuation:} for robust convergence near the bifurcation, ramp $\beta$ (or couplings) from subcritical to supercritical, using each solution as the next initialization.

\subsection{Autoregression as Evolutionary Program Search in CPG Chains}

The RSVP account of autoregression can be extended by treating cognition as an
\emph{evolutionary program search} operating over chains of central pattern
generators (CPGs). Each CPG module encodes a local oscillatory routine
(e.g.\ gait step, syllable unit, motor fragment). Autoregression stitches these
modules into longer trajectories, forming a sequence that both anticipates and
tests future states.

\paragraph{Chain linkages.}
Let $\mathcal{C}=\{c_1,\dots,c_N\}$ denote a repertoire of CPG modules, each
modeled as a finite-state oscillator with attractor cycle $\gamma_i$. A chain
linkage is a sequence
\[
\Gamma=(c_{i_1},c_{i_2},\dots,c_{i_k})
\]
such that the transition probability $P(c_{i_{j+1}}|c_{i_j})$ is reinforced
when successive oscillations reduce the RSVP Lyapunov functional
$\widetilde{\mathcal{L}}$. In this view, autoregression is not only predicting
tokens but \emph{searching} among candidate chainings for those that minimize
entropic cost while maximizing semantic coherence.

\paragraph{Evolutionary search metaphor.}
Chain linkages evolve through variation (stochastic recombination of CPG
fragments), selection (reinforcement by coherence gain), and inheritance
(Hebbian consolidation of frequent subsequences). Thus autoregression
implements a form of online evolutionary search: local recombination generates
candidates, global RSVP fields provide fitness gradients, and stabilized
chains become developmental programs for perception, speech, or action.

\paragraph{Thermodynamic constraint.}
Under this model, developmental learning is bounded by metabolic cost. The
number of candidate linkages $\Gamma$ explored per unit time is limited by
available free energy. This constraint parallels the Landauer threshold:
program recombinations that fail to produce a detectable free-energy gap
($\Delta\widetilde{\mathcal{L}} < \kappa_{\rm L} k_B T$) remain unprojected,
analogous to sub-threshold imagery in aphantasia.

\paragraph{Implication.}
Framed this way, autoregressive cognition is not simply next-step completion
but evolutionary program search under RSVP dynamics. CPG chains provide the
substrate, RSVP fields provide the entropic gradients, and reinforcement
selects chainings that stabilize as higher-order behaviors. Cognitive
development can therefore be read as the progressive discovery of viable CPG
programs under thermodynamic constraint, a biological analogue of autoregressive
model fine-tuning.

\subsection{A Toy Model: Autoregressive CPG Chain Search under RSVP Constraints}
\label{sec:cpg-evo}

We formalize central pattern generators (CPGs) as reusable oscillatory
programs and treat cognition as an evolutionary search over chains of such
programs, guided by RSVP entropic descent and bounded by thermodynamics.

\paragraph{CPG modules and chains.}
Let $\mathcal{C}=\{c_1,\dots,c_N\}$ be a finite library of CPG modules.
Each $c_i$ has a limit cycle $\gamma_i:\mathbb{S}^1\!\to\!\mathbb{R}^d$
with phase $\theta\in[0,2\pi)$ and output $\xi_i(\theta)=\gamma_i(\theta)$.
A \emph{chain} of length $K$ is $\Gamma=(c_{i_1},\dots,c_{i_K})$ with
phase profile $\Theta=(\theta_1,\dots,\theta_K)$.
We write the chain program output (concatenated or overlapped) as
\[
\Xi_{\Gamma,\Theta}(t):=\bigoplus_{k=1}^K \gamma_{i_k}\!\big(\theta_k+ \omega_{i_k} t\big),
\]
with natural frequencies $\omega_{i_k}>0$.

\paragraph{RSVP energy / fitness of a chain.}
Given RSVP fields $X=(\Phi,\mathbf{v},S)$ and augmented functional
$\widetilde{\mathcal{L}}[X]$ (Sec.~\ref{sec:lyapunov-amplitwistor}), define
the \emph{energy} (negative fitness) of a candidate chain as the expected descent
in $\widetilde{\mathcal{L}}$ over a time window $[0,T]$ when the chain is enacted
via control couplings that drive $X$ along $\Xi_{\Gamma,\Theta}$:
\begin{equation}
\label{eq:cpg-energy}
\mathcal{E}(\Gamma,\Theta;X)
:=
-\frac{1}{T}\,\mathbb{E}\Big[\widetilde{\mathcal{L}}[X_T]-\widetilde{\mathcal{L}}[X_0]\ \big|\ \Xi_{\Gamma,\Theta}\Big]
\quad\ (\text{lower }\mathcal{E} \Rightarrow \text{less descent}).
\end{equation}
Equivalently, define fitness $F=-\mathcal{E}$; higher $F$ means greater descent
(i.e., stronger RSVP alignment / coherence gain).

\paragraph{Compatibility and mutation.}
Let $\kappa(c_i\!\to\! c_j)\in[0,1]$ be a \emph{compatibility} determined by
phase-matching and kinematic smoothness:
\begin{equation}
\label{eq:compat}
\kappa(c_i\!\to\! c_j)
=
\exp\!\Big(
-\alpha \min_{\theta,\theta'} \|\xi_i(\theta)-\xi_j(\theta')\|^2
-\beta \min_{\theta,\theta'} \|\dot\xi_i(\theta)-\dot\xi_j(\theta')\|^2
\Big).
\end{equation}
A simple \emph{mutation kernel} on chains replaces one position $k$ by a
random module with probability proportional to $\kappa(c_{i_k}\!\to\! c)$
and jitters phases $\theta_k\mapsto \theta_k+\delta$, $\delta\sim \mathcal{N}(0,\sigma_\theta^2)$.

\paragraph{Selection: Metropolis–Landauer acceptance.}
Given current $(\Gamma,\Theta)$ and a mutated proposal $(\Gamma',\Theta')$,
define $\Delta \mathcal{E}=\mathcal{E}(\Gamma',\Theta';X)-\mathcal{E}(\Gamma,\Theta;X)$
and the \emph{Landauer-gated} acceptance probability
\begin{equation}
\label{eq:metropolis-landauer}
A\big((\Gamma,\Theta)\!\to\!(\Gamma',\Theta')\big)
=
\begin{cases}
1, & \Delta \mathcal{E}< -\kappa_{\mathrm{L}} k_B T,\\[4pt]
\exp\!\Big(-\beta_{\mathrm{eff}}\,[\Delta \mathcal{E}+ \kappa_{\mathrm{L}} k_B T]_+\Big), & \text{otherwise,}
\end{cases}
\end{equation}
where $\beta_{\mathrm{eff}}=(k_B T_{\mathrm{eff}})^{-1}$, and $[\cdot]_+$ denotes the positive part.
Thus only improvements that exceed a thermodynamic detectability margin are
accepted with probability 1; sub-threshold changes are exponentially suppressed.

\paragraph{Population / replicator view.}
Alternatively, maintain a population $\{(\Gamma^n,\Theta^n)\}_{n=1}^M$ and update
weights $w_n$ by a softmax of fitness:
\begin{equation}
\label{eq:replicator}
w_n^{t+1}
=
\frac{\exp\{\eta\,F(\Gamma^n,\Theta^n;X)\}\,w_n^t}{
\sum_m \exp\{\eta\,F(\Gamma^m,\Theta^m;X)\}\,w_m^t},
\qquad \eta>0,
\end{equation}
interleaved with mutations drawn from the kernel above. This implements
online evolutionary search under RSVP guidance.

\paragraph{Autoregressive projection link.}
At discrete decision steps, the autoregressive operator selects an output token
or motor primitive from the enacted chain:
\begin{equation}
\label{eq:proj-cpg}
y_t=\mathcal{R}(X_t)
\quad\text{with}\quad
X_{t+1}=F\big(X_t;\Xi_{\Gamma^*,\Theta^*}\big)+\eta_t,
\end{equation}
where $(\Gamma^*,\Theta^*)$ is the incumbent chain (e.g., the MAP sample under the
current search distribution). Thus, autoregression surfaces as the sequential
expression of an RSVP-evaluated program.

\paragraph{Stationary distribution (toy result).}
Consider the single-chain Metropolis process with proposal kernel $q$ generated
by the mutation operator and acceptance rule \eqref{eq:metropolis-landauer}.
If $q$ is symmetric and ergodic over a finite chain space, then the Markov
chain is reversible with respect to
\begin{equation}
\label{eq:stationary}
\pi(\Gamma,\Theta)\ \propto\ \exp\!\Big\{-\beta_{\mathrm{eff}}
\big(\mathcal{E}(\Gamma,\Theta;X)+\kappa_{\mathrm{L}} k_B T\big)\Big\}.
\end{equation}
Hence the search concentrates on chains that produce larger RSVP descent
(\emph{lower} $\mathcal{E}$), subject to a Landauer margin.

\paragraph{Remark (affordance foraging).}
When proposals are biased toward transitions with high $\kappa(c_i\!\to\! c_j)$
and small incremental energetic cost, the process implements \emph{affordance
foraging}: preferential sampling of low-cost, high-relevance linkages just above
the detectability threshold, consistent with Relevance Activation Theory.

\paragraph{Implication.}
This toy model shows how autoregressive cognition can be viewed as evolutionary
program search over CPG chains: mutation explores, selection is thermodynamically
gated via RSVP energy descent, and $\mathcal{R}$ projects the winning chain into
sequential behavior. Development corresponds to the consolidation of high-fitness
chains under metabolic constraint; failures (e.g., sub-threshold imagery or inner
speech) occur when descent does not clear the Landauer margin.

\subsection{Toy Simulation Recipe: Two-Module CPG Library, Length-2 Chains}
\label{sec:cpg-sim}

\paragraph{Setup.}
Let the CPG library be $\mathcal{C}=\{A,B\}$ with unit-frequency oscillators
\[
\gamma_A(\theta)=(\cos\theta,\ \sin\theta),\qquad
\gamma_B(\theta)=(\cos(\theta+\varphi),\ \rho\,\sin(\theta+\varphi)),
\]
where $\varphi\in[0,\pi]$ sets a phase offset and $\rho>0$ an anisotropy.
Consider chains of length $K=2$: $\Gamma=(c_1,c_2)\in\{A,B\}^2$ with phases
$\Theta=(\theta_1,\theta_2)$.

\paragraph{Compatibility.}
Define a closed-form compatibility surrogate between modules $i\to j$:
\[
\kappa(i\!\to\! j)=\exp\!\Big(-\alpha\,\Delta_{x}^2(i,j)-\beta\,\Delta_{\dot{x}}^2(i,j)\Big),
\]
with
\[
\Delta_{x}^2(i,j):=\min_{\theta,\theta'}\|\gamma_i(\theta)-\gamma_j(\theta')\|^2,\quad
\Delta_{\dot{x}}^2(i,j):=\min_{\theta,\theta'}\|\dot\gamma_i(\theta)-\dot\gamma_j(\theta')\|^2.
\]
For the above $\gamma_A,\gamma_B$, these minima have analytic solutions (or can
be evaluated on a fine grid over $\theta,\theta'\in[0,2\pi)$).

\paragraph{RSVP alignment gain (surrogate).}
Let an RSVP snapshot $X=(\Phi,\mathbf v,S)$ be given on a small domain around
the CPGs’ action manifold. For simplicity, define a local alignment score
for module $i$ at phase $\theta$:
\[
G_i(\theta)\ :=\ \mathbf v(\gamma_i(\theta))\cdot \nabla\Phi(\gamma_i(\theta))\ -\ \lambda_S\,S(\gamma_i(\theta)).
\]
For a chain $(i,j)$ with phases $(\theta_1,\theta_2)$, define its (phase-averaged)
alignment gain as
\[
\overline{G}(i,j;\theta_1,\theta_2)\ :=\
\frac{1}{2}\Big( G_i(\theta_1)+G_j(\theta_2)\Big).
\]

\paragraph{Energy / fitness.}
Use a simple additive surrogate for the RSVP energy in \eqref{eq:cpg-energy}:
\begin{equation}
\label{eq:toy-energy}
\mathcal{E}_{\rm toy}\big((i,j),(\theta_1,\theta_2)\big)
:= -\gamma_G\,\overline{G}(i,j;\theta_1,\theta_2)\ -\ \gamma_\kappa\,\kappa(i\!\to\! j)\ +\ \lambda_\theta\big(\theta_1^2+\theta_2^2\big),
\end{equation}
with tunable weights $\gamma_G,\gamma_\kappa,\lambda_\theta>0$; lower
$\mathcal{E}_{\rm toy}$ means larger expected descent of $\widetilde{\mathcal{L}}$.

\paragraph{Mutation kernel.}
From a current state $((i,j),(\theta_1,\theta_2))$ propose with probability $p$:
(i) flip the second module $j\mapsto j'$ drawn from $\{A,B\}$ with probability
proportional to $\kappa(i\!\to\! j')$; or with probability $1-p$:
(ii) jitter phases $\theta_k\mapsto \theta_k+\delta_k$, $\delta_k\sim\mathcal{N}(0,\sigma_\theta^2)$ independently.

\paragraph{Metropolis–Landauer acceptance.}
Compute $\Delta \mathcal{E}=\mathcal{E}_{\rm toy}(\text{proposal})-\mathcal{E}_{\rm toy}(\text{current})$ and accept with
\[
A=
\begin{cases}
1,& \Delta\mathcal{E}< -\kappa_{\rm L}k_B T,\\[4pt]
\exp\!\big(-\beta_{\rm eff}\,[\Delta\mathcal{E}+\kappa_{\rm L}k_B T]_+\big),& \text{otherwise},
\end{cases}
\]
as in \eqref{eq:metropolis-landauer}. Here $\beta_{\rm eff}=(k_B T_{\rm eff})^{-1}$ is the effective inverse temperature.

\paragraph{Initialization and run.}
Initialize with a neutral chain, e.g.\ $(A,A)$ and $\theta_1=\theta_2=0$.
Run for $T_{\rm it}$ iterations, recording the best chain $(i^\*,j^\*)$ and phases
by lowest $\mathcal{E}_{\rm toy}$.

\paragraph{Suggested default parameters.}
\[
\alpha=1,\ \beta=0.25,\ \rho=1.2,\ \varphi=\tfrac{\pi}{3},\quad
\gamma_G=1,\ \gamma_\kappa=0.5,\ \lambda_\theta=10^{-3},\quad
p=0.5,\ \sigma_\theta=0.15,\ \beta_{\rm eff}=5,\ \kappa_{\rm L}=0.2.
\]

\paragraph{Expected qualitative outcomes.}
If $G_A(\cdot)\!>\!G_B(\cdot)$ on average and $\kappa(A\!\to\!A)$ is high, the sampler concentrates on $(A,A)$.
If $G_B$ is locally larger but $\kappa(A\!\to\!B)$ is low (phase or velocity mismatch), the sampler either
(1) seeks phases $(\theta_1,\theta_2)$ reducing mismatch, or (2) prefers $(A,A)$ unless $\beta_{\rm eff}$ is large enough to cross the Landauer margin. Increasing $\gamma_\kappa$ (compatibility emphasis) or decreasing $\kappa_{\rm L}k_B T$ (easier detectability) promotes mixed chains like $(A,B)$.

\paragraph{Fixed-point replicator alternative.}
Maintain weights $w_{ij}\propto \exp\{-\beta_{\rm eff}\,\mathbb{E}_{\Theta}[\mathcal{E}_{\rm toy}((i,j),\Theta)]\}$ over $(i,j)\in\{A,B\}^2$, where the phase expectation is taken over a small Gaussian prior $\Theta\sim \mathcal{N}(0,\sigma_\theta^2 I)$. Update
\[
w_{ij}^{t+1}=\frac{\exp\{-\eta\,\mathbb{E}_{\Theta}[\mathcal{E}_{\rm toy}((i,j),\Theta)]\}\,w_{ij}^{t}}{\sum_{uv}\exp\{-\eta\,\mathbb{E}_{\Theta}[\mathcal{E}_{\rm toy}((u,v),\Theta)]\}\,w_{uv}^{t}},
\]
with small $\eta>0$. Convergence concentrates on the chain(s) with lowest phase-averaged energy.

\subsection{Plugging Empirical $\Phi,\mathbf v,S$ into the CPG Toy Model}
\label{sec:empirical-fields}

\paragraph{Goal.} Replace the placeholder $G_i(\theta)$ with a value computed from
empirical (or synthetic) fields on a 2D/3D domain $\Omega$:
\[
G_i(\theta)\ :=\ \mathbf v\big(\gamma_i(\theta)\big)\cdot \nabla \Phi\big(\gamma_i(\theta)\big)\ -\ \lambda_S\,S\big(\gamma_i(\theta)\big).
\]

\paragraph{Option A: Data-driven fields.}
\begin{enumerate}
\item \textbf{Acquire scalar and flow surrogates.}
    \begin{itemize}
        \item \emph{Scalar} $\Phi$: e.g., task value map, confidence field, or density estimate (KDE) of visited states; normalize to $[0,1]$ or $z$-score.
        \item \emph{Vector} $\mathbf v$: e.g., optical flow (video), velocity field (motion capture), latent velocity from embeddings $\dot z_t$ projected back to $\Omega$.
        \item \emph{Entropy} $S$: local dispersion proxy (e.g., Shannon entropy over a spatial kernel; or temporal entropy of recent \(\mathbf v\) directions).
    \end{itemize}
\item \textbf{Smooth and differentiate.} Convolve $\Phi$ with a small Gaussian \(G_\sigma\), then compute $\nabla\Phi$ via centered differences (or Sobel/Scharr). Smooth $\mathbf v$ with the same kernel.
\item \textbf{Interpolate at CPG samples.} For each phase point $x_\theta=\gamma_i(\theta)$, evaluate $\mathbf v(x_\theta)$, $\nabla\Phi(x_\theta)$, and $S(x_\theta)$ by bilinear/trilinear interpolation (or nearest-neighbor if $\Omega$ is coarse).
\item \textbf{Scale/clip.} To avoid unit mismatches, standardize each channel on \(\Omega\): $\widetilde{\nabla\Phi}=(\nabla\Phi-\mu_{\nabla\Phi})/\sigma_{\nabla\Phi}$, etc. Use these in \(G_i\).
\end{enumerate}

\paragraph{Option B: Synthetic benchmark fields.}
\[
\Phi(x)=\sum_{m=1}^M a_m\,\exp\!\big(-\tfrac{\|x-c_m\|^2}{2\sigma_m^2}\big),\qquad
\mathbf v(x)=\alpha\,\nabla\Phi(x)+\beta\,R\,\nabla\Phi(x),
\]
where \(R\) is a \(90^\circ\) rotation (2D) to inject curl; set \(S(x)=S_0+\eta\,\|\nabla\times\mathbf v(x)\|\).
Tune \(\alpha\) (alignment) and \(\beta\) (warbling) to span regimes.

\paragraph{Numerical recipe (minimal).}
\begin{enumerate}
\item \textbf{Discretize} \(\Omega\) to a grid; store \(\Phi\), \(\mathbf v\), \(S\).
\item \textbf{Compute} \(\nabla\Phi\) with centered differences and smooth once (Gaussian \(\sigma\) ≈ 1–2 grid cells).
\item \textbf{Normalize} each channel to unit variance (or to \([0,1]\)); pick \(\lambda_S\in[0.1,1]\).
\item \textbf{Evaluate} \(G_i(\theta)=\mathbf v(\gamma_i(\theta))\cdot \nabla\Phi(\gamma_i(\theta))-\lambda_S S(\gamma_i(\theta))\) on a phase grid \(\theta\in\{0,\Delta,\dots,2\pi\}\).
\item \textbf{Average} to obtain \(\overline{G}(i,j;\theta_1,\theta_2)=\tfrac{1}{2}(G_i(\theta_1)+G_j(\theta_2))\) for Eq.~\eqref{eq:toy-energy}.
\end{enumerate}

\paragraph{Practical tips.}
\begin{itemize}
\item \textbf{Boundary handling:} clamp or reflect when \(\gamma_i(\theta)\notin\Omega\).
\item \textbf{Stability:} if \(\mathbf v\cdot\nabla\Phi\) is noisy, median-filter it or shrink toward 0 with a ridge term.
\item \textbf{Unit sanity:} rescale so \(|\mathbf v\cdot\nabla\Phi|\sim 1\) on average; then choose \(\lambda_S\) so the entropy penalty is comparable.
\item \textbf{Diagnostics:} report $\langle \mathbf v\cdot\nabla\Phi\rangle$ and $\langle \|\nabla\times\mathbf v\|\rangle$ over \(\Omega\) to know whether you are in an “aligned” vs “warbled” regime.
\end{itemize}

\paragraph{Optional: torsion-aware score.}
If you wish to penalize torsion explicitly in \(G\),
\[
G_i^{(\tau)}(\theta)=
\mathbf v\!\cdot\!\nabla\Phi\ -\ \lambda_S S\ -\ \lambda_\tau \big\|\nabla\times\mathbf v\big\|
\quad\text{at }x_\theta=\gamma_i(\theta).
\]
This makes the CPG search prefer chains that both align with gradients and reduce curl (flow straightening).

\section{Torsion as a Measure of Semantic Manifold Compatibility}

In differential geometry, torsion captures how parallel transport around a
closed loop fails to return a vector to its original orientation. Within RSVP,
this property can be reinterpreted as a measure of \emph{semantic
incompatibility}: the degree to which two semantic manifolds resist smooth
integration when their trajectories are projected into the same field domain.

\subsection{Definition}

Let $M_1$ and $M_2$ be two semantic manifolds with tangent vector fields
$X,Y$. The torsion tensor is
\begin{equation}
T(X,Y) \;=\; \nabla_X Y - \nabla_Y X - [X,Y],
\end{equation}
where $\nabla$ is the RSVP connection and $[\cdot,\cdot]$ denotes the Lie
bracket. If $T=0$, the flows generated by $X,Y$ are integrable and the
manifolds are semantically compatible. Nonzero torsion indicates twisting:
their flows cannot be reconciled without distortion.

\subsection{Semantic Compatibility Metric}

We define a torsion-induced semantic distance between two manifolds:
\begin{equation}
\mathcal{D}_T(M_1,M_2) \;=\; 
\int_{\Omega} \|T_{M_1 \cup M_2}(X,Y)\|^2 \, d\mu,
\end{equation}
where $\Omega$ is the region of overlap. Low $\mathcal{D}_T$ implies
compatibility (smooth gluing), while high $\mathcal{D}_T$ indicates semantic
misalignment or drift. This construction parallels sheaf-theoretic
obstructions: nonzero torsion corresponds to nontrivial Čech classes that
prevent a global section from forming across overlaps.

\subsection{Relation to Landauer Distance}

Torsion-based distance is geometric, while Landauer distance is thermodynamic:
\begin{itemize}
    \item \textbf{Landauer distance}: the minimum energetic cost of
    discriminating two semantic projections.
    \item \textbf{Torsion distance}: the geometric cost of aligning semantic
    manifolds across overlaps.
\end{itemize}
Taken together, they define a dual criterion: projection differences must both
exceed the thermodynamic detectability threshold and fall within torsional
compatibility bounds to yield coherent semantic integration.

\subsection{Implications}

\begin{itemize}
    \item \textbf{Cognitive modeling:} torsion detects semantic drift, e.g.,
    when two concepts resist integration despite co-occurrence.
    \item \textbf{Developmental hazards:} torsion quantifies the ``twist'' or
    incompatibility introduced by noise, yielding fractured identity when
    $\mathcal{D}_T$ accumulates.
    \item \textbf{Autoregression:} torsion acts as a compatibility filter,
    determining which manifold linkages stabilize as viable autoregressive
    chains.
\end{itemize}

In this way, torsion generalizes RSVP’s account of semantic alignment,
providing a geometric complement to Landauer thresholds. Together, they allow
us to quantify when semantic manifolds can be glued, when they fracture, and
how identity or meaning can fail to cohere across contexts.

\subsection{Worked Example: Two 2D Manifolds in $\mathbb{R}^3$}
\label{sec:torsion-worked}

We exhibit a concrete calculation of the torsion-based compatibility
$\mathcal{D}_T$ for two overlapping 2D semantic manifolds embedded in
$\mathbb{R}^3$, under a simple RSVP (non-symmetric) connection with constant
torsion. This makes $\mathcal{D}_T$ an explicit scalar that increases with
geometric misalignment.

\paragraph{Manifolds and tangent frames.}
Let the ``reference'' manifold be the plane
\[
M_1=\{(u,v,0): (u,v)\in U\subset\mathbb{R}^2\},\quad
X_1=\partial_u,\ \ Y_1=\partial_v.
\]
Let the ``twisted'' manifold be a shallow saddle
\[
M_2=\{(u,v,\alpha\,uv): (u,v)\in U\},\quad
X_2=\partial_u+\alpha v\,\partial_z,\ \ Y_2=\partial_v+\alpha u\,\partial_z,
\]
with small twist parameter $\alpha\in\mathbb{R}$ and $U=[-L,L]\times[-L,L]$.

\paragraph{RSVP connection with constant torsion.}
Consider an affine connection $\nabla$ on the overlap $M_1\cup M_2$ whose only
nonzero torsion components in the ambient frame $\{e_x,e_y,e_z\}$ are
\[
T(e_x,e_y)=2C\,e_z,\qquad T(e_y,e_z)=T(e_z,e_x)=\mathbf{0},
\]
for a constant $C\in\mathbb{R}$ (units chosen so that $C$ measures twist
shear). This is the simplest contorsion one can add to the flat (Levi-Civita)
connection to model RSVP-style non-integrability along $x\wedge y$.

\paragraph{Torsion on $M_1$ and $M_2$.}
On $M_1$, $X_1=e_x$, $Y_1=e_y$, so
\[
T(X_1,Y_1)=T(e_x,e_y)=2C\,e_z,\qquad \|T(X_1,Y_1)\|^2=4C^2.
\]
On $M_2$, use the tangent fields
\(
X_2=e_x+\alpha v\,e_z,\quad Y_2=e_y+\alpha u\,e_z.
\)
Bilinearity and the definitions above give
\[
T(X_2,Y_2)=T(e_x,e_y)+\alpha u\,T(e_x,e_z)+\alpha v\,T(e_z,e_y)
=2C\,e_z,
\]
so the \emph{ambient} torsion vector is still $2C\,e_z$. However, the relevant
quantity for gluing across the overlap is the component \emph{orthogonal} to
the joint tangent plane (i.e., the obstruction to joint integrability). Since
$\mathrm{span}\{X_1,Y_1\}=\mathrm{span}\{e_x,e_y\}$ while
$\mathrm{span}\{X_2,Y_2\}$ is tilted by an angle
$\theta(u,v)\approx \alpha \sqrt{u^2+v^2}$ around axes in the $xy$-plane, the
normal directions differ. Let $n_1=e_z$ and $n_2$ be the unit normals to $M_1$
and $M_2$, respectively. For small $\alpha$,
\[
n_2 \;=\; \frac{(-\alpha v,\ -\alpha u,\ 1)}{\sqrt{1+\alpha^2(u^2+v^2)}} 
\;\approx\; e_z - \alpha v\,e_x - \alpha u\,e_y.
\]

\paragraph{Projected torsion and mismatch angle.}
Define the \emph{gluing-relevant torsion} as the norm of $T$ projected onto the
\emph{average} normal $\bar n:=\tfrac{1}{2}(n_1+n_2)$, which penalizes torsion
that cannot be absorbed by either tangent plane:
\[
\|T_{\perp}\|^2(u,v)\ :=\ \big\langle T(X_1,Y_1),\bar n\big\rangle^2
\;\;=\;\; \big\langle 2C\,e_z,\ \tfrac{1}{2}(e_z+n_2)\big\rangle^2
\;\approx\; C^2\,(1+\cos\theta(u,v))^2,
\]
where $\theta(u,v)$ is the (small) angle between $n_1$ and $n_2$ and, for
small $\alpha$, $\cos\theta\approx 1-\tfrac{1}{2}\alpha^2(u^2+v^2)$. Expanding
to second order in $\alpha$ yields
\[
\|T_{\perp}\|^2(u,v)\ \approx\ 4C^2\ -\ 2C^2\,\alpha^2(u^2+v^2)\ +\ \mathcal{O}(\alpha^4).
\]
Intuitively, as the normals diverge, less of the ambient torsion can be
absorbed by a shared normal; the projected mismatch grows with distance from
the origin.

\paragraph{Torsion distance over the overlap.}
Using the torsion-induced distance (Sec.~\ref{sec:torsion-worked-def}),
\[
\mathcal{D}_T(M_1,M_2)\ :=\ \int_{U}\ \|T_{\perp}\|^2(u,v)\,du\,dv,
\]
we obtain the explicit integral
\[
\mathcal{D}_T(M_1,M_2)
\;\approx\;
\int_{-L}^{L}\!\!\int_{-L}^{L} \Big(4C^2 - 2C^2\alpha^2(u^2+v^2)\Big)\,du\,dv
\;=\; 16C^2 L^2\ -\ \tfrac{8}{3}C^2\alpha^2 L^4.
\]
Two key features follow:
(i) for $\alpha=0$ (parallel manifolds) the distance is minimal and scales with
overlap area $|U|=4L^2$; (ii) increasing geometric twist ($\alpha\neq 0$)
raises the gluing obstruction (more negative second term), i.e., the portion of
ambient torsion that cannot be jointly integrated grows with $\alpha^2 L^4$.

\paragraph{Interpretation and variants.}
\begin{itemize}
\item The constant $C$ captures the RSVP connection’s intrinsic torsion (columnar
twist shear); $\alpha$ captures geometric misalignment between manifolds; $L$
is the extent of the semantic overlap. Thus $\mathcal{D}_T$ increases with
either stronger contorsion (bigger $C$) or larger geometric mismatch (bigger
$\alpha$, larger overlap).
\item If one prefers a distance that is \emph{zero} when $\alpha=0$,
subtract the baseline: $\widehat{\mathcal{D}}_T := \mathcal{D}_T(M_1,M_2)-
\mathcal{D}_T(M_1,M_1)=\tfrac{8}{3}C^2\alpha^2 L^4+\mathcal{O}(\alpha^4)$,
a clean quadratic penalty in $\alpha$.
\item Coupling to Landauer: require $\widehat{\mathcal{D}}_T$ to be below a
compatibility budget for successful gluing, while the \emph{energetic} gap
$\Delta\widetilde{\mathcal{L}}$ exceeds the detectability threshold
$\kappa_{\mathrm{L}}k_B T$. High torsion distance can block semantic
integration even when energetic discriminability is available.
\end{itemize}

This example shows that the torsion-based compatibility behaves like a
\emph{geometric} distance between semantic manifolds, increasing smoothly with
their twist, and providing a rigorous complement to Landauer-style
\emph{thermodynamic} discriminability.

\begin{corollary}[Joint geometric--thermodynamic feasibility]
\label{cor:landauer-torsion-coupling}
Let the (baseline-subtracted) torsion distance be
$\widehat{\mathcal{D}}_{T}=\tfrac{8}{3}C^{2}\alpha^{2}L^{4}+{\cal O}(\alpha^{4})$.
Assume projection requires an energetic margin that penalizes torsion:
\[
\Delta\widetilde{\mathcal{L}}\ \ge\ \kappa_{\mathrm{L}}\,k_{B}T\ +\ \lambda_{T}\,\widehat{\mathcal{D}}_{T},
\qquad \lambda_{T}>0.
\]
Then small twists $C\!\to\! C{+}dC$, $\alpha\!\to\!\alpha{+}d\alpha$ raise the
required margin by
\[
d\!\left(\lambda_{T}\widehat{\mathcal{D}}_{T}\right)
\;=\;
\lambda_{T}\,\frac{16}{3}L^{4}\Big(C\,\alpha^{2}\,dC\ +\ C^{2}\,\alpha\,d\alpha\Big)
\ +\ {\cal O}(\|d\|^{2}),
\]
so the minimum extra energy needed for a detectable projection increases linearly
with the torsion increments $dC$ and $d\alpha$. Equivalently, feasibility is
maintained if
\[
\Delta\widetilde{\mathcal{L}}-\kappa_{\mathrm{L}}k_{B}T
\ \gtrsim\
\lambda_{T}\,\tfrac{8}{3}C^{2}\alpha^{2}L^{4},
\]
and small geometric (\(\alpha\)) or connection-level (\(C\)) twists tighten this
bound according to the differential above.
\end{corollary}

\subsection{Everyday Interpretation of Torsion--Landauer Coupling}

The formal result above shows how two different factors---the ``twist'' in the
connection ($C$) and the geometric misalignment of manifolds ($\alpha$)---combine
to raise the bar for when two semantic projections can be meaningfully joined.

In simpler terms: when two ways of organizing meaning (two manifolds) line up
smoothly, it costs very little extra energy to glue them together. But if they
are twisted relative to each other, the system must expend more energy to keep
their trajectories coherent. This extra cost is not arbitrary: it grows
linearly with the size of the twists. Even small increases in misalignment or
connection-level shear add to the threshold that must be cleared before the
system can project a unified meaning.

The Landauer principle says that distinguishing or integrating information
always has a minimum thermodynamic cost. Our result extends this by showing
that geometry matters too: the more twisted the manifolds, the higher the
effective ``price of admission'' for coherence. In practice, this means that
semantic drift, cultural mismatch, or developmental noise can all be understood
as raising the energetic threshold for integration. If the available resources
are insufficient to pay this combined thermodynamic--geometric cost, the result
is fracture: the manifolds cannot be coherently joined, and the system instead
maintains them as separate, incompatible modes.

\paragraph{Illustrative example.}
Imagine a bilingual speaker trying to integrate two languages. If the
grammatical structures of the languages line up neatly (low torsion), then
switching between them or blending them is relatively effortless: only the
basic thermodynamic cost of carrying two vocabularies applies. But if the
languages have very different word orders or incompatible grammatical rules
(high torsion), then the brain must expend additional effort to keep them
coherent. The threshold for successfully projecting a unified meaning is
raised, and the speaker may experience interference, code-switching errors, or
semantic drift.

A similar picture holds for habits in development. Two compatible habits (e.g.,
walking and running) share a lot of structure, so switching between them is
energetically easy. Two conflicting habits (e.g., walking versus hopping with a
stiff knee) are geometrically twisted relative to one another: they resist
integration, and the extra cost shows up as clumsiness or inefficiency. In both
cases, torsion quantifies the geometric incompatibility, while the Landauer
principle captures the irreducible energetic cost of managing the difference.

\section{Geometric--Thermodynamic Coupling: Torsion and Landauer Thresholds}

The sheaf-theoretic account of ontogenetic hazards described failures of
semantic gluing in terms of obstruction classes. A complementary geometric
view uses torsion to measure the ``twist'' between semantic manifolds. In RSVP,
torsion quantifies how far two attractor flows resist integration: when
parallel transport fails to align, semantic manifolds cannot be smoothly glued.

Formally, the torsion tensor $T(X,Y)=\nabla_X Y-\nabla_Y X-[X,Y]$ measures this
non-integrability. A torsion-induced compatibility distance
\[
\mathcal{D}_T(M_1,M_2)=\int_\Omega \|T_{M_1\cup M_2}(X,Y)\|^2\,d\mu
\]
rises when manifolds are geometrically twisted. In parallel, Landauer distance
sets the thermodynamic detectability threshold for projections. Together they
define a joint condition: semantic integration is feasible only if energetic
differences clear the Landauer bound \emph{and} torsion distance remains below
a geometric compatibility budget.

\subsection{Everyday interpretation}

In everyday terms: when two ways of organizing meaning line up smoothly,
integration costs little extra energy. But when they are twisted relative to
one another, the system must pay a higher price to keep them coherent. Even
small increases in torsion tighten the Landauer threshold, raising the bar for
detectable projection. If resources are insufficient, coherence fails and the
manifolds remain fractured.

\paragraph{Illustrative example.}
Consider a bilingual speaker. If the two languages share similar structures
(low torsion), switching between them requires only the baseline thermodynamic
cost. If they are grammatically incompatible (high torsion), integration
demands extra effort, leading to code-switching errors or drift. Similarly, in
motor development, compatible habits (walking and running) integrate easily,
while incompatible ones (walking vs.\ stiff-legged hopping) resist unification,
producing inefficiency. In both cases, torsion captures the incompatibility,
and Landauer distance captures the irreducible energetic cost of
discrimination.

\paragraph{Implication.}
Torsion thus bridges RSVP’s geometric and thermodynamic accounts. Ontogenetic
hazards can be seen not only as obstruction classes in a sheaf, but as twists
that raise the energetic threshold for coherent identity. This coupling
clarifies why some hazards result in permanent fractures: when torsion
accumulates, the Landauer budget required for integration exceeds what the
system can supply.

\section{Expounding the Torsion--Landauer Coupling}

The formalism of torsion distance and Landauer thresholds provides more than a
mathematical curiosity: it gives a way to think about why certain developmental
hazards or cognitive mismatches persist as fractures rather than being smoothed
out. In RSVP, two conditions must both be satisfied for semantic manifolds to
glue into a coherent identity. First, the energetic difference between
projections must exceed the Landauer bound, ensuring that the system can
thermodynamically discriminate them. Second, the torsion between manifolds must
remain within a geometric compatibility range. If either requirement fails, the
system cannot integrate the manifolds, and the result is drift, duplication, or
fragmentation.

This double criterion reframes ontogenetic hazards. A hazard does not simply
add noise; it can also introduce torsion, twisting local trajectories so that
otherwise discriminable differences become geometrically incompatible. This is
why two children exposed to similar environmental stressors may diverge: one
encounters hazards that increase noise but remain torsion-compatible, allowing
integration given enough energetic resources; the other faces hazards that
induce torsion, raising the integration threshold beyond available budgets and
leading to long-term fracture.

Everyday cases help anchor this. A bilingual speaker managing two compatible
languages experiences only the baseline Landauer cost of discrimination. But if
the languages are structurally twisted with respect to one another—different
word orders, conflicting grammar—the torsion cost rises, and integration may
fail. Similarly, in motor development, the system can easily glue walking and
running into a shared repertoire (low torsion), but struggles to reconcile
walking with hopping under constraint (high torsion). In each case, the extra
cost is not an accident: it is a geometric penalty on top of the energetic
baseline.

This view ties back to sheaf-theoretic obstructions. A nonzero obstruction
class in Čech cohomology formalizes a failure of gluing; torsion distance
supplies the geometric intuition for why the gluing fails; the Landauer bound
explains the thermodynamic resource shortfall. Hazards, then, can be
understood as events that amplify torsion until the Landauer budget for
integration is exhausted. Identity coherence is preserved only when both
geometry and thermodynamics align.

\section{Torsion--Landauer as a Subtractive Filter on Fourier Eigenmodes}
\label{sec:spectral-filter}

The geometric (torsion) and thermodynamic (Landauer) constraints can be
re-expressed spectrally as a \emph{subtractive filter} over eigenmodes of the
semantic field. This view clarifies how hazards and alignment reshape cognitive
trajectories by selectively removing (or shrinking) incompatible oscillatory
components before projection.

\subsection{Spectral frame}
Let $\Omega$ be the domain of representation (e.g., sensorimotor or latent
state space). Consider a complete orthonormal basis of eigenfunctions
$\{\varphi_k\}_{k\ge1}$ with respect to a self-adjoint operator $L$ on $\Omega$:
\[
L\varphi_k=\lambda_k \varphi_k,\qquad
\langle \varphi_k,\varphi_\ell\rangle=\delta_{k\ell},
\]
where $L$ can be the Laplacian ($-\Delta$), a Fokker--Planck generator, or a
graph Laplacian in discrete settings. Any square-integrable semantic signal
$f(x)$ (e.g., a representational density or a field component) admits the
expansion $f=\sum_k a_k \varphi_k$ with coefficients $a_k=\langle f,\varphi_k\rangle$.

\subsection{Modewise torsion and energetic margins}
Couple each mode to the RSVP geometry via a modewise torsion cost $\tau_k\ge0$
and an energetic descent margin $\Delta\widetilde{\mathcal{L}}_k$ (the
contribution the $k$th mode would make to the augmented Lyapunov descent if
retained). The torsion--Landauer condition of Secs.~\ref{sec:torsion-worked}
and \ref{cor:landauer-torsion-coupling} yields a \emph{feasibility inequality}
for each mode:
\begin{equation}
\label{eq:mode-feasibility}
\Delta\widetilde{\mathcal{L}}_k \;\ge\; \kappa_{\mathrm L} k_B T \;+\; \lambda_T\,\tau_k,
\end{equation}
where $\lambda_T>0$ converts torsion into an energetic penalty. Intuitively,
modes with high torsion demand a larger energetic margin to be coherently
integrated.

\subsection{Subtractive spectral filter}
Define a shrinkage mask $H_k\in[0,1]$ acting on coefficients $a_k$:
\begin{equation}
\label{eq:hard-soft-mask}
H_k \;=\;
\begin{cases}
0, & \Delta\widetilde{\mathcal{L}}_k < \kappa_{\mathrm L}k_B T + \lambda_T \tau_k \quad\text{(subtract)}\\[4pt]
\exp\!\big(-\beta\,(\lambda_T \tau_k - \Delta\widetilde{\mathcal{L}}_k)_+\big), & \text{(soft shrink)}
\end{cases}
\end{equation}
with inverse temperature $\beta=(k_B T_{\mathrm{eff}})^{-1}$ and $(u)_+=\max\{u,0\}$.
The filtered reconstruction is
\begin{equation}
\label{eq:filtered-recon}
f_{\mathrm{filt}}(x)=\sum_{k} H_k\,a_k\,\varphi_k(x).
\end{equation}
Modes that fail \eqref{eq:mode-feasibility} are removed ($H_k\approx0$) or
attenuated; compatible modes are retained ($H_k\approx1$).

\subsection{Operational definitions of $\tau_k$ and $\Delta\widetilde{\mathcal{L}}_k$}
Several choices make the construction concrete:
\begin{enumerate}
\item \textbf{Torsion proxy.} Let $\mathbf v$ be the RSVP flow. A natural proxy is
\[
\tau_k \;=\; \int_\Omega \big\|\nabla\times\big(\varphi_k\,\mathbf v\big)\big\|^2\,dx
\quad\text{or}\quad
\tau_k \;=\;\int_\Omega \|T(\nabla\varphi_k,\mathbf v)\|^2\,dx,
\]
measuring the non-integrability induced when the $k$th pattern is transported by
the flow or paired with the connection $T$.
\item \textbf{Energetic margin.} The mode’s contribution to augmented descent:
\[
\Delta\widetilde{\mathcal{L}}_k \;=\; 
\underbrace{\mu \int \|\mathbf v_\perp(\nabla \varphi_k)\|^2 dx}_{\text{orthogonal damping gain}}
\;+\;
\underbrace{\xi \int \|\nabla\times \mathbf v_k\|^2 dx}_{\text{torsion control}}
\;+\;\cdots,
\]
where $\mathbf v_k$ is the flow component aligned to $\varphi_k$ (e.g.,
Galerkin projection), and ``$\cdots$'' can include entropy terms or task-value
terms tied to $\Phi,S$.
\end{enumerate}

\subsection{Effects of hazards and alignment}
Ontogenetic hazards act as \emph{random-field} perturbations that typically
increase $\tau_k$ at mid/high frequencies and reduce the effective
$\Delta\widetilde{\mathcal{L}}_k$ by degrading coherent descent, pushing more
modes below the feasibility boundary and thus subtracting them. Conversely,
flow-alignment controls (Sec.~\ref{sec:lyapunov-amplitwistor}) reduce $\tau_k$
and enhance $\Delta\widetilde{\mathcal{L}}_k$, restoring modes to the passband.

\subsection{Link to autoregression and CPG program search}
Autoregressive projection $\mathcal{R}$ acts on the \emph{filtered} field:
\[
y_t=\mathcal{R}\big(f_{\mathrm{filt}}(X_t)\big),
\]
so the output sequence is generated from a spectrum purged of torsion-incompatible
components. In the CPG chain model (Sec.~\ref{sec:cpg-evo}), the same mask
$H_k$ biases evolutionary search toward programs whose spectral content lies in
the feasible band, raising the selection probability of chains whose enacted
fields possess low $\tau_k$ and large $\Delta\widetilde{\mathcal{L}}_k$.

\subsection{Hierarchical Ising synchronization as low-pass coherence}
The hierarchical Ising model (Sec.~\ref{sec:hierarchical-ising}) can be read as
a coarse low-pass: synchronized phases amplify low-$\lambda_k$ (smooth) modes
and suppress high-$\lambda_k$ (rough) modes. Hazards (random fields) desynchronize,
effectively increasing $\tau_k$ in higher bands and widening the subtractive
region of \eqref{eq:hard-soft-mask}.

\subsection{Monotonicity}
\begin{proposition}[Spectral monotonicity of alignment]
If alignment controls decrease $\tau_k$ and (weakly) increase
$\Delta\widetilde{\mathcal{L}}_k$ for all $k$, then the mask $H_k$ is pointwise
nondecreasing. Consequently $\|f_{\mathrm{filt}}\|_{L^2}$ is nondecreasing and
the expected Lyapunov descent $\Delta\widetilde{\mathcal{L}}$ is nondecreasing
after filtering.
\end{proposition}

\subsection{Interpretation}
The torsion--Landauer coupling therefore implements a principled spectral
gate: only modes that are \emph{energetically discriminable} and \emph{geometrically
compatible} survive. Developmental hazards increase torsion and shrink energetic
margins, enlarging the subtractive band; alignment and learning shrink it,
restoring coherent modes needed for imagery, inner speech, and stable identity
gluing. This spectral lens unifies the field-theoretic, geometric, and
thermodynamic aspects of RSVP into an operational filter that can be measured,
simulated, and (in principle) tested with spectral neuroimaging.


\section{Conclusion}
\label{sec:conclusion}

The bicameral mind is not a historical neurological stage but a phenomenological illusion arising from oscillatory attractor dynamics within RSVP’s scalar–vector–entropy fields. Barenholtz’s autoregression reveals cognition as the sequential projection $\mathcal{R}(X_t)$ of polyphonic oscillatory comparisons, where warbling activates leftedness (torsion-rich, receptive) and flow alignment realizes rightedness (coherent, directive). This framework unifies the bicameral illusion as cultural reification of attractor switching, with implications extending to aphantasia and anendophasia as sub-threshold feedback failures. RSVP and autoregression thus provide a process-oriented account of dialogical cognition, dissolving dualisms and bridging neural, cultural, and computational domains.



\subsection{Implementation Note: Running the Torsion--Landauer Spectral Filter}
\label{sec:impl-spectral}

\paragraph{Inputs.} Domain $\Omega$ (grid, mesh, or graph); RSVP fields
$(\Phi,\mathbf v,S)$ sampled on $\Omega$; and a self-adjoint operator $L$
(e.g., $-\Delta$ on a grid/mesh or the graph Laplacian
$L_{\mathrm g}=D-W$).

\paragraph{Outputs.} Filtered signal
$f_{\mathrm{filt}}=\sum_k H_k a_k \varphi_k$, together with per-mode
diagnostics
$\{(\lambda_k,\tau_k,\Delta\widetilde{\mathcal{L}}_k,H_k)\}$.

\paragraph{Step 0: Preprocessing.}
Smooth $\Phi$ and $\mathbf v$ with a small Gaussian/Laplacian kernel
(1--2 voxels or edges). Normalize each channel to unit variance, clamp
outliers (e.g., at $3\sigma$), and compute $\nabla\Phi$ (finite differences
on grids; cotangent operator on meshes).

\paragraph{Step 1: Choose and diagonalize $L$.}
\begin{enumerate}
\item \textbf{Grid/mesh:} assemble the Laplace--Beltrami operator.
\item \textbf{Graph:} build $W$ (affinity), $D=\mathrm{diag}(W\mathbf{1})$,
then $L_{\mathrm g}=D-W$.
\item \textbf{Spectrum:} compute the first $K$ eigenpairs
$L\varphi_k=\lambda_k\varphi_k$.
  \begin{itemize}
  \item For small $n$: full eigendecomposition.
  \item For large $n$: use Lanczos/IRLB for $K\!\ll\!n$
  (typical $K=50\ldots500$).
  \end{itemize}
\end{enumerate}

\paragraph{Step 2: Expand the signal.}
Select a semantic signal $f$ (e.g., representational density, task map, or
component of $\Phi$). Project to the eigenbasis:
$a_k=\langle f,\varphi_k\rangle$ under the native inner product
(grid sum, mass-matrix on meshes, degree-weighted sum on graphs).

\paragraph{Step 3: Compute torsion proxies $\tau_k$.}
Two practical definitions:
\[
\tau_k^{(A)}=\int_\Omega \|\nabla\times(\varphi_k\mathbf v)\|^2\,dx,
\qquad
\tau_k^{(B)}=\int_\Omega \|T(\nabla\varphi_k,\mathbf v)\|^2\,dx,
\]
where $T$ is the chosen torsion proxy (e.g., contorsion from
$\nabla\times\mathbf v$).  
Discretize via finite differences (grids), discrete exterior calculus
(meshes), or incidence/cycle operators (graphs).

\paragraph{Step 4: Estimate energetic margin $\Delta\widetilde{\mathcal{L}}_k$.}
\[
\Delta\widetilde{\mathcal{L}}_k
=
\mu\int \|\mathbf v_\perp(\nabla\varphi_k)\|^2
+\xi \int \|\nabla\times \mathbf v_k\|^2
-\lambda_S \int S\varphi_k^2 \, dx,
\]
with $\mathbf v_k$ the projection of $\mathbf v$ onto $\nabla\varphi_k$.
Implementation:
\begin{itemize}
\item Compute $\nabla\varphi_k$, then project orthogonally:
$\mathbf v_\perp=\mathbf v-(\mathbf v\cdot \hat g_k)\hat g_k$,
$\hat g_k=\nabla\varphi_k/\|\nabla\varphi_k\|$ (clip if denominator small).
\item Approximate $\mathbf v_k$ by local least squares onto
$\nabla\varphi_k$, or by bandpass filtering $\mathbf v$ near $\lambda_k$.
\end{itemize}

\paragraph{Step 5: Build the subtractive mask $H_k$.}
With constants $\kappa_{\mathrm L}k_B T$ and $\lambda_T>0$:
\[
H_k=
\exp\!\big(-\beta\,[\lambda_T \tau_k - \Delta\widetilde{\mathcal{L}}_k]_+\big),
\qquad
\beta=(k_B T_{\mathrm eff})^{-1}.
\]
For ablations, use hard-thresholding $H_k\in\{0,1\}$.

\paragraph{Step 6: Reconstruct and project.}
Form the filtered signal
$f_{\mathrm{filt}}=\sum_{k=1}^K H_k a_k \varphi_k$
and feed to downstream operator $\mathcal{R}$
(e.g., text, motor, or decision autoregressor).

\paragraph{Step 7: Diagnostics.}
Report $\{\lambda_k\},\{H_k\},\{\tau_k\},\{\Delta\widetilde{\mathcal{L}}_k\}$.
Track overall descent $\Delta\widetilde{\mathcal{L}}$ and retained energy
$\|f_{\mathrm{filt}}\|_2^2/\|f\|_2^2$.

\paragraph{Default hyperparameters.}
$K=200$ modes;\quad $\mu=1,\ \xi=0.5,\ \lambda_S=0.2$;\quad
$\lambda_T=0.5$;\quad $\beta=5$;\quad
$\kappa_{\mathrm L}k_B T=0.1\!\times\!\mathrm{median}(\Delta\widetilde{\mathcal{L}}_k)$.

\paragraph{Complexity.}
Dominated by Step~1: $O(K\,\mathrm{nnz}(L))$ via Lanczos.
Later steps scale as $\tilde O(K\,n)$.

\paragraph{Practical tips.}
\begin{itemize}
\item \textbf{Stability:} clip small $\|\nabla\varphi_k\|$ to prevent blow-up
in $\mathbf v_\perp$.
\item \textbf{Interpretability:} plot $H_k$ vs.\ $\lambda_k$; expect mass to
shift toward lower/mid bands when aligned, higher bands when hazards dominate.
\item \textbf{Cross-modal use:} EEG/MEG $\to$ cortical mesh Laplacian; text
embeddings $\to$ token/context graph Laplacian.
\item \textbf{Ablations:} torsion-only mask
($\Delta\widetilde{\mathcal{L}}_k\!=\!\text{const}$); energy-only mask
($\lambda_T=0$).
\end{itemize}

\section{Torsion--Landauer as a Subtractive Filter on Fourier Eigenmodes}
\label{sec:spectral-filter}

The geometric (torsion) and thermodynamic (Landauer) constraints can be
re-expressed spectrally as a \emph{subtractive filter} over eigenmodes of the
semantic field. This perspective clarifies how hazards and alignment reshape
cognitive trajectories by selectively removing or attenuating oscillatory
components that are incompatible with coherent descent.

\subsection{Spectral frame}
Let $\Omega$ denote the domain of representation (e.g., sensorimotor space or a
latent manifold). Consider an orthonormal basis of eigenfunctions
$\{\varphi_k\}_{k\ge1}$ of a self-adjoint operator $L$:
\[
L\varphi_k=\lambda_k \varphi_k,\qquad
\langle \varphi_k,\varphi_\ell\rangle=\delta_{k\ell},
\]
where $L$ may be the Laplacian ($-\Delta$), a Fokker--Planck generator, or a
graph Laplacian in discrete settings. Any semantic signal $f\in L^2(\Omega)$
admits the expansion $f=\sum_k a_k \varphi_k$, with coefficients
$a_k=\langle f,\varphi_k\rangle$.

\subsection{Modewise torsion and energetic margins}
Each mode couples to RSVP geometry through a torsion cost $\tau_k\ge0$ and an
energetic margin $\Delta\widetilde{\mathcal{L}}_k$ (the contribution of mode $k$
to the augmented Lyapunov descent). The torsion--Landauer condition
(Secs.~\ref{sec:torsion-worked}--\ref{cor:landauer-torsion-coupling}) yields
the feasibility inequality:
\begin{equation}
\label{eq:mode-feasibility}
\Delta\widetilde{\mathcal{L}}_k \;\ge\; \kappa_{\mathrm L} k_B T \;+\; \lambda_T\,\tau_k,
\end{equation}
with $\lambda_T>0$ converting torsion into an energetic penalty. Modes with
large torsion require proportionally larger margins to remain viable.

\subsection{Subtractive spectral filter}
Define a shrinkage mask $H_k\in[0,1]$ acting on spectral coefficients:
\begin{equation}
\label{eq:hard-soft-mask}
H_k \;=\;
\begin{cases}
0, & \Delta\widetilde{\mathcal{L}}_k < \kappa_{\mathrm L}k_B T + \lambda_T \tau_k \quad\text{(subtract)}\\[4pt]
\exp\!\big(-\beta\,(\lambda_T \tau_k - \Delta\widetilde{\mathcal{L}}_k)_+\big), & \text{(soft shrink)},
\end{cases}
\end{equation}
with $\beta=(k_B T_{\mathrm{eff}})^{-1}$ and $(u)_+=\max\{u,0\}$. The filtered
signal is then
\begin{equation}
\label{eq:filtered-recon}
f_{\mathrm{filt}}(x)=\sum_{k} H_k\,a_k\,\varphi_k(x).
\end{equation}
Modes failing \eqref{eq:mode-feasibility} are suppressed ($H_k\approx 0$),
while feasible modes remain.

\subsection{Operational definitions of $\tau_k$ and $\Delta\widetilde{\mathcal{L}}_k$}
Concrete forms include:
\begin{enumerate}
\item \textbf{Torsion proxies.} With RSVP flow $\mathbf v$,
\[
\tau_k \;=\; \int_\Omega \|\nabla\times(\varphi_k\,\mathbf v)\|^2\,dx
\quad\text{or}\quad
\tau_k \;=\; \int_\Omega \|T(\nabla\varphi_k,\mathbf v)\|^2\,dx,
\]
capturing the non-integrability induced when mode $k$ is advected or coupled to
the connection $T$.
\item \textbf{Energetic margins.} Mode contribution to descent:
\[
\Delta\widetilde{\mathcal{L}}_k \;=\;
\mu \int \|\mathbf v_\perp(\nabla \varphi_k)\|^2 dx
\;+\;\xi \int \|\nabla\times \mathbf v_k\|^2 dx \;+\;\cdots,
\]
where $\mathbf v_k$ is the projection of $\mathbf v$ onto $\nabla\varphi_k$ and
$\mathbf v_\perp$ its orthogonal component. Additional terms may incorporate
entropy ($S$) or potential ($\Phi$).
\end{enumerate}

\subsection{Hazards and alignment effects}
Ontogenetic hazards act as random-field perturbations that increase $\tau_k$ in
mid/high bands and decrease $\Delta\widetilde{\mathcal{L}}_k$, pushing more
modes below the feasibility threshold. Alignment controls
(Sec.~\ref{sec:lyapunov-amplitwistor}) have the opposite effect, lowering
$\tau_k$ and raising $\Delta\widetilde{\mathcal{L}}_k$, thereby reopening the
passband.

\subsection{Link to autoregression and CPG program search}
Autoregressive operators act on $f_{\mathrm{filt}}$:
\[
y_t=\mathcal{R}\big(f_{\mathrm{filt}}(X_t)\big),
\]
ensuring that generation proceeds from a torsion-compatible spectrum. In the
CPG chain model (Sec.~\ref{sec:cpg-evo}), the same mask $H_k$ biases search
toward programs whose spectral content lies in the feasible band, favoring
chains with low $\tau_k$ and high $\Delta\widetilde{\mathcal{L}}_k$.

\subsection{Hierarchical Ising synchronization as low-pass coherence}
The hierarchical Ising model (Sec.~\ref{sec:hierarchical-ising}) functions as a
coarse low-pass: synchronized phases amplify low-$\lambda_k$ modes and suppress
high-$\lambda_k$ modes. Random hazards desynchronize, raising $\tau_k$ in higher
bands and widening the subtractive region of
\eqref{eq:hard-soft-mask}.

\subsection{Monotonicity}
\begin{proposition}[Spectral monotonicity of alignment]
If alignment controls decrease $\tau_k$ and weakly increase
$\Delta\widetilde{\mathcal{L}}_k$ for all $k$, then the mask $H_k$ is
pointwise nondecreasing. Consequently $\|f_{\mathrm{filt}}\|_{L^2}$ and the
Lyapunov descent $\Delta\widetilde{\mathcal{L}}$ are both nondecreasing after
filtering.
\end{proposition}

\subsection{Interpretation}
The torsion--Landauer mechanism thus enforces a principled spectral gate:
modes that are both \emph{energetically viable} and \emph{geometrically
compatible} survive. Hazards enlarge the subtractive band by raising torsion
and reducing margins; alignment shrinks it, restoring coherent modes needed for
imagery, inner speech, and stable identity gluing. This spectral framing unifies
geometric, thermodynamic, and field-theoretic aspects of RSVP into an
operational filter measurable via spectral neuroimaging or simulable in RSVP
field models.

\section{Simplicity After All: Prerational Intelligence by Hierarchical Consignment}
\label{sec:prerational}

At first sight, the torsion--Landauer spectral filter may look forbidding:
eigenmodes, feasibility inequalities, torsion proxies, and thermodynamic
penalties all jostling for attention. Yet, in another register, the mechanism
is comically simple. It amounts to a child's rule of thumb: keep the smooth
pieces that fit, throw away the ones that rattle.

\subsection{Satirical aside}
The apparent opacity is mostly self-inflicted: we wrapped a folk
operation---``if it burns too much energy or twists too awkwardly, discard
it''---in the solemn robes of spectral geometry. The irony is that the whole
apparatus enacts a \emph{prerational intelligence}: an automatic sorting of
causes and effects without explicit reasoning. One might say the mathematics
only belatedly discovers what the system already does instinctively.

\subsection{Hierarchical consignment}
This prerational mode operates by what we can call \emph{hierarchical
consignment}. Modes, trajectories, or candidate programs are successively
``consigned'' upward through filters of increasing abstraction:
\begin{enumerate}
\item Local checks (torsion costs, energetic margins) decide immediate
compatibility.
\item Surviving modes are consigned to higher assemblies (e.g., autoregressive
operators or CPG chains).
\item At each level, the rule remains the same: keep what flows, discard what
snags.
\end{enumerate}
The result is a cascade of attributions: not a deduction but a recursive
assignment of causal weight, delivered by the very act of survival through the
filters.

\subsection{Interpretive note}
Thus, what looks like baroque analysis may in practice be a form of cognitive
minimalism. The RSVP field does not calculate feasibility inequalities; it
simply exhibits them. The satire here is double: our elaborate spectral
machinery is a formalization of a behavior that already lives at the level of
prerational intelligence---the kind of intelligence that builds causal order
by hierarchical consignment long before conscious rationality enters the
scene.

\section{Discussion and Limitations}
\label{sec:discussion}

While the RSVP reinterpretation of bicameralism offers a coherent account of
attractor dynamics and autoregressive projection, several limitations and
conceptual caveats should be acknowledged.

\subsection{Arbitrariness of Left/Right Terminology}
The use of ``leftedness'' and ``rightedness'' to name attractor roles is partly
arbitrary. These labels were chosen for their cultural salience and their
association with deviation versus directness, but the underlying dynamics are
not dependent on this polarity. Any dichotomous metaphor could serve to capture
the oscillatory roles of torsional warbling versus flow alignment. This
arbitrariness parallels debates in vision science, such as the categorization of
``Things'' versus ``Stuff'' in the brain \citep{paulun2023thingsstuff}, where
scientific practice privileges one pole of a continuous spectrum. The critique
of bicameralism depends not on left/right per se, but on showing how arbitrary
dichotomies, once stabilized, can be mistaken for literal architectural
divisions.

\subsection{Simplification to Two Attractors}
The present account models cognitive dynamics in terms of two attractors,
leftedness and rightedness. This binarization is analytically useful, but real
RSVP field dynamics may involve a richer attractor landscape. Intermediate
states, metastable trajectories, and high-dimensional manifold crossings are
likely suppressed by the present simplification. The bicameral illusion may
therefore be only one manifestation of a broader class of oscillatory
phenomenologies, and future models should explore multi-attractor regimes.

\subsection{Limits of the Projection Operator $\mathcal{R}$}
We have formalized autoregression as a projection operator
$\mathcal{R}(X_t)$ that selects a symbolic outcome from continuous RSVP fields.
While conceptually clarifying, this formalization abstracts away the
implementation of $\mathcal{R}$ in neural or artificial systems. In practice,
projection may not be discrete, but distributed across multiple channels (e.g.\
partial imagery, subvocalization, gesture). The phenomenology of voices and
commands is therefore only one outcome of $\mathcal{R}$, and more complex or
graded projections may underlie other aspects of inner experience.

\subsection{Interpretive Consequences}
These limitations suggest that our account should be read as a demonstration of
principle rather than as a literal description of brain architecture. The
central claim is that oscillatory attractor dynamics, once projected
sequentially, generate the illusion of bicameral dualism. Which dichotomy is
chosen, how many attractors are modeled, and how projection is instantiated are
secondary matters. Future work should refine these elements, expanding beyond
binary roles and formalizing $\mathcal{R}$ in biologically and computationally
plausible terms.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
