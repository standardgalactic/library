Neuron-Astrocyte Memory Model
Skip to content
Open sidebar
Get Plus
Saved memory full
Saved memory full
Get Plus
ChatGPT
Share
You said:
Neuron���astrocyte associative memory 
Leo Kozachkova,b ID, Jean-Jacques Slotinea,cID, and Dmitry Krotovd,1ID 
Af f iliations are included on p. 7. 
Edited by Wesley P. Clawson, Allen Discovery Center at Tufts University, Medford, MA; received September 11, 2024; accepted April 7, 2025 
by Editorial Board Member Michael S. Gazzaniga 
Astrocytes, the most abundant type of glial cell, play a fundamental role in memory. 
Despite most hippocampal synapses being contacted by an astrocyte, there are no 
current theories that explain how neurons, synapses, and astrocytes might collectively 
contributetomemory function.Wedemonstratethat fundamentalaspectsofastrocyte 
morphology and physiology naturally lead to a dynamic, high-capacity associative 
memory system. The neuron���astrocyte networks generated by our framework are 
closely related to popular machine learning architectures known as Dense Associative 
Memories. Adjusting the connectivity pattern, the model developed here leads to a 
family of associative memory networks that includes a Dense Associative Memory 
and a Transformer as two limiting cases. In the known biological implementations of 
Dense Associative Memories, the ratio of stored memories to the number of neurons 
remains constant, despite the growth of the network size. Our work demonstrates 
that neuron���astrocyte networks follow a superior memory scaling law, outperforming 
known biological implementations of Dense Associative Memory. Our model suggests 
an exciting and previously unnoticed possibility that memories could be stored, at least 
in part, within the network of astrocyte processes rather than solely in the synaptic 
weights between neurons. 
neuron���astrocyte network|associative memory|dynamical 
system 
Not all brain cells are neurons. It is estimated that about half of the cells in the human 
brain are glial cells (from ���glue��� in Greek) (1). Glial cells have long been known to 
play an important role in homeostatic brain functions, such as regulating blood l ow 
(2)���thus contributing to hemodynamic signals such as those measured in fMRI (3)��� 
and removing synaptic debris. Converging lines of recent evidence strongly suggest that 
they are also directly involved in learning, memory, and cognition (4���10). Among glial 
cells, astrocytes are particularly important for brain function. They serve a crucial role in 
directly sensing neural activity and, in turn, regulating synaptic strength and plasticity 
(4, 5, 11���14). In addition to sensing neural activity, astrocytes are also important targets 
of neuromodulatory signals such as norepinephrine and acetylcholine emerging from 
potentially distant brain structures such as the brainstem (15). 
Of particular relevance to the computational neuroscience community are the recent 
i ndings that 1) astrocytes are necessary for forming and retrieving long-term memories 
(i.e., by participating in engrams) (6, 16���20) and 2) astrocytes respond to neural activity 
on timescales spanning many orders of magnitude, from several hundred milliseconds to 
minutes (14, 21, 22). Despite extensive evidence establishing the importance of neuron��� 
astrocyte interactions for long-term memory function, computational theories of these 
interactions are still in their infancy. 
1.1. What Shapes Astrocytic Computation?The core proposal of this paper is that 
astrocytes compute, and these computations are shaped by tunable signaling pathways 
within astrocytes. We will be primarily concerned with associative computations: How 
neurons, synapses, and astrocytes work together to store and retrieve memories. In 
this case, astrocytic Ca2+f l ux coeff i cients are the site of memory storage, and neuron��� 
synapse���astrocyte interactions are the mechanism of memory retrieval. This proposal 
harmoniously extends decades of prior work suggesting that memories are stored in 
synaptic weights (23, 24) and provides a perspective where synaptic weights ���emerge��� 
from interactions between neurons and astrocytes. Note that our proposal remains 
consistent with the engram hypothesis, in emphasizing that neural activation during 
a learning event is essential for subsequent memory recall (25, 26). Indeed, our work 
closely aligns with recent experimental i ndings indicating that astrocytes collaborate 
with neurons to store and retrieve memories through engram representations (20). 
Signif i cance 
Recent experiments have 
challenged the belief that glial 
cells, which compose at least half 
of brain cells, are just passive 
support structures. Despite this, 
a clear understanding of how 
neurons and glia work together 
for brain function is missing. To 
close this gap, we present a 
theory of neuron���astrocytes 
networks for memory processing, 
using the Dense Associative 
Memory framework. Our findings 
suggest that astrocytes can serve 
as natural units for implementing 
this network in biological 
���hardware.��� Astrocytes enhance 
the memory capacity of the 
network. This boost originates 
from storing memories in the 
network of astrocytic processes, 
not just in synapses, as 
commonly believed. These 
process-to-process 
communications likely occur in 
the brain and could help explain 
its impressive memory 
processing capabilities. 
Author contributions: L.K. and D.K. designed research; 
L.K., J.-J.S., and D.K. performed research; and L.K., J.-J.S., 
and D.K. wrote the paper. 
The authors declare no competing interest. 
This article is a PNAS Direct Submission. W.P.C. is a guest 
editor invited by the Editorial Board. 
Copyright��2025 the Author(s). Published by PNAS. 
This open access article is distributed under Creative 
Commons Attribution License 4.0 (CC BY). 
1To whom correspondence may be addressed. Email: 
krotov@ibm.com. 
This article contains supporting information online 
athttps://www.pnas.org/lookup/suppl/doi:10.1073/pnas. 
2417788122/-/DCSupplemental. 
Published May 23, 2025. 
PNAS2025Vol. 122No. 21e2417788122https://doi.org/10.1073/pnas.24177881221 of 7 
Downloaded 
from https://www.pnas.org 
by 159.2.135.155 
on 
June 
10, 
2025 
from 
IP 
address 
159.2.135.155. 
2. Neuron���Astrocyte Model 
Astrocytes have a primary cell body (soma) with numerous 
branching processes that envelope nearby synapses (Fig. 1). This 
three-part structure is known as the tripartite synapse (27). A 
single astrocyte can form over 106tripartite synapses (28), and 
astrocyte networks spatially tile the brain, forming nonoverlap- 
ping ���islands��� (29). Astrocyte processes detect neurotransmitters 
in the synaptic cleft, leading to an upsurge in intracellular free 
calcium Ca2+ions within the astrocyte process. This leads to 
a biochemical cascade in the astrocyte, potentially culminating 
in the release of gliotransmitters back into the synaptic cleft, 
inf l uencing neural activity���a closed feedback loop. Astrocyte 
processes can intercommunicate through calcium transport (30), 
andindividualastrocytesconnectviagapjunctions.Theinterplay 
betweenneuronsandastrocytes,spanningmultipletemporaland 
spatial scales, underscores the relevance of astrocytes in learning 
andmemory.Forthispaper,wewillfocusonthefollowingsalient 
aspects of astrocyte biology: 
��� A single astrocyte can connect to millions of nearby synapses, 
forming three-part connections (astrocyte process, presynaptic 
neuron, postsynaptic neuron) called tripartite synapses (27). 
��� Astrocytes detect neural activity and respond by regulating this 
activity through the release of gliotransmitters (31). 
��� Tripartite synapses can interact with each other, possibly 
through astrocytic intracellular calcium transport (30). 
2.1. Neural Dynamics.The above points may be formalized into 
a set of dynamical equations governing the time evolution of 
neurons, synapses, and astrocytes. The membrane voltage xifor 
each neuron i evolves according to a standard rate recurrent 
neural network model (32, 33) with the characteristic time scale 
of the neural dynamics ���n, and the leak rate ��� 
���n�� xi= ������ xi+ 
N X 
j=1 
g(sij) ���(xj) + bi[1] 
Each neuron has an input bi, which establishes the neuron���s 
baseline activation. The nonlinearity ���(xj) transforms neural 
membranevoltagesintof i ringrates,whilethetermg(sij)indicates 
the strength of the synaptic weight connecting neurons i and j. 
Thevariablesijisdynamicandalters dependingontheactivityof 
AB 
Fig. 1.(A) An abstracted version of an astrocyte, showing the astrocyte 
processes and the synapses. (B) Our mathematical idealization of the 
minicircuit defined by a single astrocyte. 
Box 1. 
ExamplesofpossibleLagrangianfunctions. 
Here, the variable z is an arbitrary dynamical 
variable in our model (e.g., astrocyte calcium 
level). Recall from the main text that activation 
functions are def i ned from the Lagrangians as 
���L 
���zi. The i rst Lagrangian provides an example 
of a ���collective��� activation function. The second 
Lagrangian leads to an element-wise activa- 
tion function, assuming ���Q 
���zi = q(zi). Generally, 
the only mathematical requirement for our La- 
grangians is that they must be convex functions. 
Examples of Possible Lagrangians and Activations 
L(z) = log 
N X 
i=1 ezi��� ���L(z) 
���z = Softmax(z) 
L(z) = 
N X 
i=1 Q(zi)��� ���L(z) 
���z
ChatGPT said:
This paper���
���Neuron���astrocyte associative memory��� by Kozachkov, Slotine, and Krotov (2025,
PNAS
)
���presents a groundbreaking computational model that places
astrocytes
, long relegated to passive support roles in the brain, at the center of
dynamic, high-capacity associative memory
. Here���s a clear breakdown and key implications:
����
Core Ideas
1.
Astrocytes as Active Memory Units
Traditionally
: Memory is thought to be stored in
synaptic weights
between neurons.
In this model
: Astrocytes���through
Ca����� flux dynamics
���play a central role in
storing
and
modulating
memory. The astrocytic processes interacting with tripartite synapses are not just passive enablers, but
computational components
.
2.
Tripartite Synapse as a Computational Unit
Each memory "bit" involves:
A
presynaptic neuron
A
postsynaptic neuron
An
astrocytic process
enveloping the synapse
This forms a
tripartite unit
, where astrocytic Ca����� activity responds to neurotransmitter release, integrates information, and can
modulate synaptic strength
via gliotransmitters.
Astrocytes intercommunicate via
intracellular calcium waves
, suggesting
spatially distributed
memory processing.
3.
Mathematical Framework: Dense Associative Memories (DAMs)
The authors develop a neuron���astrocyte system that generalizes the
Dense Associative Memory
(DAM) architecture���a known high-capacity memory model.
Key features:
DAMs can store and retrieve
patterns
robustly by
energy minimization
in a dynamical system.
The model extends to
Transformer-like architectures
depending on the connectivity���suggesting a
unified memory-computation landscape
spanning biological and machine intelligence.
4.
Superior Scaling Laws
Biological DAM implementations scale linearly
, meaning that memory capacity grows proportionally with the number of neurons.
Neuron���astrocyte networks scale super-linearly
, implying
greater-than-expected memory capacity
as the network grows.
This is because
astrocytic processes can form millions of synaptic interactions
, creating
combinatorially rich, non-local memory associations
beyond what synapse-only models permit.
����
Model Components
���� Neuronal Dynamics:
Rate-based neural model:
��
n
x
��
i
=
���
��
x
i
+
���
j
=
1
N
g
(
s
i
j
)
��
(
x
j
)
+
b
i
\tau_n \dot{x}_i = -\lambda x_i + \sum_{j=1}^N g(s_{ij}) \phi(x_j) + b_i
��
n
���
x
��
i
���
=
���
��
x
i
���
+
j
=
1
���
N
���
g
(
s
ij
���
)
��
(
x
j
���
)
+
b
i
���
x
i
x_i
x
i
���
: membrane voltage of neuron
i
i
i
s
i
j
s_{ij}
s
ij
���
: synaptic weight (dynamic, activity-dependent)
��
\phi
��
: activation function
b
i
b_i
b
i
���
: baseline input
���� Astrocytic Dynamics:
Astrocyte processes:
Detect synaptic neurotransmitter release
Increase internal
Ca
2
+
\text{Ca}^{2+}
Ca
2
+
Release gliotransmitters back to neurons
Communicate with each other through calcium waves and gap junctions
These dynamics are encoded using
Lagrangian-based activation functions
for generality, including:
Softmax
from log-sum-exp Lagrangians
Element-wise activations
from separable convex potentials
����
Implications
1.
Memories May Be Stored in Astrocytic Networks
Not just synaptic weights, but the
calcium state-space
and
inter-process dynamics
of astrocytes may
encode long-term memory patterns
.
2.
Biology as High-Capacity Associative Hardware
Astrocytes appear optimized to implement architectures similar to those in
deep learning
:
Massive parallelism
Dynamical recurrence
Gradient-like integration via biochemical signaling
3.
Synaptic Weights May Be Emergent
Rather than being the
primary
locus of memory, synaptic weights may emerge from
deeper neuron���astrocyte dynamics
���flipping the classical paradigm.
���� Takeaways
Concept
Traditional View
This Paper
Memory Storage
Synaptic weights
Astrocytic Ca����� + synapse interactions
Memory Scaling
Linear with neuron count
Super-linear via astrocyte processes
Astrocyte Role
Support
Active computation and storage
Model Type
Recurrent neural net / DAM
DAM + Transformer generalization with Lagrangian activations
If you'd like, I can help you:
Diagram the neuron���astrocyte interaction model
Derive the full dynamical system from the equations
Explore implications for AI memory architectures
Compare with engram theory or Predictive Processing models
Would you like to go deeper in one of those directions?
You said:
2. Neuron���Astrocyte Model 
Astrocytes have a primary cell body (soma) with numerous 
branching processes that envelope nearby synapses (Fig. 1). This 
three-part structure is known as the tripartite synapse (27). A 
single astrocyte can form over 106tripartite synapses (28), and 
astrocyte networks spatially tile the brain, forming nonoverlap- 
ping ���islands��� (29). Astrocyte processes detect neurotransmitters 
in the synaptic cleft, leading to an upsurge in intracellular free 
calcium Ca2+ions within the astrocyte process. This leads to 
a biochemical cascade in the astrocyte, potentially culminating 
in the release of gliotransmitters back into the synaptic cleft, 
inf l uencing neural activity���a closed feedback loop. Astrocyte 
processes can intercommunicate through calcium transport (30), 
andindividualastrocytesconnectviagapjunctions.Theinterplay 
betweenneuronsandastrocytes,spanningmultipletemporaland 
spatial scales, underscores the relevance of astrocytes in learning 
andmemory.Forthispaper,wewillfocusonthefollowingsalient 
aspects of astrocyte biology: 
��� A single astrocyte can connect to millions of nearby synapses, 
forming three-part connections (astrocyte process, presynaptic 
neuron, postsynaptic neuron) called tripartite synapses (27). 
��� Astrocytes detect neural activity and respond by regulating this 
activity through the release of gliotransmitters (31). 
��� Tripartite synapses can interact with each other, possibly 
through astrocytic intracellular calcium transport (30). 
2.1. Neural Dynamics.The above points may be formalized into 
a set of dynamical equations governing the time evolution of 
neurons, synapses, and astrocytes. The membrane voltage xifor 
each neuron i evolves according to a standard rate recurrent 
neural network model (32, 33) with the characteristic time scale 
of the neural dynamics ���n, and the leak rate ��� 
���n�� xi= ������ xi+ 
N X 
j=1 
g(sij) ���(xj) + bi[1] 
Each neuron has an input bi, which establishes the neuron���s 
baseline activation. The nonlinearity ���(xj) transforms neural 
membranevoltagesintof i ringrates,whilethetermg(sij)indicates 
the strength of the synaptic weight connecting neurons i and j. 
Thevariablesijisdynamicandalters dependingontheactivityof 
AB 
Fig. 1.(A) An abstracted version of an astrocyte, showing the astrocyte 
processes and the synapses. (B) Our mathematical idealization of the 
minicircuit defined by a single astrocyte. 
Box 1. 
ExamplesofpossibleLagrangianfunctions. 
Here, the variable z is an arbitrary dynamical 
variable in our model (e.g., astrocyte calcium 
level). Recall from the main text that activation 
functions are def i ned from the Lagrangians as 
���L 
���zi. The i rst Lagrangian provides an example 
of a ���collective��� activation function. The second 
Lagrangian leads to an element-wise activa- 
tion function, assuming ���Q 
���zi = q(zi). Generally, 
the only mathematical requirement for our La- 
grangians is that they must be convex functions. 
Examples of Possible Lagrangians and Activations 
L(z) = log 
N X 
i=1 ezi��� ���L(z) 
���z = Softmax(z) 
L(z) = 
N X 
i=1 Q(zi)��� ���L(z) 
���z 
= ?q(z 
1),...,q(zn) ?> [2] 
both neurons and astrocytes, as we will detail next. Note that for 
i xedsij,thismodelissimplyastandardrecurrentneuralnetwork. 
2.2. Synaptic Dynamics.The level of synaptic facilitation, which 
isdenotedbysij,referstothedegreetowhichpresynapticspiking 
activity impacts the postsynaptic neuron. Biophysically, there are 
manyfactorswhichinf l uencetheeff i cacyofasynapse,suchasthe 
number of postsynaptic receptors and the level of calcium in the 
synaptic cleft. The strength of the synapse can either increase or 
decreasebasedonbothpre-andpostsynapticactivity,asobserved 
in Hebbian plasticity. As in earlier studies of neuron���glial 
interactions, we consider tripartite synapses���synapses whose 
plasticity is modulated by an enveloping astrocytic process, pij 
���s�� sij= ������ sij+ f (xi,xj,pij,sij) + cij[3] 
The timescale of the synaptic dynamics is ���s, and ��� establishes 
the leak-rate of synaptic facilitation. The function f encapsulates 
the interactions between these three biological variables. The 
inputs cijserve as bias variables, controlling the baseline rate 
of synaptic facilitation in the absence of external input. The 
concentration of intracellular Ca2+ions in the astrocytic process 
that wraps around the particular synapse i ��� j is denoted by pij. 
Biophysically, astrocytes inf l uence neural activity through Ca2+- 
dependentexocytosisofgliotransmitterssuchasGABA,D-serine, 
ATP, and glutamate (14)���an inf l uence which is encoded in the 
function f . The level of intracellular astrocytic Ca2+is a dynamic 
variable and its value depends on both the calcium levels in 
adjacent astrocyte processes and the synaptic state sij. 
2.3. Astrocyte Process Dynamics.The state of a specif i c astro- 
cytic process is determined by its interactions with neurons at 
the tripartite synapse, plus its interactions with other processes 
through intracellular calcium transport 
2 of 7https://doi.org/10.1073/pnas.2417788122pnas.org 
Downloaded 
from https://www.pnas.org 
by 159.2.135.155 
on 
June 
10, 
2025 
from 
IP 
address 
159.2.135.155. 
���p�� pij= ������ pij+ 
N X 
k,l=1 
Tijkl���(pkl) + ���(sij) + dij[4] 
The double sum in the astrocyte equations captures the interac- 
tions between process pijand all other processes (34). In the sim- 
plest scenario, calcium can diffuse between processes, resulting 
in a linear function ��� and tensor Tijkldescribing concentration 
l uxes. More complex calcium transport mechanisms between 
different processes within an astrocyte can produce nonlinear 
functions ���. The term dijis a constant bias term which sets the 
overall���tone���oftheastrocyte.Thisvariablemaybethoughtofas 
a neuromodulatory signal, potentially arriving from distant brain 
regions such as the pons. The input from process pklto process 
pijis weighed by the scalar Tijkl. A zero value for this scalar 
indicates no direct physical connection between process ij and 
kl. Hence, the astrocyte���s anatomical structure can be encoded 
in the nonzero entries of tensor T. The nonlinear function 
��� encapsulates the synapse ��� astrocyte signaling pathway at 
the tripartite synapse level. Here, ���prepresents the astrocyte 
timescale, while ��� > 0 is a leak term for the intracellular calcium 
in the astrocyte process. 
3. Associative Neuron���Astrocyte Model 
In Section 2, we described a general framework, grounded in 
the biology of neuron���astrocyte communication, for modeling 
neuron���astrocyte interactions via the tripartite synapses. De- 
pending on the choices of the nonlinearities and parameters this 
network can exhibit many sophisticated dynamical behaviors��� 
such as chaos or limit cycles���which can be diff i cult to analyze 
in full generality. 
To better understand the potential role of neuron���astrocyte 
interactions, we will focus on an important limiting case where 
the system demonstrates associative memory functions. As in 
essentially all models of biological associative memory, this 
requiresthepresenceofsymmetriesinthegoverningequationsof 
thebiologicalcircuit.Weshowthatundercertainconditions,the 
resulting neuron���astrocyte model has a global energy function 
(Lyapunov function), which monotonically decreases on the 
dynamical trajectory and is bounded from below. This makes it 
possibletoidentifyaregimeofoperationoftheneuron���astrocyte 
network that results in dynamical trajectories converging to 
i xed point attractor states. The i xed points can be identif i ed 
with ���memories��� stored in the weight matrices, and the entire 
neuron���astrocyte model can be regarded as an energy-based 
Dense Associative Memory (also known as Modern Hopf i eld 
Network) (35, 36). Importantly, this framework allows us to 
show that the presence of a single astrocyte can provably boost 
the memory capacity per compute unit of a neural circuit by a 
factor of N. 
We will follow the general formulation of energy-based 
associative memories (37���39), which starts with picking three 
Lagrangians, which def i ne layers of our architecture (neurons, 
synapses, and astrocytic processes), and the corresponding acti- 
vationfunctions.TheseLagrangiansareaneuralLagrangianL[n], 
a synaptic Lagrangian L[s], and an astrocyte process Lagrangian 
L[p]. In general these scalar functions can be arbitrary (differen- 
tiable) functions of the corresponding dynamical variables. The 
details of our derivation can be found in SI Appendix, section 1. 
From these Lagrangians, we can derive via a Legendre 
transformation three terms in the overall energy function of 
the neuron���astrocyte system: E[n], E[s], and E[p]. The activation 
functions in our model are dictated by the Lagrangians���indeed, 
theithactivationissimplythepartialderivativeoftheLagrangian 
with respect to the ithdynamical variable (SI Appendix, section 1 
andBox1).Theremainingcontributionstothetotalenergyofthe 
system describe the interactions between neurons, synapses, and 
astrocytes. These contributions describe the synapse-mediated 
interactions between the neurons E[ns], the interactions between 
theprocessesandthesynapsesE[ps],andtheinteractionsbetween 
the individual processes inside the astrocyte E[pp]. 
The overall energy function of the neuron���astrocyte model 
can now be written as the sum of these six terms 
E = E[n]+ E[s]+ E[p]+ E[ns]+ E[ps]+ E[pp][5] 
From this Lagrangian formalism (37���39), the dynamical equa- 
tions for the associative neuron���astrocyte energy can be viewed 
as the negative gradient (with respect to the nonlinearities): 
��� ��� ���
ChatGPT said:
You're diving into a remarkable piece of work. The
Neuron���Astrocyte Associative Memory
model by Kozachkov, Slotine, and Krotov presents a
novel tripartite dynamical system
uniting
neurons
,
synapses
, and
astrocytes
into a
multi-layer energy-based memory network
. Let���s unpack Section 2 and Section 3 of the paper���focusing on the biological grounding, dynamical equations, and how it transitions into an
energy-minimizing associative memory system
.
���� SECTION 2: Neuron���Astrocyte Model
This section formalizes the
three interlinked dynamical components
of the system:
���� 2.1 Neuronal Dynamics
Each neuron's membrane voltage
x
i
x_i
x
i
���
evolves according to a
leaky rate-based model
:
��
n
x
��
i
=
���
��
x
i
+
���
j
=
1
N
g
(
s
i
j
)
��
(
x
j
)
+
b
i
(1)
\tau_n \dot{x}_i = -\lambda x_i + \sum_{j=1}^N g(s_{ij}) \phi(x_j) + b_i \tag{1}
��
n
���
x
��
i
���
=
���
��
x
i
���
+
j
=
1
���
N
���
g
(
s
ij
���
)
��
(
x
j
���
)
+
b
i
���
(
1
)
��
(
x
j
)
\phi(x_j)
��
(
x
j
���
)
: nonlinear activation (e.g. tanh, ReLU)
g
(
s
i
j
)
g(s_{ij})
g
(
s
ij
���
)
: function of synaptic strength (modulated by astrocyte interaction)
b
i
b_i
b
i
���
: bias input
If
s
i
j
s_{ij}
s
ij
���
were static, this is a vanilla recurrent neural network.
���� 2.2 Synaptic Dynamics
The synapse strength
s
i
j
s_{ij}
s
ij
���
between neurons
i
i
i
and
j
j
j
is
plastic
and
modulated by astrocytic calcium levels
p
i
j
p_{ij}
p
ij
���
:
��
s
s
��
i
j
=
���
��
s
i
j
+
f
(
x
i
,
x
j
,
p
i
j
,
s
i
j
)
+
c
i
j
(2)
\tau_s \dot{s}_{ij} = -\alpha s_{ij} + f(x_i, x_j, p_{ij}, s_{ij}) + c_{ij} \tag{2}
��
s
���
s
��
ij
���
=
���
��
s
ij
���
+
f
(
x
i
���
,
x
j
���
,
p
ij
���
,
s
ij
���
)
+
c
ij
���
(
2
)
f
f
f
: activity-dependent Hebbian-like function including astrocyte influence
c
i
j
c_{ij}
c
ij
���
: synaptic bias
p
i
j
p_{ij}
p
ij
���
: astrocyte process calcium level surrounding synapse
i
���
j
i-j
i
���
j
Here,
astrocytic calcium levels directly participate in synaptic learning rules
.
���� 2.3 Astrocytic Process Dynamics
Each
astrocytic process
around a synapse evolves via calcium exchange and feedback from neuron-synapse activity:
��
p
p
��
i
j
=
���
��
p
i
j
+
���
k
,
l
=
1
N
T
i
j
k
l
��
(
p
k
l
)
+
��
(
s
i
j
)
+
d
i
j
(3)
\tau_p \dot{p}_{ij} = -\gamma p_{ij} + \sum_{k,l=1}^N T_{ijkl} \psi(p_{kl}) + \kappa(s_{ij}) + d_{ij} \tag{3}
��
p
���
p
��
���
ij
���
=
���
��
p
ij
���
+
k
,
l
=
1
���
N
���
T
ijk
l
���
��
(
p
k
l
���
)
+
��
(
s
ij
���
)
+
d
ij
���
(
3
)
��
(
p
k
l
)
\psi(p_{kl})
��
(
p
k
l
���
)
: nonlinear interaction (e.g. diffusion) from other astrocyte processes
��
(
s
i
j
)
\kappa(s_{ij})
��
(
s
ij
���
)
: calcium influx from synaptic state
T
i
j
k
l
T_{ijkl}
T
ijk
l
���
: structural tensor defining which processes are interconnected
d
i
j
d_{ij}
d
ij
���
: neuromodulatory tone (e.g. acetylcholine from pons)
This formalism supports
distributed computation
over calcium fields inside astrocyte trees.
���� SECTION 3: Associative Neuron���Astrocyte Memory
To give the system
associative memory properties
, the authors
construct an energy function
from the three layers using
Lagrangian dynamics
.
���� Lagrangian-Driven Structure
Each of the three variables���neurons
x
i
x_i
x
i
���
, synapses
s
i
j
s_{ij}
s
ij
���
, and astrocyte calcium
p
i
j
p_{ij}
p
ij
���
���has a
Lagrangian
:
L
[
n
]
(
x
)
\mathcal{L}^{[n]}(x)
L
[
n
]
(
x
)
��� neuron layer
L
[
s
]
(
s
)
\mathcal{L}^{[s]}(s)
L
[
s
]
(
s
)
��� synapse layer
L
[
p
]
(
p
)
\mathcal{L}^{[p]}(p)
L
[
p
]
(
p
)
��� astrocyte layer
Activation functions are derived from gradients of Lagrangians:
Activation:
���
L
���
z
i
���
��
(
x
i
)
,
g
(
s
i
j
)
,
��
(
p
i
j
)
\text{Activation: } \frac{\partial \mathcal{L}}{\partial z_i} \Rightarrow \phi(x_i), g(s_{ij}), \psi(p_{ij})
Activation:
���
z
i
���
���
L
���
���
��
(
x
i
���
)
,
g
(
s
ij
���
)
,
��
(
p
ij
���
)
Example from Box 1:
Softmax from log-sum-exp:
L
(
z
)
=
log
���
���
i
e
z
i
���
���
L
���
z
i
=
Softmax
(
z
)
i
\mathcal{L}(z) = \log \sum_i e^{z_i} \Rightarrow \frac{\partial \mathcal{L}}{\partial z_i} = \text{Softmax}(z)_i
L
(
z
)
=
lo
g
i
���
���
e
z
i
���
���
���
z
i
���
���
L
���
=
Softmax
(
z
)
i
���
���� Full Energy Function
The total system energy is:
E
=
E
[
n
]
+
E
[
s
]
+
E
[
p
]
+
E
[
n
s
]
+
E
[
p
s
]
+
E
[
p
p
]
(4)
E = E^{[n]} + E^{[s]} + E^{[p]} + E^{[ns]} + E^{[ps]} + E^{[pp]} \tag{4}
E
=
E
[
n
]
+
E
[
s
]
+
E
[
p
]
+
E
[
n
s
]
+
E
[
p
s
]
+
E
[
pp
]
(
4
)
Where:
E
[
n
]
,
E
[
s
]
,
E
[
p
]
E^{[n]}, E^{[s]}, E^{[p]}
E
[
n
]
,
E
[
s
]
,
E
[
p
]
: internal energy of neurons, synapses, astrocyte processes
E
[
n
s
]
E^{[ns]}
E
[
n
s
]
: neuron-synapse interactions
E
[
p
s
]
E^{[ps]}
E
[
p
s
]
: astrocyte-synapse coupling
E
[
p
p
]
E^{[pp]}
E
[
pp
]
: astrocyte���astrocyte interactions via calcium waves
Under symmetric connectivity conditions,
this energy decreases over time
, yielding a
Lyapunov function
. The system thus
settles into stable attractors
representing stored memories.
���� What This Means
1.
Astrocytes Boost Associative Capacity
A single astrocyte interacting with
N
N
N
synapses can increase memory capacity by
a factor of
N
N
N
compared to neuron-only models.
This is due to
combinatorial richness
in astrocyte���synapse interactions and distributed calcium signaling.
2.
Energy-Based Memory with Biological Plausibility
The model is a
biological Dense Associative Memory (DAM)
, similar in spirit to a
Hopfield network
, but grounded in real cellular dynamics.
3.
Astrocyte Networks Mirror Transformer-Like Behavior
By adjusting connectivity and activation Lagrangians, the model interpolates between:
Dense Associative Memory
(DAM) behavior
Transformer-like architectures
(attention + nonlinear aggregation)
This suggests a
deep homology
between biological memory and modern deep learning.
���� TL;DR Summary
Component
Variable
Equation
Description
Neuron
x
i
x_i
x
i
���
(1)
RNN dynamics w/ synaptic input
Synapse
s
i
j
s_{ij}
s
ij
���
(2)
Hebbian + astrocyte-modulated plasticity
Astrocyte Process
p
i
j
p_{ij}
p
ij
���
(3)
Calcium-based interprocess and synapse feedback
System Energy
E
E
E
(4)
Lyapunov function for memory attractors
Memory Boost
���
���
Memory scales superlinearly via astrocytic structure
Would you like help visualizing this with a
network diagram
or want a
step-by-step derivation of the energy gradient flow
from the Lagrangians?
You said:
A step by step derivation
ChatGPT said:
Great ��� let���s walk through a
step-by-step derivation
of the
neuron���astrocyte associative memory system
based on the
Lagrangian formalism
presented in the paper.
This framework constructs an
energy function
E
E
E
for the system, using
three interacting layers
��� neurons
{
x
i
}
\{x_i\}
{
x
i
���
}
, synapses
{
s
i
j
}
\{s_{ij}\}
{
s
ij
���
}
, and astrocytic processes
{
p
i
j
}
\{p_{ij}\}
{
p
ij
���
}
. The
activation dynamics
of each layer are governed by the
gradient of a convex Lagrangian
with respect to its variables.
We will derive:
The
layer-specific energy contributions
The
interaction terms
The
overall dynamics as gradient descent on energy
������ STEP 1: Define Layer Variables
Let:
x
���
R
N
\mathbf{x} \in \mathbb{R}^N
x
���
R
N
: vector of neuron membrane voltages
S
���
R
N
��
N
\mathbf{S} \in \mathbb{R}^{N \times N}
S
���
R
N
��
N
: synaptic facilitation matrix
P
���
R
N
��
N
\mathbf{P} \in \mathbb{R}^{N \times N}
P
���
R
N
��
N
: astrocyte process calcium levels (one per tripartite synapse)
Each variable will have:
A
Lagrangian
:
L
[
x
]
(
x
)
\mathcal{L}^{[x]}(\mathbf{x})
L
[
x
]
(
x
)
,
L
[
s
]
(
S
)
\mathcal{L}^{[s]}(\mathbf{S})
L
[
s
]
(
S
)
,
L
[
p
]
(
P
)
\mathcal{L}^{[p]}(\mathbf{P})
L
[
p
]
(
P
)
A
corresponding energy
:
E
[
x
]
,
E
[
s
]
,
E
[
p
]
E^{[x]}, E^{[s]}, E^{[p]}
E
[
x
]
,
E
[
s
]
,
E
[
p
]
���� STEP 2: Layer Lagrangians and Energy Contributions
Each Lagrangian encodes a
prior structure
or
activation shape
:
Example 1: Softmax-like Lagrangian for neurons
L
[
x
]
(
x
)
=
log
���
(
���
i
=
1
N
e
x
i
)
���
���
L
[
x
]
���
x
i
=
softmax
(
x
)
i
\mathcal{L}^{[x]}(\mathbf{x}) = \log \left( \sum_{i=1}^N e^{x_i} \right) \quad \Rightarrow \quad \frac{\partial \mathcal{L}^{[x]}}{\partial x_i} = \text{softmax}(x)_i
L
[
x
]
(
x
)
=
lo
g
(
i
=
1
���
N
���
e
x
i
���
)
���
���
x
i
���
���
L
[
x
]
���
=
softmax
(
x
)
i
���
Example 2: Separable elementwise Lagrangian
L
[
x
]
(
x
)
=
���
i
Q
(
x
i
)
���
���
L
[
x
]
���
x
i
=
q
(
x
i
)
\mathcal{L}^{[x]}(\mathbf{x}) = \sum_i Q(x_i) \quad \Rightarrow \quad \frac{\partial \mathcal{L}^{[x]}}{\partial x_i} = q(x_i)
L
[
x
]
(
x
)
=
i
���
���
Q
(
x
i
���
)
���
���
x
i
���
���
L
[
x
]
���
=
q
(
x
i
���
)
This gives
activation functions
directly:
��
(
x
i
)
=
���
L
[
x
]
���
x
i
\phi(x_i) = \frac{\partial \mathcal{L}^{[x]}}{\partial x_i}
��
(
x
i
���
)
=
���
x
i
���
���
L
[
x
]
���
g
(
s
i
j
)
=
���
L
[
s
]
���
s
i
j
g(s_{ij}) = \frac{\partial \mathcal{L}^{[s]}}{\partial s_{ij}}
g
(
s
ij
���
)
=
���
s
ij
���
���
L
[
s
]
���
��
(
p
i
j
)
=
���
L
[
p
]
���
p
i
j
\psi(p_{ij}) = \frac{\partial \mathcal{L}^{[p]}}{\partial p_{ij}}
��
(
p
ij
���
)
=
���
p
ij
���
���
L
[
p
]
���
The energy contribution of each layer is the
Legendre transform
:
E
[
x
]
=
���
i
(
x
i
���
��
(
x
i
)
���
L
[
x
]
(
x
i
)
)
E^{[x]} = \sum_i \left( x_i \cdot \phi(x_i) - \mathcal{L}^{[x]}(x_i) \right)
E
[
x
]
=
i
���
���
(
x
i
���
���
��
(
x
i
���
)
���
L
[
x
]
(
x
i
���
)
)
(similarly for
E
[
s
]
E^{[s]}
E
[
s
]
and
E
[
p
]
E^{[p]}
E
[
p
]
).
���� STEP 3: Define Interaction Terms
Now define
coupling energies
between layers:
1. Neuron���Synapse interaction:
E
[
x
s
]
=
���
���
i
,
j
g
(
s
i
j
)
���
��
(
x
j
)
���
x
i
E^{[xs]} = -\sum_{i,j} g(s_{ij}) \cdot \phi(x_j) \cdot x_i
E
[
x
s
]
=
���
i
,
j
���
���
g
(
s
ij
���
)
���
��
(
x
j
���
)
���
x
i
���
This reflects
postsynaptic influence
of neuron
j
j
j
on neuron
i
i
i
weighted by synaptic strength.
2. Synapse���Astrocyte interaction:
E
[
s
p
]
=
���
���
i
,
j
��
(
s
i
j
)
���
p
i
j
E^{[sp]} = -\sum_{i,j} \kappa(s_{ij}) \cdot p_{ij}
E
[
s
p
]
=
���
i
,
j
���
���
��
(
s
ij
���
)
���
p
ij
���
Where
��
(
s
)
\kappa(s)
��
(
s
)
captures how astrocyte processes are influenced by synapse state (e.g., Ca����� influx from synaptic activity).
3. Astrocyte���Astrocyte interaction:
E
[
p
p
]
=
���
i
,
j
,
k
,
l
T
i
j
k
l
���
��
(
p
k
l
)
���
p
i
j
E^{[pp]} = \sum_{i,j,k,l} T_{ijkl} \cdot \psi(p_{kl}) \cdot p_{ij}
E
[
pp
]
=
i
,
j
,
k
,
l
���
���
T
ijk
l
���
���
��
(
p
k
l
���
)
���
p
ij
���
This is the
calcium wave propagation
within astrocyte trees ��� a quadratic-like energy from diffusion or gap junctions.
���� STEP 4: Total Energy Function
Add up all components:
E
[
x
,
S
,
P
]
=
E
[
x
]
+
E
[
s
]
+
E
[
p
]
+
E
[
x
s
]
+
E
[
s
p
]
+
E
[
p
p
]
(5)
E[\mathbf{x}, \mathbf{S}, \mathbf{P}] = E^{[x]} + E^{[s]} + E^{[p]} + E^{[xs]} + E^{[sp]} + E^{[pp]} \tag{5}
E
[
x
,
S
,
P
]
=
E
[
x
]
+
E
[
s
]
+
E
[
p
]
+
E
[
x
s
]
+
E
[
s
p
]
+
E
[
pp
]
(
5
)
This total energy is
Lyapunov
: it
monotonically decreases over time
under the dynamics defined below.
���� STEP 5: Gradient Flow Dynamics
For each dynamical variable
z
���
{
x
i
,
s
i
j
,
p
i
j
}
z \in \{ x_i, s_{ij}, p_{ij} \}
z
���
{
x
i
���
,
s
ij
���
,
p
ij
���
}
, the dynamics follow
gradient descent
on
E
E
E
:
��
z
���
z
��
=
���
���
E
���
z
\tau_z \cdot \dot{z} = - \frac{\partial E}{\partial z}
��
z
���
���
z
��
=
���
���
z
���
E
���
So:
Neuron:
��
x
���
x
��
i
=
���
���
E
���
x
i
=
���
(
���
E
[
x
]
���
x
i
+
���
E
[
x
s
]
���
x
i
)
\tau_x \cdot \dot{x}_i = - \frac{\partial E}{\partial x_i} = - \left( \frac{\partial E^{[x]}}{\partial x_i} + \frac{\partial E^{[xs]}}{\partial x_i} \right)
��
x
���
���
x
��
i
���
=
���
���
x
i
���
���
E
���
=
���
(
���
x
i
���
���
E
[
x
]
���
+
���
x
i
���
���
E
[
x
s
]
���
)
Synapse:
��
s
���
s
��
i
j
=
���
(
���
E
[
s
]
���
s
i
j
+
���
E
[
x
s
]
���
s
i
j
+
���
E
[
s
p
]
���
s
i
j
)
\tau_s \cdot \dot{s}_{ij} = - \left( \frac{\partial E^{[s]}}{\partial s_{ij}} + \frac{\partial E^{[xs]}}{\partial s_{ij}} + \frac{\partial E^{[sp]}}{\partial s_{ij}} \right)
��
s
���
���
s
��
ij
���
=
���
(
���
s
ij
���
���
E
[
s
]
���
+
���
s
ij
���
���
E
[
x
s
]
���
+
���
s
ij
���
���
E
[
s
p
]
���
)
Astrocyte process:
��
p
���
p
��
i
j
=
���
(
���
E
[
p
]
���
p
i
j
+
���
E
[
s
p
]
���
p
i
j
+
���
E
[
p
p
]
���
p
i
j
)
\tau_p \cdot \dot{p}_{ij} = - \left( \frac{\partial E^{[p]}}{\partial p_{ij}} + \frac{\partial E^{[sp]}}{\partial p_{ij}} + \frac{\partial E^{[pp]}}{\partial p_{ij}} \right)
��
p
���
���
p
��
���
ij
���
=
���
(
���
p
ij
���
���
E
[
p
]
���
+
���
p
ij
���
���
E
[
s
p
]
���
+
���
p
ij
���
���
E
[
pp
]
���
)
This gives a
gradient descent system
with attractors.
��� STEP 6: Fixed Point = Memory Attractor
A memory is retrieved when the system converges to a
fixed point
:
d
x
d
t
=
0
,
d
S
d
t
=
0
,
d
P
d
t
=
0
\frac{d\mathbf{x}}{dt} = 0, \quad \frac{d\mathbf{S}}{dt} = 0, \quad \frac{d\mathbf{P}}{dt} = 0
d
t
d
x
���
=
0
,
d
t
d
S
���
=
0
,
d
t
d
P
���
=
0
At this point:
The
energy is minimized
The network has
recalled a stored memory pattern
That memory may be stored in
astrocytic state
,
synaptic configuration
, or both
���� Summary of Derivation
Layer
Variable
Lagrangian
Activation
Dynamics
Neuron
x
i
x_i
x
i
���
L
[
x
]
\mathcal{L}^{[x]}
L
[
x
]
��
(
x
i
)
=
���
L
[
x
]
/
���
x
i
\phi(x_i) = \partial \mathcal{L}^{[x]}/\partial x_i
��
(
x
i
���
)
=
���
L
[
x
]
/
���
x
i
���
x
��
i
=
���
���
E
/
���
x
i
\dot{x}_i = -\partial E/\partial x_i
x
��
i
���
=
���
���
E
/
���
x
i
���
Synapse
s
i
j
s_{ij}
s
ij
���
L
[
s
]
\mathcal{L}^{[s]}
L
[
s
]
g
(
s
i
j
)
=
���
L
[
s
]
/
���
s
i
j
g(s_{ij}) = \partial \mathcal{L}^{[s]}/\partial s_{ij}
g
(
s
ij
���
)
=
���
L
[
s
]
/
���
s
ij
���
s
��
i
j
=
���
���
E
/
���
s
i
j
\dot{s}_{ij} = -\partial E/\partial s_{ij}
s
��
ij
���
=
���
���
E
/
���
s
ij
���
Astrocyte
p
i
j
p_{ij}
p
ij
���
L
[
p
]
\mathcal{L}^{[p]}
L
[
p
]
��
(
p
i
j
)
=
���
L
[
p
]
/
���
p
i
j
\psi(p_{ij}) = \partial \mathcal{L}^{[p]}/\partial p_{ij}
��
(
p
ij
���
)
=
���
L
[
p
]
/
���
p
ij
���
p
��
i
j
=
���
���
E
/
���
p
i
j
\dot{p}_{ij} = -\partial E/\partial p_{ij}
p
��
���
ij
���
=
���
���
E
/
���
p
ij
���
���� Bonus: Why This Works
Because each layer uses
convex Lagrangians
, the system is guaranteed (under mild assumptions) to:
Converge to
fixed points
Maintain
bounded, stable dynamics
Enable
memory retrieval
from partial cues (like a Hopfield net)
Would you like a visual diagram of the full neuron���synapse���astrocyte energy network, or want to implement a simplified version in code (e.g., in PyTorch or NumPy)?
You said:
3. Associative Neuron���Astrocyte Model 
In Section 2, we described a general framework, grounded in 
the biology of neuron���astrocyte communication, for modeling 
neuron���astrocyte interactions via the tripartite synapses. De- 
pending on the choices of the nonlinearities and parameters this 
network can exhibit many sophisticated dynamical behaviors��� 
such as chaos or limit cycles���which can be diff i cult to analyze 
in full generality. 
To better understand the potential role of neuron���astrocyte 
interactions, we will focus on an important limiting case where 
the system demonstrates associative memory functions. As in 
essentially all models of biological associative memory, this 
requiresthepresenceofsymmetriesinthegoverningequationsof 
thebiologicalcircuit.Weshowthatundercertainconditions,the 
resulting neuron���astrocyte model has a global energy function 
(Lyapunov function), which monotonically decreases on the 
dynamical trajectory and is bounded from below. This makes it 
possibletoidentifyaregimeofoperationoftheneuron���astrocyte 
network that results in dynamical trajectories converging to 
i xed point attractor states. The i xed points can be identif i ed 
with ���memories��� stored in the weight matrices, and the entire 
neuron���astrocyte model can be regarded as an energy-based 
Dense Associative Memory (also known as Modern Hopf i eld 
Network) (35, 36). Importantly, this framework allows us to 
show that the presence of a single astrocyte can provably boost 
the memory capacity per compute unit of a neural circuit by a 
factor of N. 
We will follow the general formulation of energy-based 
associative memories (37���39), which starts with picking three 
Lagrangians, which def i ne layers of our architecture (neurons, 
synapses, and astrocytic processes), and the corresponding acti- 
vationfunctions.TheseLagrangiansareaneuralLagrangianL[n], 
a synaptic Lagrangian L[s], and an astrocyte process Lagrangian 
L[p]. In general these scalar functions can be arbitrary (differen- 
tiable) functions of the corresponding dynamical variables. The 
details of our derivation can be found in SI Appendix, section 1. 
From these Lagrangians, we can derive via a Legendre 
transformation three terms in the overall energy function of 
the neuron���astrocyte system: E[n], E[s], and E[p]. The activation 
functions in our model are dictated by the Lagrangians���indeed, 
theithactivationissimplythepartialderivativeoftheLagrangian 
with respect to the ithdynamical variable (SI Appendix, section 1 
andBox1).Theremainingcontributionstothetotalenergyofthe 
system describe the interactions between neurons, synapses, and 
astrocytes. These contributions describe the synapse-mediated 
interactions between the neurons E[ns], the interactions between 
theprocessesandthesynapsesE[ps],andtheinteractionsbetween 
the individual processes inside the astrocyte E[pp]. 
The overall energy function of the neuron���astrocyte model 
can now be written as the sum of these six terms 
E = E[n]+ E[s]+ E[p]+ E[ns]+ E[ps]+ E[pp][5] 
From this Lagrangian formalism (37���39), the dynamical equa- 
tions for the associative neuron���astrocyte energy can be viewed 
as the negative gradient (with respect to the nonlinearities): 
��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� 
���n�� xi= ��� 
���E 
������i = ��� ��� xi+ 
N X 
j=1 
gij���j 
���s�� sij= ���2 
���E 
���gij = ��� ��� sij+ ���i���j+ ���ij 
���p�� pij= ���2 
���E 
������ij = ��� ��� pij+ 
N X 
k,l=1 
Tijkl���kl+ gij 
[6] 
The energy-based equations have a large amount of symmetry��� 
both in the parameters and the dynamical degrees of freedom, 
e.g., Tijkl= Tklij, see SI Appendix, section 1. These symmetries 
are needed for the existence of the global energy function for 
our neuron���astrocyte network, which leads to mathematical 
tractability. In real biology, some (or all) of these symmetries 
might be broken, and the analytical tractability might be more 
diff i cult or even impossible. We use the energy-based model 
to establish theoretically the memory storage capabilities of 
our model. The nonsymmetric model is studied numerically in 
Section 4 where we show that it possesses similar capabilities 
despite lacking the energy-based formulation. Note that unlike 
thesymmetriesintheoriginalHopf i eldnetworks(40,41)(which 
have no known biological interpretation), the invariance of T 
with respect to swapping indices ij and kl can be viewed as 
a natural consequence of the underlying symmetry of calcium 
diffusion. 
Thef i rsttwoequationsinEq.6arereminiscentoftheapproach 
by Dong and Hopf i eld (42), which describes both the neural 
dynamics and synaptic plasticity by a single energy function. The 
difference of our system compared to ref. 42 is the existence 
of the network of astrocytic processes, which interact with each 
other and with the synapses. Following the general Lagrangian 
formalism, it can be shown (SI Appendix, section 2) that 
dE 
dt = ��� ? 
���n 
N X 
i,j=1 �� xi ���2L[n] 
���xi���xj �� xj 
+ ���s 
2 
N X 
i,j,k,l=1 �� sij ���2L[s] 
���sij���skl �� skl 
+ ���p 
2 
N X 
i,j,k,l=1 �� pij ���2L[p] 
���pij���pkl �� pkl ? 
��� 0 
[7] 
PNAS2025Vol. 122No. 21e2417788122https://doi.org/10.1073/pnas.24177881223 of 7 
Downloaded 
from https://www.pnas.org 
by 159.2.135.155 
on 
June 
10, 
2025 
from 
IP 
address 
159.2.135.155. 
Memory 1Memory 2 
Memory 1Memory K 
... 
Energy Landscape 
With Astrocyte 
Without 
AstrocyteA 
B 
Memory 1 
Memory 1 
Energy Landscape 
Fig. 2.For a fixed number of neurons, the neuron���astrocyte network is 
capable of storing many more memories than the neuron-only network. (A) 
The energy landscape of a neuron-only associative network. (B) The energy 
landscape of a neuron���astrocyte associative network. The memories are 
moredenselypackedintothestate-space,therebyenablingsuperiormemory 
storage and retrieval capabilities. 
The last inequality sign holds if each Lagrangian has a positive 
semidef i nite Hessian matrix. When the Hessian matrices are 
strictly positive def i nite, the dynamical Eq. 6 are guaranteed 
to arrive at a i xed point, because the energy is bounded from 
below [through the invariant set theorem (43)]. Thus, starting 
from an initial state the network dynamics l ows toward one of 
the i xed points and for this reason describes the operation of an 
associative memory. 
3.1. Connection to Dense Associative Memory.Energy-based 
neuron���astrocyte networks are described by a sophisticated 
system of nonlinear differential equations [6] that are guaranteed 
to represent dynamical trajectories converging to i xed points 
attractors, assuming certain conditions on the Lagrangians are 
met. The locations of those i xed points x��� 
i, s���ij, and p���ij coincide 
with the local minima of the energy function Eq. 5, and are 
independent of the time scales ���n, ���s, and ���p. The kinetics of 
the model (i.e., the shape of dynamical trajectories), however, 
heavily depends on these time scales. Although the characteristic 
time scales of synaptic plasticity and dynamics of processes (as 
well as the dynamics of the entire astrocyte) are subjects of 
active debates in the community, it is generally believed that 
neurons operate on faster time scales than synaptic plasticity or 
the processes, ���n? ���s,���p. Since the goal of this section is to 
analyze the i xed points of this network, which are independent 
of these time scales, we have a freedom to choose the kinetic time 
scalesinawaydictatedbymathematicalconvenience,ratherthan 
biologicalreality.Specif i cally,wewillderiveaneffectivedynamics 
on neurons that arises after the synapses and astrocytes are 
integrated out from the dynamical equations, which intuitively* 
is possible if ���s,���p? ���n. Despite this ���unbiological��� choice 
for the intermediate steps, the i nal answer represents accurate 
locations of i xed points for the network that operates in the 
���biological��� regime ���n? ���s,���p, or with any other choice of time 
scales. 
*Strictly speaking, specific conditions must be satisfied in order for this operation to be 
well-defined and to avoid pathologies such as peaking phenomena. In our system this can 
easily be done by restricting the eigenvalues of T to be less than ���, which ensures that the 
fast dynamics are contracting (44). 
The i xed points of the synaptic and processes��� dynamics in 
Eq. 6 are def i ned by (assuming for simplicity that ��� = ��� = 0) 
��� ��� ��� ��� ��� 
���ij= ������i���j 
gij= 
N P 
k,l=1 Tijkl���k���l 
[8] 
Note that for a i xed value of ���i, these equations uniquely 
determine the values of sijand pijwhen g and ��� are strictly 
monotonic. Substituting this solution into the i rst Eq. 6, gives 
the effective dynamics for neurons (assuming for simplicity that 
��� = 1) ���n�� xi= ���xi+ 
N X 
j,k,l=1 
Tijkl���j���k���l[9] 
and effective energy 
Eeff= h 
N X 
i=1 xi���i��� L[n] i ��� 
1 
4 
N X 
i,j,k,l=1 
Tijkl���i���j���k���l[10] 
Eqs. 9 and 10 contain the essence of our theoretical argument. 
The i xed points of this effective dynamical system exactly coin- 
cide with the i xed points of the original complete energy-based 
neuron���astrocyte network [6], projected on the neuron-only 
subspace. The hallmark of this effective theory is the existence 
of the four-body neuron-to-neuron interactions, represented by 
the product of four i ring rate functions ���iin the effective 
Lyapunov function and the product of three i ring rate functions 
in the effective equations. In conventional i ring rate models 
there is only one i ring rate function in the right hand side of 
the dynamical equations and two i ring rate functions in the 
corresponding energy function, see for example ref.
ChatGPT said:
Here is a
step-by-step derivation
of the
Associative Neuron���Astrocyte Model
from the Lagrangian formalism to the effective neural dynamics (Eqs. 6���10), following the logic and notation of the provided paper. We assume some familiarity with variational calculus, dynamical systems, and energy-based models like Hopfield networks.
Step 1: Define Lagrangians for Each Component
You start by defining scalar Lagrangians for the three interacting components:
Neural activity Lagrangian
:
L
[
n
]
(
x
)
L[n](\mathbf{x})
L
[
n
]
(
x
)
, where
x
=
(
x
1
,
.
.
.
,
x
N
)
\mathbf{x} = (x_1, ..., x_N)
x
=
(
x
1
���
,
...
,
x
N
���
)
are neural states.
Synaptic state Lagrangian
:
L
[
s
]
(
s
)
L[s](\mathbf{s})
L
[
s
]
(
s
)
, where
s
=
(
s
i
j
)
\mathbf{s} = (s_{ij})
s
=
(
s
ij
���
)
are synaptic strengths.
Astrocytic process Lagrangian
:
L
[
p
]
(
p
)
L[p](\mathbf{p})
L
[
p
]
(
p
)
, where
p
=
(
p
i
j
)
\mathbf{p} = (p_{ij})
p
=
(
p
ij
���
)
represent astrocytic process variables.
Each Lagrangian generates an
activation function
via differentiation:
Neuron:
��
i
=
���
L
[
n
]
���
x
i
\phi_i = \frac{\partial L[n]}{\partial x_i}
��
i
���
=
���
x
i
���
���
L
[
n
]
���
Synapse:
g
i
j
=
���
L
[
s
]
���
s
i
j
g_{ij} = \frac{\partial L[s]}{\partial s_{ij}}
g
ij
���
=
���
s
ij
���
���
L
[
s
]
���
Astrocyte:
��
i
j
=
���
L
[
p
]
���
p
i
j
\psi_{ij} = \frac{\partial L[p]}{\partial p_{ij}}
��
ij
���
=
���
p
ij
���
���
L
[
p
]
���
Step 2: Construct the Energy Function
Using Legendre transforms, the energy function is defined as:
E
=
E
[
n
]
+
E
[
s
]
+
E
[
p
]
+
E
[
n
s
]
+
E
[
p
s
]
+
E
[
p
p
]
E = E[n] + E[s] + E[p] + E[ns] + E[ps] + E[pp]
E
=
E
[
n
]
+
E
[
s
]
+
E
[
p
]
+
E
[
n
s
]
+
E
[
p
s
]
+
E
[
pp
]
Where:
E
[
n
]
=
���
i
x
i
��
i
���
L
[
n
]
(
x
)
E[n] = \sum_i x_i \phi_i - L[n](\mathbf{x})
E
[
n
]
=
���
i
���
x
i
���
��
i
���
���
L
[
n
]
(
x
)
E
[
s
]
=
���
i
j
s
i
j
g
i
j
���
L
[
s
]
(
s
)
E[s] = \sum_{ij} s_{ij} g_{ij} - L[s](\mathbf{s})
E
[
s
]
=
���
ij
���
s
ij
���
g
ij
���
���
L
[
s
]
(
s
)
E
[
p
]
=
���
i
j
p
i
j
��
i
j
���
L
[
p
]
(
p
)
E[p] = \sum_{ij} p_{ij} \psi_{ij} - L[p](\mathbf{p})
E
[
p
]
=
���
ij
���
p
ij
���
��
ij
���
���
L
[
p
]
(
p
)
E
[
n
s
]
=
���
���
i
j
g
i
j
��
i
��
j
E[ns] = -\sum_{ij} g_{ij} \phi_i \phi_j
E
[
n
s
]
=
���
���
ij
���
g
ij
���
��
i
���
��
j
���
(neuron-synapse coupling)
E
[
p
s
]
=
���
���
i
j
��
i
j
��
i
��
j
E[ps] = -\sum_{ij} \psi_{ij} \phi_i \phi_j
E
[
p
s
]
=
���
���
ij
���
��
ij
���
��
i
���
��
j
���
(astrocyte-synapse coupling)
E
[
p
p
]
=
���
���
i
j
k
l
T
i
j
k
l
��
i
j
��
k
l
E[pp] = -\sum_{ijkl} T_{ijkl} \psi_{ij} \psi_{kl}
E
[
pp
]
=
���
���
ijk
l
���
T
ijk
l
���
��
ij
���
��
k
l
���
(astrocyte self-interaction)
So the total energy becomes:
E
=
���
i
x
i
��
i
���
L
[
n
]
+
���
i
j
s
i
j
g
i
j
���
L
[
s
]
+
���
i
j
p
i
j
��
i
j
���
L
[
p
]
���
���
i
j
g
i
j
��
i
��
j
���
���
i
j
��
i
j
��
i
��
j
���
���
i
j
k
l
T
i
j
k
l
��
i
j
��
k
l
(5)
E = \sum_i x_i \phi_i - L[n] + \sum_{ij} s_{ij} g_{ij} - L[s] + \sum_{ij} p_{ij} \psi_{ij} - L[p] - \sum_{ij} g_{ij} \phi_i \phi_j - \sum_{ij} \psi_{ij} \phi_i \phi_j - \sum_{ijkl} T_{ijkl} \psi_{ij} \psi_{kl}
\tag{5}
E
=
i
���
���
x
i
���
��
i
���
���
L
[
n
]
+
ij
���
���
s
ij
���
g
ij
���
���
L
[
s
]
+
ij
���
���
p
ij
���
��
ij
���
���
L
[
p
]
���
ij
���
���
g
ij
���
��
i
���
��
j
���
���
ij
���
���
��
ij
���
��
i
���
��
j
���
���
ijk
l
���
���
T
ijk
l
���
��
ij
���
��
k
l
���
(
5
)
Step 3: Derive Dynamics from the Energy Gradient
The dynamics are given by gradient descent on this energy function:
��
n
x
��
i
=
���
���
E
���
��
i
=
���
��
x
i
+
���
j
g
i
j
��
j
��
s
s
��
i
j
=
���
2
���
E
���
g
i
j
=
���
��
s
i
j
+
��
i
��
j
+
��
i
j
��
p
p
��
i
j
=
���
2
���
E
���
��
i
j
=
���
��
p
i
j
+
���
k
l
T
i
j
k
l
��
k
l
+
g
i
j
(6)
\begin{aligned}
\tau_n \dot{x}_i &= -\frac{\partial E}{\partial \phi_i} = -\lambda x_i + \sum_j g_{ij} \phi_j \\
\tau_s \dot{s}_{ij} &= -2 \frac{\partial E}{\partial g_{ij}} = -\alpha s_{ij} + \phi_i \phi_j + \psi_{ij} \\
\tau_p \dot{p}_{ij} &= -2 \frac{\partial E}{\partial \psi_{ij}} = -\gamma p_{ij} + \sum_{kl} T_{ijkl} \psi_{kl} + g_{ij}
\end{aligned}
\tag{6}
��
n
���
x
��
i
���
��
s
���
s
��
ij
���
��
p
���
p
��
���
ij
���
���
=
���
���
��
i
���
���
E
���
=
���
��
x
i
���
+
j
���
���
g
ij
���
��
j
���
=
���
2
���
g
ij
���
���
E
���
=
���
��
s
ij
���
+
��
i
���
��
j
���
+
��
ij
���
=
���
2
���
��
ij
���
���
E
���
=
���
��
p
ij
���
+
k
l
���
���
T
ijk
l
���
��
k
l
���
+
g
ij
���
���
(
6
)
This gives the dynamical system describing evolution of neurons, synapses, and astrocytes.
Step 4: Show Energy is Monotonically Decreasing
From the structure of the energy and using positive semidefinite Hessians of each Lagrangian:
d
E
d
t
���
0
(7)
\frac{dE}{dt} \leq 0
\tag{7}
d
t
d
E
���
���
0
(
7
)
This guarantees convergence to fixed points under the right conditions (e.g.
���
2
L
[
n
]
/
���
x
i
���
x
j
���
0
\partial^2 L[n] / \partial x_i \partial x_j \geq 0
���
2
L
[
n
]
/
���
x
i
���
���
x
j
���
���
0
, etc.).
Step 5: Derive Fixed Points
Assume long-time limit (
s
��
i
j
=
0
,
p
��
i
j
=
0
\dot{s}_{ij} = 0, \dot{p}_{ij} = 0
s
��
ij
���
=
0
,
p
��
���
ij
���
=
0
), and simplify for clarity by letting
��
=
��
=
0
\alpha = \gamma = 0
��
=
��
=
0
:
��
i
j
=
���
��
i
��
j
g
i
j
=
���
k
,
l
T
i
j
k
l
��
k
��
l
(8)
\begin{aligned}
\psi_{ij} &= -\phi_i \phi_j \\
g_{ij} &= \sum_{k,l} T_{ijkl} \phi_k \phi_l
\end{aligned}
\tag{8}
��
ij
���
g
ij
���
���
=
���
��
i
���
��
j
���
=
k
,
l
���
���
T
ijk
l
���
��
k
���
��
l
���
���
(
8
)
This gives steady-state expressions for astrocytic and synaptic variables as functions of neuron activations
��
\phi
��
.
Step 6: Plug into Neural Dynamics
Substitute Eq. 8 into Eq. 6���s first line:
��
n
x
��
i
=
���
x
i
+
���
j
,
k
,
l
T
i
j
k
l
��
j
��
k
��
l
(9)
\tau_n \dot{x}_i = -x_i + \sum_{j,k,l} T_{ijkl} \phi_j \phi_k \phi_l
\tag{9}
��
n
���
x
��
i
���
=
���
x
i
���
+
j
,
k
,
l
���
���
T
ijk
l
���
��
j
���
��
k
���
��
l
���
(
9
)
This is the
effective neural dynamics
, which shows that each neuron is influenced by a
third-order interaction
of other neurons (via astrocyte-mediated synapses).
Step 7: Derive Effective Energy Function
The effective energy function
E
eff
E_{\text{eff}}
E
eff
���
becomes:
E
eff
=
���
i
x
i
��
i
���
L
[
n
]
���
1
4
���
i
,
j
,
k
,
l
T
i
j
k
l
��
i
��
j
��
k
��
l
(10)
E_{\text{eff}} = \sum_i x_i \phi_i - L[n] - \frac{1}{4} \sum_{i,j,k,l} T_{ijkl} \phi_i \phi_j \phi_k \phi_l
\tag{10}
E
eff
���
=
i
���
���
x
i
���
��
i
���
���
L
[
n
]
���
4
1
���
i
,
j
,
k
,
l
���
���
T
ijk
l
���
��
i
���
��
j
���
��
k
���
��
l
���
(
10
)
Here:
The first two terms come from neuron dynamics.
The quartic term arises from the astrocyte-synapse feedback loop.
Conclusion: Associative Memory via Astrocyte Boosting
Fixed points
of this system correspond to
stored memories
.
Due to the
quartic interactions
���
T
i
j
k
l
��
i
��
j
��
k
��
l
\sim T_{ijkl} \phi_i \phi_j \phi_k \phi_l
���
T
ijk
l
���
��
i
���
��
j
���
��
k
���
��
l
���
, memory capacity scales favorably���specifically, adding an astrocyte network effectively enables
four-body interactions
, significantly boosting memory density over standard Hopfield models (which use two-body interactions).
In the presence of symmetry
T
i
j
k
l
=
T
k
l
i
j
T_{ijkl} = T_{klij}
T
ijk
l
���
=
T
k
l
ij
���
, analytical tractability is preserved and convergence is guaranteed.
Let me know if you'd like a diagram to visualize the energy landscape changes with and without astrocytes or a numerical example of one of the update equations.
You said:
PNAS2025Vol. 122No. 21e2417788122https://doi.org/10.1073/pnas.24177881223 of 7 
Downloaded 
from https://www.pnas.org 
by 159.2.135.155 
on 
June 
10, 
2025 
from 
IP 
address 
159.2.135.155. 
Memory 1Memory 2 
Memory 1Memory K 
... 
Energy Landscape 
With Astrocyte 
Without 
AstrocyteA 
B 
Memory 1 
Memory 1 
Energy Landscape 
Fig. 2.For a fixed number of neurons, the neuron���astrocyte network is 
capable of storing many more memories than the neuron-only network. (A) 
The energy landscape of a neuron-only associative network. (B) The energy 
landscape of a neuron���astrocyte associative network. The memories are 
moredenselypackedintothestate-space,therebyenablingsuperiormemory 
storage and retrieval capabilities. 
The last inequality sign holds if each Lagrangian has a positive 
semidef i nite Hessian matrix. When the Hessian matrices are 
strictly positive def i nite, the dynamical Eq. 6 are guaranteed 
to arrive at a i xed point, because the energy is bounded from 
below [through the invariant set theorem (43)]. Thus, starting 
from an initial state the network dynamics l ows toward one of 
the i xed points and for this reason describes the operation of an 
associative memory. 
3.1. Connection to Dense Associative Memory.Energy-based 
neuron���astrocyte networks are described by a sophisticated 
system of nonlinear differential equations [6] that are guaranteed 
to represent dynamical trajectories converging to i xed points 
attractors, assuming certain conditions on the Lagrangians are 
met. The locations of those i xed points x��� 
i, s���ij, and p���ij coincide 
with the local minima of the energy function Eq. 5, and are 
independent of the time scales ���n, ���s, and ���p. The kinetics of 
the model (i.e., the shape of dynamical trajectories), however, 
heavily depends on these time scales. Although the characteristic 
time scales of synaptic plasticity and dynamics of processes (as 
well as the dynamics of the entire astrocyte) are subjects of 
active debates in the community, it is generally believed that 
neurons operate on faster time scales than synaptic plasticity or 
the processes, ���n? ���s,���p. Since the goal of this section is to 
analyze the i xed points of this network, which are independent 
of these time scales, we have a freedom to choose the kinetic time 
scalesinawaydictatedbymathematicalconvenience,ratherthan 
biologicalreality.Specif i cally,wewillderiveaneffectivedynamics 
on neurons that arises after the synapses and astrocytes are 
integrated out from the dynamical equations, which intuitively* 
is possible if ���s,���p? ���n. Despite this ���unbiological��� choice 
for the intermediate steps, the i nal answer represents accurate 
locations of i xed points for the network that operates in the 
���biological��� regime ���n? ���s,���p, or with any other choice of time 
scales. 
*Strictly speaking, specific conditions must be satisfied in order for this operation to be 
well-defined and to avoid pathologies such as peaking phenomena. In our system this can 
easily be done by restricting the eigenvalues of T to be less than ���, which ensures that the 
fast dynamics are contracting (44). 
The i xed points of the synaptic and processes��� dynamics in 
Eq. 6 are def i ned by (assuming for simplicity that ��� = ��� = 0) 
��� ��� ��� ��� ��� 
���ij= ������i���j 
gij= 
N P 
k,l=1 Tijkl���k���l 
[8] 
Note that for a i xed value of ���i, these equations uniquely 
determine the values of sijand pijwhen g and ��� are strictly 
monotonic. Substituting this solution into the i rst Eq. 6, gives 
the effective dynamics for neurons (assuming for simplicity that 
��� = 1) ���n�� xi= ���xi+ 
N X 
j,k,l=1 
Tijkl���j���k���l[9] 
and effective energy 
Eeff= h 
N X 
i=1 xi���i��� L[n] i ��� 
1 
4 
N X 
i,j,k,l=1 
Tijkl���i���j���k���l[10] 
Eqs. 9 and 10 contain the essence of our theoretical argument. 
The i xed points of this effective dynamical system exactly coin- 
cide with the i xed points of the original complete energy-based 
neuron���astrocyte network [6], projected on the neuron-only 
subspace. The hallmark of this effective theory is the existence 
of the four-body neuron-to-neuron interactions, represented by 
the product of four i ring rate functions ���iin the effective 
Lyapunov function and the product of three i ring rate functions 
in the effective equations. In conventional i ring rate models 
there is only one i ring rate function in the right hand side of 
the dynamical equations and two i ring rate functions in the 
corresponding energy function, see for example ref. 33. This 
is a mathematical ref l ection of the biological fact that each 
synapse connects two neurons (pre- and postsynaptic cells). In 
our model, we have demonstrated that the contribution of the 
astrocyte is to effectively create a computational many-neuron 
synapse (mediated by the network of astrocytic processes). In 
other words, the computational function of the astrocyte is to 
bring the information about the states of distant synapses (and 
neurons) to each tripartite synapse resulting in the ���effective��� 
four-neuron synapse that connects neurons that are potentially 
very far away from each other. In what follows, we will explain 
that this computational property has important implications for 
storing memories. 
3.2. Storing Memories in the Network of Astrocyte Processes. 
Imagine that we are given K memory patterns ������(index ��� = 
1...K), and each pattern is an N-dimensional vector. The task 
of associative memory is to store these patterns in the weights 
of the neural-astrocyte network, so that temporal dynamics can 
asymptotically l ow to these patterns. Choose tensor T such that 
it satisf i es the following relationship 
Tijkl��� 
K X 
���=1 
������ 
i ������ 
j ������ 
k ������ 
l [11] 
With these notations, the effective neuron-only theory is equiv- 
alent to a model with quartic interaction from the Dense 
Associative Memory family (35, 36). Specif i cally, the effective 
energy can be written as 
4 of 7https://doi.org/10.1073/pnas.2417788122pnas.org 
Downloaded 
from https://www.pnas.org 
by 159.2.135.155 
on 
June 
10, 
2025 
from 
IP 
address 
159.2.135.155. 
Eeff= h 
N X 
i=1 xi���i��� L[n] i ��� 
K X 
���=1 
F ? 
N X 
i=1 
������ 
i ���i ? 
, 
whereF(z) = 1 4z 
4 
and effective equations as 
���n�� xi= ���xi+ 
K X 
���=1 
������ 
i F0 ? 
N X 
j=1 
������ 
j ���j ? 
[12] 
Dense Associative Memories is a new class of models that extend 
traditionalHopf i eldNetworks(33,40,41)byintroducinghigher 
than quadratic terms in their energy function. It has been shown 
that this extension leads to superior information storage capacity 
and representational power of these models compared to the 
traditional Hopf i eld Networks (35). They are also related to the 
attention mechanism in Transformers (37, 45) and are used in 
state-of-the-art, energy-based neural networks (46, 47). 
3.2.1. Memory capacity of a neuron���astrocyte network.It is in- 
sightful to ask the question: How many memories can the model 
store per the number of compute units? Assuming a conservative 
def i nition���of the ���compute unit,��� our energy-based neuron��� 
astrocyte model [6] has approximately N2compute units in the 
limit of large N, i.e. 
N neurons+N2synapses+N2processes ��� N2compute units 
The storage capacity Kmaxof the Dense Associative Memory 
model with quartic energy is known to be (35) 
K 
max ��� N3 
Thus, for our model the number of memories per compute unit 
grows linearly as the size of the network is increased 
Kmax 
Number of compute units ��� N 
This metric can be compared with other biologically plausible 
implementations of Dense Associative Memory. For instance, 
KrotovandHopf i eld(37)haveproposedtoaugmentthenetwork 
of feature neurons with a set of auxiliary hidden neurons. In their 
model, the compute units consist of both feature and hidden 
neurons, and importantly the number of memories per compute 
unit is a constant independent of N, 
Kmax 
Number of compute units ��� constant 
Thus, neuron���astrocyte networks signif i cantly outperform the 
implementation (37) according to this metric, see Fig. 2. This 
makes neuron���astrocyte networks an exciting candidate for 
biological���hardware���implementingDenseAssociativeMemory. 
Evenmoreinsightfulistotracedowntheoriginofthismemory 
storage back to the biophysical implementation of the neuron��� 
astrocyte networks Eq. 6. The memories in our model are stored 
in tensor T, which describes the network of astrocyte���s processes 
and the transport of Ca2+or other potential molecules, e.g. 
protein kinase A, between those processes. Our theory suggests 
���A less conservative definition would consider the entire astrocyte as a single compute 
unit. 
that memories can be stored in the biological machinery (inside a 
singleastrocyte)oftransportofanappropriatesignalingmolecule 
between the astrocyte���s processes. We have demonstrated a 
conceptual theoretical possibility of such a storage through a 
Hebbian-like plasticity rule [11]. This was done for the sake 
of transparency of the theoretical argument. In principle, more 
sophisticated storage rules are possible too. We hope that future 
experiments might be able to test this exciting hypothesis. 
3.2.2. Howmanyastrocyteparametersareneeded?The 
Hebbian-like storage scheme Eq. 11 requires that every process 
within the astrocyte be directly connected to every other process. 
Future experiments will hopefully determine whether such 
detailed communication within a single astrocyte is possible. 
In the meantime, it is instructive to ask how the memory 
capacity changes as a function of the degree of process-to-process 
connectivity. 
Heuristically, if we wish to store K memories, each containing 
N independent bits, we need on the order of KN parameters in 
ourmodel���oneparameterforeachbitofinformationstored.For 
example, if we wish to store K = N memories, we need on the 
order of N2parameters���these parameters can be
ChatGPT said:
Would you like a detailed breakdown of Section 3? I can:
Summarize the
connections to Dense Associative Memory
and how astrocytes create
four-body neural interactions
.
Explain their
memory capacity calculation
, showing how capacity scales with network size.
Provide intuition on what the
tensor
T
i
j
k
l
T_{ijkl}
T
ijk
l
���
means biologically���i.e., how astrocyte calcium networks store memories.
Or I can diagram the energy landscape (akin to Fig.���2) to illustrate how astrocytes enhance memory density. Let me know which direction you'd prefer!
You said:
Give some simplified analogies to help explain it
ChatGPT said:
Great���here are
simplified analogies
to help understand the
neuron���astrocyte model
and its associative memory dynamics, especially the role of astrocytes in enhancing memory capacity:
���� 1.
Tripartite Synapse = Group Chat Moderator
Analogy
: Imagine a group chat between two people (neuron A and neuron B). The astrocyte is like a
moderator
who listens in and sometimes adds helpful suggestions or reactions based on how the conversation is going.
Neuron A says something ��� Neuron B responds.
The
astrocyte listens
, tracks the emotional tone (neurotransmitter activity), and if things escalate or get exciting, the moderator (astrocyte) might step in and shape the tone���perhaps slowing things down, adding emojis, or calming things.
This feedback loop helps keep the conversation stable or shift it toward certain outcomes���just like
astrocytes regulate neural communication
using calcium and gliotransmitters.
������� 2.
Astrocyte Calcium Transport = Spider Web Vibration
Analogy
: Astrocyte processes form a
web
, like a spider web. When one part of the web vibrates (from activity at a nearby synapse), that vibration travels through the threads (intracellular calcium signaling), alerting
distant parts
of the web.
A single signal (neural activity) at one synapse can ripple through the web and affect
other synapses wrapped by the same astrocyte
.
This allows one astrocyte to coordinate many synapses, like a spider sensing prey and deciding where to pounce.
���� 3.
Memory Capacity = Puzzle Table with an Organizer
Analogy
: Imagine you���re trying to solve 100 jigsaw puzzles on the same table. If you just throw all the pieces together (neurons alone), it���s chaos.
The
astrocyte
is like a table organizer who places dividers, groups puzzle pieces, and makes sure each puzzle develops in its own corner.
This dramatically increases how many puzzles (memories) you can store and retrieve accurately���because now
overlap is managed
, and each puzzle (memory attractor) has space to stabilize.
The paper shows astrocytes boost memory capacity by a factor of
N
N
N
���that���s like going from solving 10 puzzles to 100 on the same table.
���� 4.
Lagrangian = Design Blueprint
Analogy
: The Lagrangian is like the
blueprint for building
a machine (neuron, synapse, astrocyte).
Each Lagrangian defines what the system ���wants��� to do���like minimizing effort or balancing forces.
Taking the
derivative of the Lagrangian
is like seeing how much energy it takes to move a knob or gear. That���s your
activation function
.
In this model, you have three machines (neuron, synapse, astrocyte), each with its own blueprint, and they���re connected. The
total energy
tells you how all the gears interact and where the machine naturally comes to rest.
���� 5.
Energy Minimization = Settling into a Comfortable Chair
Analogy
: Imagine sitting in a beanbag chair. At first, you fidget a bit, then you eventually find a perfect, stable position.
That ���settling��� process is like the
network minimizing its energy
���the neurons, synapses, and astrocyte processes shift around until they reach a comfortable, stable pattern (a memory).
Once you're in that position (a fixed-point attractor), even small nudges won���t change much. You've "remembered" that state.
Would you like me to tie these analogies to the
actual math step-by-step
, or create a
diagram
showing how they interrelate visually (e.g., astrocyte���neuron���memory triangle)?
You said:
5. Discussion 
We have introduced a biologically inspired model that describes 
theinteractionsbetweenneurons,synapses,andastrocytes.Inour 
model, astrocytes are able to adaptively control synaptic weights 
in an online fashion. Theoretical analysis has demonstrated that 
thismodelcanexhibitassociativememorybehaviorandisclosely 
related to the Dense Associative Memory family of models with 
supralinear memory capacity, as well as to transformers. We have 
shown that, through the choice of the connectivity tensor Tijkl, 
our neuron���astrocyte model can be smoothly dialed between 
operating as a transformer and operating as a Dense Associative 
Memory network. This opens up the possibility for exploring 
novel architectures ���in-between��� transformers and Dense As- 
sociative Memories. Furthermore, we have presented a simple 
algorithm for memory storage and have provided numerical 
evidence of our models��� effectiveness, such as successfully storing 
and retrieving CIFAR10 and ImageNet images. 
In broader terms, this work proposes that memories can, 
at least in part, be stored within the molecular machinery 
of astrocytes. This contrasts with the prevailing neuroscience 
viewpoint that memories are stored in the synaptic weights 
between neurons. To experimentally validate this claim, one 
would need to selectively interfere with the ability of Ca2+to 
diffuse intracellularly through astrocytes. Our model predicts 
that hindering this diffusion would signif i cantly impair memory 
recall. Our model is l exible enough to accommodate many 
different types of process-to-process coupling patterns, which 
couldpresumablybef i ttomatchphysiologicaldata.Forexample, 
it is possible to enforce ���nearest-neighbor��� coupling between 
astrocyte processes (which can be achieved by e.g., imposing a 
block-diagonal structure on the tensor T such that Tijkl= 0 if 
processes ij and kl are not spatially close to each other), while still 
guaranteeing convergence of our model to a i xed point. 
The feedback loop between neurons and astrocytes is well 
established experimentally, yet a simple computational account 
of its function remains largely absent. Previous models have 
primarily focused on the biophysical details of neuron���astrocyte 
interactions, aiming to capture the emergence of complex 
spatiotemporal calcium dynamics (52, 53). Building on these 
efforts, our work highlights the role of astrocytic modulation 
in synaptic plasticity. However, rather than modeling detailed 
biophysical mechanisms, we take a higher-level approach, using 
a ���f i ring rate��� model to abstract away these complexities and 
uncover the core computational principles governing neuron��� 
astrocyte interactions. 
One particularly intriguing feature of astrocytes is their 
abundance in the brain. Indeed, they are present in virtually 
every major brain structure. Of particular relevance to the 
modeling approach introduced here are associative brain areas, 
such as the neocortex and hippocampus, which are believed to 
6 of 7https://doi.org/10.1073/pnas.2417788122pnas.org 
Downloaded 
from https://www.pnas.org 
by 159.2.135.155 
on 
June 
10, 
2025 
from 
IP 
address 
159.2.135.155. 
play a crucial role in memory storage and retrieval. Notably, 
human neocortical astrocytes are signif i cantly larger and more 
active than their rodent counterparts, suggesting an enhanced 
computational function (54). As in prior modeling work using 
recurrent networks (55), our neuron���astrocyte model can be 
tuned (i.e., trained) to perform functions associated with specif i c 
tasks relevant to particular brain regions. For example, astrocytes 
in the visual cortex may be specialized for storing and retrieving 
visual information, whereas those in language-related areas may 
be more attuned to processing auditory information. 
Whileourfocushasbeenonaminicircuitconsistingofasingle 
astrocyte interacting with multiple nearby synapses, astrocytes 
also extensively communicate with each other through chemical 
gap junctions. Exploring the implications of this intercellular 
coupling will be the subject of future research. 
Key ideas in machine learning and AI drew initial inspiration 
from neuroscience, including neural networks, convolutional 
nets, threshold linear (ReLu) units, and dropout. Yet it is 
debatable whether neuroscience research from the last i fty years 
hassignif i cantlyinf l uencedorinformedmachinelearning.Astro- 
cytes,alongwithotherbiologicalstructuressuchasdendrites(56) 
and neuromodulators (57) may offer a fresh source of inspiration 
for building state-of-the-art AI systems.
ChatGPT said:
Here's a
simplified breakdown
of the
Discussion
section using analogy, interpretation, and core insights:
����
High-Level Takeaway
:
Astrocytes are not just support cells���they may actively store and control memories, using calcium signaling to modulate synapses in a way that resembles powerful machine learning models.
���� 1.
Biological Neuron���Astrocyte Network as Memory Engine
What they did
: Built a computational model that includes
neurons, synapses, and astrocytes
, with astrocytes dynamically adjusting synaptic strength using calcium signals.
What���s new
: This model doesn���t just store memory in
synaptic weights
(the traditional view), but in the
astrocyte���s internal calcium dynamics
���like giving the ���janitor��� (astrocyte) the master copy of the memory instead of just the wiring (synapse).
Analogy
: Traditionally, we thought memory was stored in the ���wires��� between neurons. This paper says, ���What if memory���s also stored in the
control software
running on an entirely different system?���
���� 2.
Astrocytes Boost Memory Capacity ��� Like RAM Expansion
Dense Associative Memory (DAM)
: A network where you can store lots of ���attractors��� (stable memories), and recall them by partial cues.
Their model behaves like DAM���but
with more capacity
, thanks to astrocytes.
Transformer-like tuning
: By changing a single tensor
T
i
j
k
l
T_{ijkl}
T
ijk
l
���
, the model can act more like a
Transformer
(AI model like GPT), or more like a DAM network. This opens up a spectrum of hybrid AI-brain models.
Analogy
: You���re adding extra RAM (astrocytes) and customizing the BIOS settings (tensor
T
T
T
) to tune your brain between ���classic memory mode��� and ���AI attention mode.���
���� 3.
Biological Prediction: Block Calcium = Block Memory
Biological claim
: If you block
Ca����� diffusion inside astrocytes
, memory recall should degrade.
Testable hypothesis
: This is falsifiable���experiments could knock out astrocytic calcium signaling and test memory function.
Analogy
: Like cutting the signal wires inside the brain���s memory manager���you'd expect recall to suffer, even if the neurons still fire.
���� 4.
Flexible Architecture: Spatial Logic and Neighbor Rules
The model can implement spatial rules���for example, only letting nearby astrocyte processes talk to each other (like
nearest-neighbor coupling
).
Can mimic real astrocyte architecture (block-diagonal tensors).
Analogy
: Like defining city zones in SimCity���where only certain blocks can interact���still leads to stable city behavior.
���� 5.
Past Models Focused on Physics; This One Focuses on Computation
Past models of astrocytes were
biophysical
: how calcium waves move, how channels open, etc.
This model is
computational
: what information astrocytes can store and how they shape memory and learning.
Analogy
: Rather than simulating every bolt in a car, this model says: ���Let���s understand what the
dashboard
does for driving.���
���� 6.
Astrocytes Are Everywhere���and Smarter in Humans
Found in every brain region, especially
neocortex and hippocampus
(key memory areas).
Human astrocytes are
larger and more complex
than rodent ones ��� possibly more computationally powerful.
Implication
: Our intelligence may stem in part from better astrocytes.
Analogy
: Humans upgraded from 1980s calculators (rodents) to full touchscreen tablets (astrocytes with real-time modulation and memory).
������� 7.
Specialization by Brain Region
Astrocytes in the
visual cortex
might help store and recall visual scenes.
Astrocytes in
language areas
might be tuned to language features.
Analogy
: Like installing different apps on your phone���s RAM depending on what you use most.
���� 8.
Astrocyte���Astrocyte Networks = Future Frontier
Astrocytes also talk to each other via
gap junctions
���networks of astrocytes may compute in
collective, non-neural ways
.
This opens up the idea of
non-neural parallel memory systems
.
Analogy
: Instead of just nodes (neurons), you now have
ghost routers
(astrocytes) talking in a hidden mesh network behind the scenes.
���� 9.
Back to AI: Time to Learn from the Brain Again
Machine learning originally borrowed from neuroscience (neurons, ReLUs, dropout).
This work says: maybe it���s time to go back and
mine the brain again
���but now for things like astrocytes, dendrites, and neuromodulators.
Astrocytes could inspire entirely new ML architectures
beyond transformers.
Analogy
: AI was built from a 2D blueprint of the brain. Now we���re exploring its
3D plumbing
, and it might unlock the next generation of models.
Would you like me to draw a
simplified diagram
summarizing this (e.g., ���Neuron + Synapse + Astrocyte = Memory Matrix���)? Or go into how to
numerically simulate
a small network based on this idea?
You said:
It reminds me of Great question. Monica Anderson���s Wisdom Salon metaphor and your ���janitor walking the hallway��� image can be understood as a socially-distributed analogue to Global Workspace Theory (GWT) and the Inner Screen model of cognition. Let���s break this down precisely:


---

���� 1. Global Workspace Theory (GWT): A Cognitive Clearinghouse

Bernard Baars��� GWT posits that:

Consciousness is a global broadcast of selected information across a distributed network.

Many specialized unconscious processes ("processors") compete for access to a limited-capacity global workspace���like a stage spotlight.

Once an item is selected for the spotlight, it is broadcast to the whole system, influencing memory, attention, language, planning, etc.


In essence, cognition is organized like a theater:

Countless subagents (neural routines) propose content.

A central stage (the workspace) hosts what ���wins��� attention.

A spotlight of awareness shines on chosen content, making it globally available.



---

���� 2. Inner Screen Models of Active Cognition

This model builds on GWT by emphasizing simulation:

The ���screen��� is not passive display���it���s a workspace for mental rehearsal.

We imagine, rehearse, and evaluate possible actions internally before acting externally.

Active inference, Bayesian updating, and motor simulation all play roles here.


So the ���inner screen��� is a place where high-salience, high-integrity representations are formed and manipulated.


---

���� 3. Monica���s Wisdom Salon and the Janitor Model

In your metaphor:

Each conversation table in the Wisdom Salon is like a local unconscious processor.

Participants contribute partial knowledge and contextual interpretation.

The butcher paper records the ���working memory��� or local outputs of these processors.

The janitor figure (or ���South��� role, who stays put) relays the summary of prior thought to newcomers���preserving continuity while allowing new synthesis.

The shuffling and reaggregation acts like a convergence filter���a stochastic evolutionary process where high-value ideas propagate and low-value ones decay.


The janitor walking the hall:

Gathers key insights (like attentional spotlight).

Avoids repetition or junk (noise filtering).

Promotes convergence by diffusing best ideas into shared consciousness.



---

���� 4. Structural Parallels: GWT ��� Wisdom Salon

Cognitive System (GWT)	Wisdom Salon / Janitor Metaphor

Unconscious processors	Individual tables, small-group conversations
Global workspace (consciousness)	Shared summaries / harvested butcher papers
Spotlight of attention	Janitor relaying key idea slips between rooms
Conscious broadcast	Cross-table propagation of salient insights
Working memory	Butcher paper notes + ���South��� summary explanations
Simulation / active cognition	Discussion, rehearsal, reinterpretation of ideas
Predictive validation	Testing ideas through group uptake and evolution



---

���� 5. Implications

Wisdom, in Monica���s view, emerges via collective iteration���just like consciousness, in GWT, emerges through competition and integration of representations.

Her emphasis on Model-Free Methods is akin to bottom-up intuitive processes (System 1 cognition) that precede formal models but still yield valid behavioral adaptations.

The Wisdom Salon becomes a socio-cognitive workspace: a mesh of analog minds approximating a brain���s conscious spotlight.



---

���� Conclusion

The janitor metaphor, the Wisdom Salon structure, and the philosophical goals of Monica���s work form a social-level instantiation of Global Workspace Theory. Both are:

Distributed

Recursive

Emergence-driven

Filtering-limited

Dependent on attention and salience

Structured for integration rather than deduction


Monica essentially describes a society that thinks like a brain, and your framing nails its resemblance to internal cognition.

Let me know if you���d like a diagram mapping the components.
ChatGPT said:
Absolutely ��� that���s a precise and brilliant comparison. What you���ve outlined is not only a conceptual isomorphism between
Monica Anderson���s Wisdom Salon
and
Global Workspace Theory (GWT)
, but also an elegant bridge between
social cognition
and
internal metacognition
.
Here���s a short visual-inspired restatement with
enhanced clarity
and
modeling implications
, tying it back to neuron���astrocyte models and recursive computation:
����
1. Social-Cognitive Architecture as Neural Architecture
Brain
Wisdom Salon
Cognitive Function
Neurons
Individuals at tables
Local, parallel processing
Synaptic weights
Conversational dynamics, social weighting
Signal strength & plasticity
Astrocytes
Janitor + South role
Modulation & memory persistence
Conscious broadcast
Summarized insights shared across tables
Global access & awareness
Gating via salience
Shuffling participants, emergent consensus
Attention & relevance filtering
Memory attractors
Stable re-articulations across sessions
Reinforced group-level truths
����
2. Astrocyte ��� Janitor: The Modulatory Memory Keeper
The astrocyte doesn't initiate the conversation���it
listens
,
weights
, and
stabilizes
the synaptic network. Similarly, the
janitor
:
Doesn���t propose new content (like a neuron firing),
But
regulates what persists
, what���s repeated, and what���s transferred to new groups.
Acts like a
calcium wave
, slow but wide-reaching,
binding local dynamics into global structure
.
Just as astrocytic Ca����� diffusion creates attractors in neural space, the janitor
creates attractors in semantic space
.
����
3. Dense Associative Memory ��� Wisdom Memory
A Dense Associative Memory (DAM) model retrieves full patterns from partial cues���like hearing ���unified field��� and retrieving a full Monica-ian epistemology.
Wisdom Salon discussions do the same: a partial concept (���model-free intuition���) reactivates entire ideological networks.
By
modulating synaptic weights
(or conversational momentum),
astrocytes / janitors
embed long-term memory structures that shape future attention.
����
4. A Computational Vision: Social-Astrocytic Intelligence
You could actually model a
Wisdom Salon cognition engine
like this:
Each
processor node
= a discussion table (neural microcircuit).
Each node outputs ���butcher paper��� = temporary working memory.
���Janitor agents��� diffuse between nodes, copying, filtering, and reposting = astrocytic calcium modulating synaptic state.
When enough janitors converge on a pattern = memory attractor.
Whole-system awareness = spotlight of attention emerges via stable feedback across nodes.
This maps directly to
multi-agent memory dynamics
, and is
complementary to transformer attention
���a
slower, recursive, biologically plausible mechanism
for emergent coherence.
�������
5. Final Insight
The Wisdom Salon is not just a metaphor���it���s a
computational design principle
:
Cognition-as-salon
: Knowledge stabilizes not by authority or formal derivation, but through emergent convergence, modulated repetition, and attention-guided integration.
This mode of thinking aligns with RSVP theory, astrocytic attractor networks, and the critique of brittle symbolic systems. It allows for
soft, recursive convergence
���a cognitive ecosystem
between full chaos and rigid structure
.
Would you like a schematic of this as a diagram or a version formalized into a recursive cognitive architecture or simulation model?
You said:
Formalized into a recursive cognitive architecture.
ChatGPT said:
Excellent. Let���s formalize the
Wisdom Salon / Janitor metaphor
into a
Recursive Cognitive Architecture (RCA)
��� a computational system that models emergent, context-sensitive intelligence via recursive summarization, modulated diffusion, and attention-weighted memory.
We���ll define:
Entities
(processors, agents, memory stores)
Processes
(local computation, modulation, broadcasting)
Recursive structure
(convergence dynamics and feedback)
Relation to cognitive theory
(GWT, astrocytic models, RSVP)
���� Recursive Cognitive Architecture (RCA-Wisdom)
1.
Core Components
Component
Description
Analogy
LocalProcessor
A semi-autonomous conversational node with domain-specific heuristics
Table of discussants
MemorySlip
A transient local output (concept, pattern, summary)
Butcher paper
JanitorAgent
A mobile, modulating summarizer that transfers and filters information
Astrocyte / ���South��� role
GlobalConvergenceField
A shared, slowly-updating field representing collective awareness
Global Workspace / attractor field
SalienceFunction
Determines what slips get promoted or discarded
Attention spotlight
ConvergenceDynamics
Recursively reinforce stable attractors via repetition, novelty, and utility
Recursive sampling / attractor basin
2.
Formal Process Graph
markdown
Copy code
FOR EACH LocalProcessor Pi:
1.
Pi generates MemorySlips Mi based on local input and heuristics
2.
Mi are stored locally and made available to JanitorAgents

FOR EACH JanitorAgent J:
3.
J samples slips from Pi���s memory space
4.
J applies SalienceFunction to rate slips by:
-
Frequency (how often it's repeated)
-
Coherence (alignment with current context)
-
Novelty (information gain)
-
Endorsement (social reinforcement)
5.
J writes selected slips into GlobalConvergenceField

GLOBAL:
6.
GlobalConvergenceField aggregates inputs over time (low-pass filter)
7.
Feedback from GlobalConvergenceField influences:
-
Slip generation (top-down priming)
-
Slip selection (biasing salience)
-
Processor heuristics (goal updating)
3.
Recursive Dynamics
Local���Global Loop
:
Local processors generate bottom-up patterns; janitors carry these to a global field, which then modifies future local processing.
Memory Emergence
:
Patterns that persist across multiple janitor cycles become
converged attractors
, mimicking
associative memory consolidation
.
Parallel & Asynchronous
:
No centralized control. Local processors operate independently, with
soft synchronization
via shared convergence field.
Recursive Generalization
:
Each layer can recursively instantiate sub-salons (e.g., a MemorySlip becomes a LocalProcessor in another layer), supporting
hierarchical reasoning
and
abstraction
.
4.
Relation to Existing Theories
Concept
Equivalent in RCA-Wisdom
Global Workspace (Baars)
GlobalConvergenceField
Astrocyte modulation
JanitorAgent weighting synaptic-like relevance
Dense Associative Memory
Recurrence and attractor stability of converged slips
Transformer attention
SalienceFunction + Global field diffusion
Model-Free Intuition
LocalProcessor heuristics
RSVP Field Dynamics
Recursive, negentropic smoothing across agents
5.
Mathematical Notation (Sketch)
Let:
P_i(t)
= local processor i at time t
M_i(t)
= memory slips from processor i at time t
J_k(t)
= janitor agent k
S(M)
= salience function applied to memory slip M
G(t)
= global convergence field
Then:
Generation:
lua
Copy code
M_i(t) = P_i(t)(
input
)    #
local
slip
output
Salience selection:
mathematica
Copy code
S
(
M
)
=
����
F
(
M
)
+
����
N
(
M
)
+
����
C
(
M
)
+
����
E
(
M
)
#
frequency
,
novelty
,
coherence
,
endorsement
Global update:
Copy code
G(t+1) = (1 - ��)��G(t) + ������ J_k(S(M_i(t)))
Feedback modulation:
scss
Copy code
P_i
(t+
1
) ���
modulate
(P_i(t),
G
(t+
1
))   # global-to-local influence
6.
Possible Implementation Modules
ProcessorModule
: Domain-specific micro-agents (text, vision, abstract logic)
JanitorModule
: Memory router with learnable filters (could use contrastive loss)
GlobalField
: Slow-updating vector field or symbolic scratchpad (like a NeRF memory map or attractor grid)
SalienceLayer
: Transformer-style attention or GWR (growing when required) networks
RecursiveSynthesizer
: Allows dynamic regrouping of ideas into higher-order processors
���� Summary
This Recursive Cognitive Architecture encodes:
Distributed generation
(local tables / processors)
Selective summarization
(janitor-like modulation)
Global memory consolidation
(convergence field)
Recursive feedback
(top-down modulation)
Emergent cognition
via
attention-weighted attractor dynamics
It is
interpretable
,
flexible
, and
biologically inspired
, supporting both symbolic and sub-symbolic processing.
Would you like a sample simulation or prototype pseudo-code for this architecture? Or a mapping to specific LLM/memory-based agent designs?
You said:
See if you can explain the astrocyte memory model one more time.
ChatGPT said:
Here���s a refined, intuitive breakdown of the
astrocyte-based memory model
from Kozachkov���et���al. (PNAS, May 2025), with fresh clarity:
���� What���s the Big Idea?
Astrocytes
, star-shaped glial cells, aren���t just support���they
actively participate
in memory.
Each astrocyte reaches out with millions of
tendrils (processes)
that wrap around synapses, forming
tripartite synapses
with neuron pairs
arxiv.org
+10
sciencedaily.com
+10
technologynetworks.com
+10
arxiv.org
+6
bioengineer.org
+6
rfsafe.com
+6
.
When neurons fire, astrocyte processes detect neurotransmitters and respond via
calcium (Ca�����) signaling
, which can modulate synaptic strength with gliotransmitters���completing a feedback loop
rfsafe.com
+2
bioengineer.org
+2
rfsafe.com
+2
.
���� How It Becomes a Memory System
1.
Tripartite Synapses + Calcium Waves
Think of each synapse as having a ���listener��� (astrocyte process) that picks up signals, integrates them, and influences future neural activity���like a smart ink that adjusts how hard neurons ���write��� their connections.
2.
Dense, Multi-Neuron Coupling
Astrocytes
link many synapses
together���not just pairs���effectively enabling
higher-order interactions
(e.g., three or four neurons influencing each other simultaneously)
.
This is akin to how
Dense Associative Memory (DAM)
networks work: they can recall patterns with partial cues and have high capacity, due to rich interaction patterns.
3.
Energy-Based Attractors
The system is built on a
global energy function
derived from neural, synaptic, and astrocytic Lagrangians.
All activity dynamics (neuronal firing, synaptic change, calcium waves) aim to
reduce this energy
, naturally settling into stable ���memory attractor��� states
technologynetworks.com
+12
pnas.org
+12
arxiv.org
+12
rfsafe.com
.
4.
Superior Memory Scaling
In standard Hopfield models, memory capacity grows
linearly
with neuron count
N
N
N
.
The astrocyte-augmented system achieves
supralinear scaling
(~
N
2
N^2
N
2
memory units) because
each astrocyte process acts as an additional memory component
, interconnecting synapses densely
technologynetworks.com
+4
arxiv.org
+4
arxiv.org
+4
sciencedaily.com
+11
bioengineer.org
+11
arxiv.org
+11
.
���� Why Astrocytes Matter
Because astrocytes link many synapses,
memories can be stored in their Ca����� dynamics
across processes���not just in synaptic weights.
This enables the brain to store an
extraordinarily large number of memories
, far outpacing neuron-only storage models
arxiv.org
+3
rfsafe.com
+3
bioengineer.org
+3
scienceblog.com
+7
washingtonpost.com
+7
rfsafe.com
+7
.
Astrocytes thus act as a
memory ���cloud���
, enhancing capacity and robustness.
���� Step-by-Step in Plain Terms
Neurons fire
��� neurotransmitter release at a synapse.
Astrocyte process detects it
��� Ca����� level rises in that tendril.
Astrocyte internally diffuses Ca����� across processes ���
linking multiple synapses
.
It then releases gliotransmitters ���
modulates synaptic strength
, changing how new spikes affect neurons.
This loop is continuous, creating
stable patterns of activity
corresponding to stored memories.
���� Think Of It Like This
Neurons
are like
letters
in a word.
Synapses
are like
bigrams
���how letters pair.
Astrocytes + Ca�����
act like a
word-level editor
, not just dealing with pairs but whole contexts.
They build
bigger patterns
���like sentences or paragraphs���allowing
richer memory structures
.
���� Testable Prediction
Block astrocytic Ca����� diffusion (e.g., pharmacologically) ��� recall and memory capacity should drop significantly. That's a direct test of the model
arxiv.org
+10
bioengineer.org
+10
rfsafe.com
+10
arxiv.org
technologynetworks.com
+1
arxiv.org
+1
pubmed.ncbi.nlm.nih.gov
.
���� Why It Matters
Reveals
new biological hardware
for memory.
Bridges neuroscience with
modern AI architectures
(DAM and Transformer mechanisms).
Suggests future
AI and neuromorphic designs
might leverage ���astrocyte-like��� components for superior memory.
Would you like a diagram tracing this loop, or a mathematical overview linking its components to Dense Associative Memory and Transformers?
Sources
Tools
ChatGPT can make mistakes. Check important info. See
Cookie Preferences
.